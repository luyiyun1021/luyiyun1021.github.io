<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>01 CUDA C Basics</title>
    <url>/2025/01/16/01-CUDA-C-Basics/</url>
    <content><![CDATA[<h2 id="课程链接"><a class="markdownIt-Anchor" href="#课程链接"></a> 课程链接</h2>
<p><a href="https://www.bilibili.com/video/BV1JJ4m1P7xW">【Nvidia官方课程】CUDA入门课【中英字幕】_哔哩哔哩_bilibili</a>﻿<br />
﻿<a href="https://www.olcf.ornl.gov/cuda-training-series/">https://www.olcf.ornl.gov/cuda-training-series/</a></p>
<h2 id="cuda-intro"><a class="markdownIt-Anchor" href="#cuda-intro"></a> CUDA Intro</h2>
<p><strong>异构计算</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162334293.png" alt="image.png" /></p>
<p>GPU加速</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162343989.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162352994.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="第一个cuda程序1-d-向量加法"><a class="markdownIt-Anchor" href="#第一个cuda程序1-d-向量加法"></a> <strong>第一个cuda程序：1-D 向量加法</strong></h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162400442.png" alt="image.png" /></p>
<p>关键词<code>__global__</code>告诉<code>nvcc</code>编译器这个函数编译成能在gpu上运行的代码，其他函数用<code>gcc</code>等编译</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162407419.png" alt="image.png" /></p>
<p>gpu函数调用方法</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162415839.png" alt="image.png" /></p>
<p>内存管理</p>
<p>使用<code>cudaMalloc</code>创建的指针仅能在gpu中解引用，使用<code>malloc</code>创建的指针仅能在cpu中解引用</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162422783.png" alt="image.png" /></p>
<h3 id="层次结构"><a class="markdownIt-Anchor" href="#层次结构"></a> 层次结构</h3>
<p>GPU 层次结构：Thread, Block 以及 Grid：</p>
<ul>
<li>
<p>Thread 是最基本的执行单位，<strong>每一个 Thread 都会把你写的 CUDA Kernel 从头到尾完整地执行一遍</strong>。</p>
</li>
<li>
<p>每一个 Block 中包含若干个 Thread，每一个 Thread 都会有一个<code>threadIdx</code>，代表这个 Thread 在它所在的 Block 中的 id。可以使用<code>blockDim</code>来获取 Block 中有多少个 Thread。</p>
</li>
<li>
<p>每一个 Grid 包含若干个 Block，每一个 Thread 也有一个<code>blockIdx</code>，代表这个 Thread 所在的 Block 在 Grid 中的 id。可以使用<code>gridDim</code>来获取 Grid 中有多少个 Block。每一次启动 CUDA Kernel 时都会生成一个 Grid（某种意义上可以理解为一个 “执行上下文”。</p>
</li>
</ul>
<p>每一个cuda线程都有自己的<strong>控制流、PC、寄存器、堆栈</strong>，能够访问GPU任意全局内存地址</p>
<p>threadIdx.{x,y,z}<br />
blockIdx.{x,y}</p>
<p>Kernel上的两层线程组织结构如下(2-dim)</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162432922.png" alt="image.png" /></p>
<p>并行运行 <code>block</code></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162445545.png" alt="image.png" /></p>
<p>&lt;&lt;&lt;&gt;&gt;&gt; 表示 &lt;&lt;&lt;每个 Grid 中有多少 Block, 每个 Block 中有多少 Thread&gt;&gt;&gt;</p>
<p>向量加法函数<code>add</code>编写方法</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162455671.png" alt="image.png" /></p>
<p>内存分配过程</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162503572.png" alt="image.png" /></p>
<blockquote>
<p>声明<code>int *a</code>但未初始化时</p>
<ul>
<li>
<p><code>a</code> 的值是未定义的，因为它指向一个随机的内存地址。</p>
</li>
<li>
<p><code>*a</code> 的值是未定义的，访问它可能导致程序崩溃。</p>
</li>
<li>
<p><code>&amp;a</code> 的值是指针变量 <code>a</code> 的内存地址，它是定义明确的。</p>
</li>
</ul>
</blockquote>
<p>把数据复制到gpu -&gt; 在gpu上并行计算 -&gt; 把数据复制回cpu -&gt; 释放内存</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162516907.png" alt="image.png" /></p>
<p>尽管cuda宣称符合c++14的标准，但它并未将标准库纳入其中，这意味着 std:: 开头的函数不被支持</p>
<p><code>thread</code></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162525426.png" alt="image.png" /></p>
<p>全局 index</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162533645.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162542386.png" alt="image.png" /></p>
<p>一个线程需要两个内置的坐标变量<code>(blockIdx,threadIdx)</code>来唯一标识，都是<code>dim3</code>变量。</p>
<p>关于<code>dim3</code>的结构类型</p>
<ul>
<li>
<p><code>dim3</code>是基于<code>uint3</code>定义的矢量类型，相当于由3个<code>unsigned int</code>型组成的结构体。<code>uint3</code>类型有三个数据成员<code>unsigned int x; unsigned int y; unsigned int z;</code></p>
</li>
<li>
<p>可使用于一维、二维或三维的索引来标识线程，构成一维、二维或三维线程块(block)。</p>
</li>
<li>
<p>相关的几个内置变量</p>
<ul>
<li>
<p><code>threadIdx</code>，顾名思义获取线程<code>thread</code>的ID索引；如果线程是一维的那么就取<code>threadIdx.x</code>，二维的还可以多取到一个值<code>threadIdx.y</code>，以此类推到三维<code>threadIdx.z</code>。</p>
</li>
<li>
<p><code>blockIdx</code>，线程块的ID索引；同样有<code>blockIdx.x</code>，<code>blockIdx.y</code>，<code>blockIdx.z</code>。</p>
</li>
<li>
<p><code>blockDim</code>，线程块的维度，同样有<code>blockDim.x</code>，<code>blockDim.y</code>，<code>blockDim.z</code>。</p>
</li>
<li>
<p><code>gridDim</code>，线程格的维度，同样有<code>gridDim.x</code>，<code>gridDim.y</code>，<code>gridDim.z</code>。</p>
</li>
</ul>
</li>
<li>
<p>对于一维的<code>block</code>，线程的<code>threadID = threadIdx.x</code></p>
</li>
<li>
<p>对于大小为<code>(blockDim.x, blockDim.y)</code>的二维<code>block</code></p>
<ul>
<li>
<p><code>int i = blockIdx.x * blockDim.x + threadIdx.x;</code></p>
</li>
<li>
<p><code>int j = blockIdx.y * blockDim.y + threadIdx.y;</code></p>
</li>
</ul>
</li>
<li>
<p>对于大小为<code>(blockDim.x, blockDim.y, blockDim.z)</code>的三维<code>block</code>，线程的<code>threadID = threadIdx.x+threadIdx.y*blockDim.x+threadIdx.z*blockDim.x*blockDim.y</code></p>
</li>
<li>
<p>对于计算线程索引偏移增量为已启动线程的总数，如<code>stride = blockDim.x * gridDim.x; threadId += stride</code></p>
</li>
</ul>
<p>01-CUDA-C-Basics.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>02 CUDA Shared Memory</title>
    <url>/2025/01/16/02-CUDA-Shared-Memory/</url>
    <content><![CDATA[<h2 id="1-d-stencil-问题"><a class="markdownIt-Anchor" href="#1-d-stencil-问题"></a> <strong>1-D Stencil 问题</strong></h2>
<p>stencil可以理解为一个计算的窗口，如下图为半径为3的1-D stencil，宽度为7（宽度一般为奇数），一次进行对七个元素的计算</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162607310.png" alt="image.png" /></p>
<span id="more"></span>
<p>已知stencil 半径为3，宽度为7，步长为1（即每个元素会有7次参与到运算），下图为stencil计算过程</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162615963.png" alt="image.png" /></p>
<p>在向量加法问题中，单一输入点仅贡献于单一输出点，而单一输出点也仅依赖于单一输入点。</p>
<p>而在 stencil 问题中，单一输入点贡献于七个输出点，单一输出点也依赖于七个输入点。</p>
<p>stencil 问题依赖于 cuda 提供的共享内存（shared memory）</p>
<h2 id="共享内存shared-memory"><a class="markdownIt-Anchor" href="#共享内存shared-memory"></a> 共享内存（shared memory）</h2>
<p>thread 之间的数据共享基于 shared memory</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162631214.png" alt="image.png" /></p>
<ul>
<li>
<p>每一个线程都有私有的寄存器，64/128KB的寄存器堆会划分到每一个线程上</p>
</li>
<li>
<p>每一个<strong>线程块</strong>都有一块<strong>共享内存</strong>，共享内存实际上是在GPU芯片上实现的，因此我们可以称之为片上内存（on-chip），也被称为用户管理的快速缓存（与之对比的是真的的cache由硬件管理）</p>
</li>
<li>
<p>每一个<strong>网格</strong>共用一块大的<strong>全局共享内存</strong>（同时有一个768KB的共享L2），这是在 host 上通过<code>cudaMalloc</code>分配的内存，这并非位于GPU芯片本身，而是由多个高速内存设备组成，与GPU相连，比线程块之间的共享内存慢五倍。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162637944.png" alt="image.png" /></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>变量声明</td>
<td>存储器</td>
<td>作用域</td>
<td>生存周期</td>
</tr>
<tr>
<td><code>int var</code></td>
<td>寄存器</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><code>int array_var[100]</code></td>
<td>寄存器/本地</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><code>__shared__ int shared_var</code></td>
<td>共享</td>
<td>线程块</td>
<td>线程块</td>
</tr>
<tr>
<td><code>__device__ int global_var</code></td>
<td>全局</td>
<td>全局</td>
<td>应用程序</td>
</tr>
<tr>
<td><code>__constant__ int constant_var</code></td>
<td>常量</td>
<td>全局</td>
<td>应用程序</td>
</tr>
</tbody>
</table>
<h2 id="回到1-d-stencil-问题"><a class="markdownIt-Anchor" href="#回到1-d-stencil-问题"></a> 回到<strong>1-D Stencil 问题</strong></h2>
<p>思路：</p>
<ol>
<li>
<p>把（blockDim(每个block包含的threads数量) + 2 * 半径（也被称为光晕））数量的数据从全局内存中加载入共享内存中</p>
</li>
<li>
<p>计算出 blockDim 维度的输出</p>
</li>
<li>
<p>把 blockDim 维度的计算结果写回全局内存</p>
</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162646435.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162652825.png" alt="image.png" /></p>
<ul>
<li>
<p><code>gindex</code>:全局内存中的数据索引</p>
</li>
<li>
<p><code>lindex</code>:共享内存中的数据索引</p>
</li>
<li>
<p>在radius为3的情况下，会有三个线程负责将左右的光晕load到共享内存中</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162659403.png" alt="image.png" /></p>
<p>以上代码的问题：线程并没有做同步</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162709653.png" alt="image.png" /></p>
<p><strong>解决方法：引入</strong><code>**__syncthreads()**</code><strong>同步</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162717375.png" alt="image.png" /></p>
<ul>
<li><strong>所有线程都必须能够参与并执行到同步线程（sync thread)语句。</strong></li>
</ul>
<p>修改后的正确版本</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162725282.png" alt="image.png" /></p>
<p>02-CUDA-Shared-Memory.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>03 Fundamental CUDA Optimization (Part 1)</title>
    <url>/2025/01/16/03-Fundamental-CUDA-Optimization-Part-1/</url>
    <content><![CDATA[<p><strong>这一章节将探讨GPU架构和第一个CUDA优化重点，即实现充分的并行性</strong></p>
<h2 id="nvidia-gpu-架构演进"><a class="markdownIt-Anchor" href="#nvidia-gpu-架构演进"></a> Nvidia GPU 架构演进</h2>
<p><strong>对 GPU 架构的一定了解，将有助于你从根本上编写出在 GPU 上性能优异的代码</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163109194.png" alt="image.png" /></p>
<span id="more"></span>
<h3 id="kepler-架构"><a class="markdownIt-Anchor" href="#kepler-架构"></a> KEPLER 架构</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163126366.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163115967.png" alt="image.png" /></p>
<p><strong>CC</strong> (Compute Capability，计算能力)：计算能力，数字越大能力越强</p>
<p><strong>SM</strong> (Streaming Multiprocessor，流式多处理器) ：GPU 架构的 buliding block，但我们构建一个大的 GPU 时，我们通过在芯片上集成大量的 SM 来达成这一目标</p>
<p><strong>SP Unit/Core</strong> (Streaming processor Unit) ：执行 32 位浮点乘法、浮点加法以及浮点融合乘加运算的算数单元</p>
<p><strong>DP Unit</strong>（Double Precision Unit）：执行 64 位浮点运算的算数单元</p>
<p><strong>SFU Unit</strong>（Special Function Unit，特殊运算单元）负责特殊的 ALU 运算，如 SIN 和 COS</p>
<p><strong>LD/ST Unit</strong>（Load-Store Unit，存储负载单元）：在 GPU 上执行操作，例如 32 位浮点乘法时，首先执行的指令是将操作数据加载到寄存器文件中，随后执行一条指令在 Register File 内部完成乘法运算，当需要保存结果时则通过加载存储单元进行存储。加载存储单元作为独立的逻辑单元，负责管理大部分进出内存的数据流。</p>
<p><strong>Warp Scheduler</strong>（Warp 调度器）：SM 中的指令分发单元，warp 调度器将决定何时以及哪些指令被执行。</p>
<h4 id="wrap"><a class="markdownIt-Anchor" href="#wrap"></a> Wrap</h4>
<p><strong>一个 warp 是 32 个线程的集合。GPU 并非向单个线程发出指令，指令总是以 warp 为单位进行发布。因此，当发出一条指令时，根据定义一个线程束中的 32 个线程將执行相同的指令。</strong></p>
<p>若一个线程块被分配至SM，该线程块将逻辑上分解为一系列的 warp。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163144224.png" alt="image.png" /></p>
<h3 id="maxwell-pascal-架构"><a class="markdownIt-Anchor" href="#maxwell-pascal-架构"></a> Maxwell &amp; Pascal 架构</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163151335.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163158377.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163206360.png" alt="image.png" /></p>
<p><strong>CC6.1 SM 引入了名为 INT8 的新计算能力</strong>：在一个时钟周期内能执行四个 8 位宽的整数运算</p>
<h3 id="pascal-volta-架构"><a class="markdownIt-Anchor" href="#pascal-volta-架构"></a> Pascal &amp; Volta 架构</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163214641.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163220770.png" alt="image.png" /></p>
<p><strong>支持 FP16 的运算</strong>：每个 SP 单元（核心）能执行一个 32 位浮点运算或两个 16 位浮点运算</p>
<p>在此之前，我们讨论的是拥有大约 20 到 24 个 SM 的 GPU，而现在我们谈论的是 50 到 80 个 SM 的 GPU</p>
<p><strong>CC7.0 SM 引入了名为 TensorCore 的新计算能力</strong></p>
<h4 id="tensorcore"><a class="markdownIt-Anchor" href="#tensorcore"></a> TensorCore</h4>
<p>Tensor Core 的思路从系统设计上还是相当直接的，目前深度学习的 workload 中最主要的计算量都在矩阵的乘加上，因此为了专门去高效地支持这些 workload，就增加一些专用于矩阵运算的专用部件进去。</p>
<p><strong>Tensor Core 为 GPU 的原始计算能力带来了 5 至 10 倍的提升,特别是在 16 位浮点数的矩阵乘法运算模式下</strong></p>
<p>这也是常见的AI ASIC（如Google的TPU和其他厂商的各种xPU等）通常采用的策略，只不过 ASIC 可以从一开始就是针对特定的 workload 去的，因此设计上可以更直接更激进一些，直接上大量的 MMU（Matrix Multiply Unit），然后采用例如脉冲阵列这种设计去最大化它的 throughput。</p>
<p>而 NV 的 GPU 毕竟还要用作其他一些通用的运算，所以只能往原本的 SM 流水线里面插进去一些额外的专用部件 lane 了。开个脑洞，如果有一天发现除了FMA之外，还有其他形式的运算存在大量需求，未来的 GPU 设计里面说不定也会出现其他 xx Core。好在 FMA 除了深度学习以外在 HPC 的 workload 里面也是挺常见的，这个设计以后还是比较有用的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163229634.png" alt="image.png" /></p>
<p><a href="https://jcf94.com/download/2020-05-24-nvidia-arch-tensorcore1.png">Tensor Core 4x4 Matrix Multiply and Accumulate</a></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163237236.png" alt="image.png" /></p>
<p><a href="https://jcf94.com/download/2020-05-24-nvidia-arch-tensorcore2.png">Mixed Precision Multiply and Accumulate in Tensor Core</a></p>
<p>Tensor Core 这个部件直接从 SM 的寄存器里面取两个 FP16 的矩阵作为输入，进行全精度的矩阵乘之后得到的结果可以是 FP16 或者 FP32 的，然后累加到 FP16/FP32 的 accumulator 里面去。选择 FP16 作为输入数据类型，并输出为 FP32，这一做法可能旨在确保结果不会因溢出而受损，然后在加速部件设计等等方面做了一些 trade off。</p>
<h3 id="软件和硬件的映射关系"><a class="markdownIt-Anchor" href="#软件和硬件的映射关系"></a> 软件和硬件的映射关系</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163244836.png" alt="image.png" /></p>
<h2 id="launch-configuration"><a class="markdownIt-Anchor" href="#launch-configuration"></a> Launch Configuration</h2>
<p><strong>首先聚焦于两大性能目标中的第一个，即实现充分的并行性（尽可能多地使用线程）。但我们要搞明白为什么要尽可能多地使用线程。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163253901.png" alt="image.png" /></p>
<ul>
<li>
<p>如前所述，指令是按 warp 宽度发布的，并且也按顺序执行，因此GPU并非乱序执行的机器。若线程执行到某条指令时，发现下一条待执行的指令所需的 operand 尚未就绪，该线程便会 stall。operand 是指令的输入数据，比如进行乘法运算，所乘的两个数即为 operand。</p>
</li>
<li>
<p>延迟是试图在GPU中避免的问题，若仅使用单一线程，会因为等待数据出现而遇到停滞。在不同GPU架构中，全局内存延迟是所面临的最长延迟之一，通常为100个时钟周期以上，而算数延迟通常在10个时钟周期左右。</p>
</li>
<li>
<p>我们的具标在于启动足够多的线程以掩盖延迟</p>
</li>
</ul>
<h3 id="一个简单的程序"><a class="markdownIt-Anchor" href="#一个简单的程序"></a> 一个简单的程序</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163301686.png" alt="image.png" /></p>
<ul>
<li>
<p>Wrap 调度器在一个 Warp 因数据未准备好而 stall 时向下一个 Warp 发送指令，以尽可能保证每个 Clock Cycle 都是忙碌的。</p>
</li>
<li>
<p><strong>Warp 数量越多越能更好地隐藏延迟。</strong></p>
</li>
<li>
<p>假设一个 SM 有64个 warp，而一个 wrap 由32个线程组成，因此我们希望每个 SM 有2048个线程</p>
</li>
<li>
<p><strong>要最大化任何GPU的性能，关键在于使其饱和，即提供足够多的线程以使机器完全占用，从而为延迟隐藏创造最佳条件。</strong></p>
</li>
<li>
<p>具有双发射能力的 wrap scheduler：如果指令流中的两条相邻指令彼此之间没有依赖关系，那么wrap调度器有可能同时发出这两条指令。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163311797.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163318519.png" alt="image.png" /></p>
<h2 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h2>
<ul>
<li>
<p><a href="https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/">https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/</a></p>
</li>
<li>
<p><a href="https://cnblogs.com/upyun/p/17824106.html">https://cnblogs.com/upyun/p/17824106.html</a></p>
</li>
</ul>
<p>03-CUDA-Fundamental-Optimization-Part-1.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>04 Fundamental CUDA Optimization (Part 2)</title>
    <url>/2025/01/16/04-Fundamental-CUDA-Optimization-Part-2/</url>
    <content><![CDATA[<p><strong>这一章节将探讨第二个CUDA优化重点，即高效利用内存子系统（memory subsystems）</strong></p>
<h2 id="内存层级结构"><a class="markdownIt-Anchor" href="#内存层级结构"></a> 内存层级结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163332513.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163340975.png" alt="image.png" /></p>
<span id="more"></span>
<p>多图方便理解</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163348818.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163354805.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163400595.png" alt="image.png" /></p>
<h2 id="全局内存优化"><a class="markdownIt-Anchor" href="#全局内存优化"></a> <strong>全局内存优化</strong></h2>
<h3 id="ldst-操作"><a class="markdownIt-Anchor" href="#ldst-操作"></a> LD/ST 操作</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163406857.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163411994.png" alt="image.png" /></p>
<p>使用 Non-caching Load 操作的场景举例：当希望在一个线程写入数据，并使其对另一线程可见，而该线程可能运行在另一SM中时，这两个SM之间的L1缓存并不保证一致性。</p>
<h3 id="load-operation"><a class="markdownIt-Anchor" href="#load-operation"></a> Load Operation</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163422653.png" alt="image.png" /></p>
<ul>
<li>
<p>内存操作是按warp为单位发出的。</p>
</li>
<li>
<p>warp中的每个线程可能正在为该加载操作请求不同的内存地址</p>
</li>
<li>
<p>内存控制器的作用是，一旦指令发出，它將获取那些多达32个地址的模式，并确定需要哪些行或段。</p>
</li>
<li>
<p>从DRAM请求的数据并非单个字节、浮点数或其他类似形式，而是一个内存事务 (memory transaction)，所请求或存储的单位为32字节的段</p>
</li>
</ul>
<p><strong>四种 Caching Load 的情况</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163430150.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163437442.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163446716.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163453826.png" alt="image.png" /></p>
<p><strong>Non-Caching Load 的情况：由于cache line 大小从128字节变为32字节，增加了利用率</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163502578.png" alt="image.png" /></p>
<h3 id="全局内存优化准则"><a class="markdownIt-Anchor" href="#全局内存优化准则"></a> <strong>全局内存优化准则</strong></h3>
<p>基本思路是，我们希望一个 warp 尽可能在连续区域内进行访问</p>
<p>对于memory bound的代码，尽可能提高并发数来使总线利用率饱和</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163515541.png" alt="image.png" /></p>
<h2 id="共享内存优化"><a class="markdownIt-Anchor" href="#共享内存优化"></a> 共享内存优化</h2>
<p>共享内存是每个SM的资源，其功能一是提升性能，功能二是为线程块内的线程间通信提供了一种方式</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163553301.png" alt="image.png" /></p>
<h3 id="共享内存的结构"><a class="markdownIt-Anchor" href="#共享内存的结构"></a> 共享内存的结构</h3>
<p>可以将共享内存视为一个二维的内存数组，列是bank（存储体），每个bank的宽度是4 bytes，数组的行数是共享内存的大小（比如48KB）/每行的大小（128B</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163600875.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163609859.png" alt="image.png" /></p>
<p>最优访问（行式访问）：从 Bank 0-31各取4 byte</p>
<p>最差访问（列式访问）：从全部从 Bank 0 访问，完全串行化</p>
<h3 id="如何避免-bank-冲突"><a class="markdownIt-Anchor" href="#如何避免-bank-冲突"></a> 如何避免 Bank 冲突</h3>
<p><strong>在不改变物理结构的情况下，我们可以改变逻辑数组的结构，如果我们将32列的二维数组拓展为33列，则可以利用到共享内存的最佳性能</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163621621.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163627802.png" alt="image.png" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163633978.png" alt="image.png" /></p>
<ul>
<li>
<p>全局内存具有一种倾向于合并的访问模式，为了最大限度地提高全局内存的吞吐量，需使同一个 wrap 内线程的访问地址尽可能连续，相邻，成组。</p>
</li>
<li>
<p>当我们使用共享内存时，需要留意“bank冲突“</p>
</li>
<li>
<p>共享内存相较于全局内存在进行分散访问时，具有更多的灵活性</p>
</li>
</ul>
<p>04-CUDA-Fundamental-Optimization-Part-2.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>05 Atomics, Reductions, and Warp Shuffle</title>
    <url>/2025/01/16/05-Atomics-Reductions-and-Warp-Shuffle/</url>
    <content><![CDATA[<h2 id="归约问题该使用什么线程策略"><a class="markdownIt-Anchor" href="#归约问题该使用什么线程策略"></a> <strong>归约问题该使用什么线程策略？</strong></h2>
<h3 id="变换transformation问题和归约reduction问题"><a class="markdownIt-Anchor" href="#变换transformation问题和归约reduction问题"></a> 变换（Transformation）问题和归约（Reduction）问题</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163703880.png" alt="image.png" /></p>
<span id="more"></span>
<h3 id="朴素策略为每个输入点分配一个线程"><a class="markdownIt-Anchor" href="#朴素策略为每个输入点分配一个线程"></a> 朴素策略：为每个输入点分配一个线程</h3>
<p><strong>问题：cuda 不保证线程执行顺序，会出现 data race</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163711567.png" alt="image.png" /></p>
<h3 id="解决方法使用-built-in-的原子-read-modify-write-指令在这里是atomicadd"><a class="markdownIt-Anchor" href="#解决方法使用-built-in-的原子-read-modify-write-指令在这里是atomicadd"></a> 解决方法：使用 built-in 的原子 read-modify-write 指令，在这里是<code>atomicAdd</code></h3>
<ul>
<li>
<p>当线程发起原子指令时，L2 缓存中存在一个协调执行机制，可以将其视为一种协调者，它会逐一处理所有这些原子操作。</p>
</li>
<li>
<p>通过GPU内置的原子硬件，以性能换取可预测性和正确性</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163719716.png" alt="image.png" /></p>
<h3 id="其他原子操作"><a class="markdownIt-Anchor" href="#其他原子操作"></a> 其他原子操作</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163726438.png" alt="image.png" /></p>
<h3 id="原子操作的其他用途"><a class="markdownIt-Anchor" href="#原子操作的其他用途"></a> 原子操作的其他用途</h3>
<h4 id="每个线程获得队列中的下一项"><a class="markdownIt-Anchor" href="#每个线程获得队列中的下一项"></a> 每个线程获得队列中的下一项</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163733534.png" alt="image.png" /></p>
<h4 id="每个线程往一片共享内存中写入不定长连续的一段数据"><a class="markdownIt-Anchor" href="#每个线程往一片共享内存中写入不定长连续的一段数据"></a> 每个线程往一片共享内存中写入不定长，连续的一段数据</h4>
<p>可以用atomic加操作预定一个在共享内存中长为 dsize 的片段，atomicAdd操作返回的旧值即为在共享内存中预留空间的起始位置。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163741818.png" alt="image.png" /></p>
<h2 id="classical-parallel-reduction"><a class="markdownIt-Anchor" href="#classical-parallel-reduction"></a> Classical Parallel Reduction</h2>
<p>我们需要一个策略能够让我们利用大量的线程，尤其是在输入数据集大小能够支持的情况下，以规避原子操作的限制</p>
<h3 id="朴素策略基于树的思想"><a class="markdownIt-Anchor" href="#朴素策略基于树的思想"></a> 朴素策略：基于树的思想</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163748963.png" alt="image.png" /></p>
<p>上图方法的问题：例如11需要等待4和7的结果算出来才能计算，因此需要做同步操作</p>
<h3 id="解决方法全局同步操作"><a class="markdownIt-Anchor" href="#解决方法全局同步操作"></a> 解决方法：全局同步操作</h3>
<p>我们需要在整个网格中，跨越所有线程块实现这一功能。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163758817.png" alt="image.png" /></p>
<ul>
<li>方案一：分解为多个kernel</li>
</ul>
<p>一种可能的解决方案是将树分解，可以理解为将其分解为一系列kernel，每个树级对应一个kernel启动。对于gpu来说，kernel的执行顺序是确定的，即如果你按顺序执行两个kernel，那第二个kernel不会在第一个kernel执行完之前执行。这样，内核启动的边界为我们提供了全局同步。</p>
<p>问题：</p>
<ul>
<li>
<ul>
<li>
<p>内核存在启动开销</p>
<ul>
<li>树的底部所对应的 grid 太小了</li>
</ul>
</li>
</ul>
</li>
<li>
<p>方案二：线程块耗尽方法</p>
</li>
</ul>
<p>另一种解决方案是使用原子操作，原子操作能确定最后一个修改的线程，一旦我们确定了最后一个完成的线程块，我们就可以将额外的工作分配给该线程块，因为我们知道其他所有线程块均已完成。</p>
<ul>
<li>方案三：协同组（cooperative groups）</li>
</ul>
<p>协同组是CUDA编程模型中较新的特性，它解决CUDA在提供线程协作和线程块分解方面的构造和原语不够丰富的问题，协同组提供了一套新的内置函数和基本组件，使我们能够构建规模更为灵活的线程组，这些线程组能够协同工作，共同执行任务。</p>
<h4 id="方案一的示例这里使用一个kernel来避免启动内核的开销"><a class="markdownIt-Anchor" href="#方案一的示例这里使用一个kernel来避免启动内核的开销"></a> 方案一的示例，这里使用一个kernel来避免启动内核的开销</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163806867.png" alt="image.png" /></p>
<h4 id="grid-stride-loops"><a class="markdownIt-Anchor" href="#grid-stride-loops"></a> Grid-Stride Loops</h4>
<p>这里的构想和目标在于我们希望创建的 kernel 函数能够实现一种解耦，即 kernel 函数的规模(换言之，网格的大小，即执行操作的线程数量)与数据集大小之间的解耦。即我们希望能够编写并启动一个具有特定线程配置的 kernel，使其能够正确执行操作，无论输入数据集是小于、等于还是大于所使用的线程数量。</p>
<p>我们的想法是拥有固定数量的线程，数量等于 grid 的宽度，这些线程最初将处理输入数据集中与网格宽度相对应的大小，处理完毕后，它们将跨步前进，将一个索引值</p>
<p>加入操作中，然后利用下一块数据重复该操作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163812754.png" alt="image.png" /></p>
<h4 id="方案一的完整实现"><a class="markdownIt-Anchor" href="#方案一的完整实现"></a> 方案一的完整实现</h4>
<p>优化：每个 block 仅最后执行一次原子操作，从而避免了启动第二个 kernel 对 out 里每个 block 产出的归约结果进行二次归约</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163823849.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163829778.png" alt="image.png" /></p>
<h2 id="wrap-shuffle"><a class="markdownIt-Anchor" href="#wrap-shuffle"></a> Wrap Shuffle</h2>
<p>共享内存是实现线程间数据通信的途径。为了以这种方式使用共享内存，需要进行两个操作。一个线程需向共享内存写入数据，另一个线程需要从共享内存中的该位置读取数据。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163836837.png" alt="image.png" /></p>
<p>如果有一种方法可以直接在不同线程间传递数据，而不使用共享内存，那岂不是很好？</p>
<p><strong>Warp Shuffle背后的理念是，它允许 warp 内部实现这种直接的线程间通信。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163844387.png" alt="image.png" /></p>
<p>例如<code>__shfl_down_sync()</code>这个函数是将被 mask 指定的线程A，再会往后偏移 delta，这样得出来的线程B，线程A会拿到线程B的var，其他如果偏移过头的线程会返回0。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163852373.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163858917.png" alt="image.png" /></p>
<h3 id="修改之前的归约代码"><a class="markdownIt-Anchor" href="#修改之前的归约代码"></a> 修改之前的归约代码</h3>
<ul>
<li>
<p>现在每个线程不再将运行总和保存在共享内存中，而是将其作为名为val的局部变量保存。</p>
</li>
<li>
<p>mask 表示我们的洗牌操作需要利用整个 warp 中的所有32个线程</p>
</li>
<li>
<p>Lane 指的是在 warp 中处于哪个线程，WarpID 指的是处于哪个 wrap</p>
</li>
<li>
<p>1^st warp-shuffle reduction：初始 offset 设为 warpSize 的一半（即16）。在第一轮循环中，这意味着对于线程零，它将取其自身的 val 值，并将其与来自线程束大小的一半（即16）的 val值相加。最后每个 warp 算出的归约结果将存在 sdata 中。一个 block 中最多有1024个线程，因此最多有32个 wrap。</p>
</li>
<li>
<p>我们必须确保每个 warp 都完成了该操作然后才能允许任何进程继续进行，因此使用<code>__Syncthreads()</code>同步。</p>
</li>
<li>
<p>2^st warp-shuffle reduction：接下来执行另一次 warp 洗牌操作，但仅需使用第0个 warp, 因为现在共享内存中最多只有32个项</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163906176.png" alt="image.png" /></p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163914434.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163922813.png" alt="image.png" /></p>
<h2 id="loop-unrolling"><a class="markdownIt-Anchor" href="#loop-unrolling"></a> Loop Unrolling</h2>
<p>循环展开的原理是，对于一个已知范围的循环，例如 for 循环，编译器能够确定其范围，通常会自动为你展开这个循环。这意味着将循环的每一次迭代按顺序展开，因此，你正在摒弃循环，而是将循环的每次迭代按顺序编入代码中。</p>
<p>这仅仅是优化的第一步，编译器希望这样做的原因之一是，优化过程的下一步通常涉及指令重排。编译器会在保持代码正确性的同时，积极地重新排序操作，以提供最佳性能。我们希望实现的是尽可能立即启动所有内存操作，以便它们能迅速进入队列，从而使内存子系统尽早工作起来。越早将内存子系统加载请求，数据返回得就越快，计算结果的速度也会随之提升。</p>
<p>如果我在循环中每次迭代都从全局内存加载数据，那么在第10次循环迭代时，如果仅遵循循环语义，我将加载第10次循环所需要的数据。但是经常发生的情况是加载操作是独立的，那么就没有理由不能立即进行加载，立即启动该加载操作，然后按照可完成顺序执行我能进行的操作。</p>
<p>通常情况下，了解循环的总次数，即遍历范围，是很有必要的。若能编写具有可识别循环次数的循环，编译器更可能对其进行积极展开。</p>
<p>05_Atomics_Reductions_Warp_Shuffle.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>06 Concurrency (Cuda Stream)</title>
    <url>/2025/01/17/06-Concurrency-Cuda-Stream/</url>
    <content><![CDATA[<h1 id="concurrency-motivation"><a class="markdownIt-Anchor" href="#concurrency-motivation"></a> Concurrency &amp; Motivation</h1>
<h2 id="concurrency"><a class="markdownIt-Anchor" href="#concurrency"></a> Concurrency</h2>
<p>并发性在解锁新的计算能力或新的算法解决方案并没有太多作为。<strong>并发性的真正意义主要在于，用系统中存在多个处理器时所提供的机会，最大限度地发挥所有这些处理器的性能。</strong></p>
<h2 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h2>
<p>在第一讲中，我们了解到基本的CUDA编程模型，将其称为三步序列。<br />
在串行化的执行中,   我们程序的总执行时间，是第一次 CUDA内存复制操作、内核持续时间以及第二次CUDA内存复制操作之和<br />
于是Motivation显而易见了, 我们希望将这些操作重叠起来<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116171213159.png" alt="image.png" /></p>
<span id="more"></span>
<h1 id="pinned-non-pageable-memory"><a class="markdownIt-Anchor" href="#pinned-non-pageable-memory"></a> PINNED (NON-PAGEABLE) MEMORY</h1>
<h2 id="pinned-memory-enables"><a class="markdownIt-Anchor" href="#pinned-memory-enables"></a> Pinned memory enables:</h2>
<ul>
<li>Faster Host ↔ Device copies</li>
<li>Memcopies asynchronous with CPU</li>
<li>Memcopies asynchronous with GPU</li>
</ul>
<p>现代操作系统如Linux或Windows 会将虚拟内存空间的概念与物理内存（即系统RAM,用于存储程序或系统正在使用的数据）分离. 这种空间的划分通常会引入一种称为分页的机制，它实质上就是将<br />
各自的区域: 虚拟地址空间和物理RAM一一分割成固定大小的块，通常是固定大小，如4K字节。</p>
<p>这种方法的一个缺点是，当尝试访问数据时，这些数据可能并不一定实际存在于RAM中。操作系统必须介入并执行一系列操作，最终将数据引入RAM, 此时程序方能继续执行, 这可能成为性能瓶颈。</p>
<p>在探讨GPU或CUDA中的并发性时，我们希望同时进行的主要操作，即希望允许同时发生的操作，包括内存复制(MemCopy）操作和内核（kernel）操作。</p>
<p>我们将启用名为复制引擎的功能，使其能够异步独立运行，实现数据从一处到另一处的转移。他们要求数据必须实际存在于物理主机的RAM内存中。因此，我们不能采用这种分页系统，让部分数据被“分页”出去。</p>
<hr />
<h2 id="usage"><a class="markdownIt-Anchor" href="#usage"></a> Usage</h2>
<ul>
<li><strong><code>cudaHostAlloc</code></strong> / <strong><code>cudaFreeHost</code></strong>
<ul>
<li>Instead of <code>malloc</code> / <code>free</code> or <code>new</code> / <code>delete</code></li>
</ul>
</li>
<li><strong><code>cudaHostRegister</code></strong> / <strong><code>cudaHostUnregister</code></strong>
<ul>
<li>Pin regular memory (e.g. allocated with <code>malloc</code>) after allocation</li>
</ul>
</li>
</ul>
<p>这个功能由 CUDA API <code>cudaHostAlloc</code>实现, 将以类似 malloc 的方式工作, 获取分配的指针将指向固定内存，固定意味着分配的页面在物理上驻留在RAM中的CPU主机内存里，并且它们不会移动。</p>
<p>这种内存固定操作的一个缺点，也是我们并非始终使用它 的原因，在于它会干扰分页虚拟内存系统</p>
<p>另一个 CUDA API <code>cudaHostRegister</code> 可以对由诸如<code>mallac</code>分配的可分页分配的内存进行固定</p>
<hr />
<h2 id="implication"><a class="markdownIt-Anchor" href="#implication"></a> Implication:</h2>
<ul>
<li>Pinned memory is essentially removed from host virtual (pageable) memory</li>
</ul>
<h1 id="cuda-streams"><a class="markdownIt-Anchor" href="#cuda-streams"></a> CUDA STREAMS</h1>
<h2 id="streams-and-async-api-overview"><a class="markdownIt-Anchor" href="#streams-and-async-api-overview"></a> STREAMS AND ASYNC API OVERVIEW</h2>
<h3 id="default-api"><a class="markdownIt-Anchor" href="#default-api"></a> Default API:</h3>
<ul>
<li>Kernel launches are asynchronous with CPU</li>
<li><code>cudaMemcpy</code> (D2H, H2D) block CPU thread</li>
<li>CUDA calls are serialized by the driver (legacy default stream)</li>
</ul>
<p><strong>Stream 是CUDA内部的一种机制、API,或一系列API, 用于组织并发性。它使我们能够向CUDA运行时指定我们希望按照什么顺序执行哪些操作。</strong></p>
<p>正如我们之前几节课中所学，<strong>内核启动是异步于CPU执行的</strong>。这意味着CPU线程在启动内核时，并不会等待内核完成执行。然而，<strong>CUDA内存拷贝操作是阻塞式的</strong>。当CPU线程遇到CUDA内存复制操作时，语义上要求所有先前发出的CUDA活动必须完成，然后该CUDA内存复制操作才会开始, 同时, 在CPU线程从CUDA内存复制操作中释放之前，该CUDA内存复制操作必须完成。</p>
<h3 id="streams-and-async-functions-provide"><a class="markdownIt-Anchor" href="#streams-and-async-functions-provide"></a> Streams and async functions provide:</h3>
<ul>
<li><code>cudaMemcpyAsync</code> (D2H, H2D) asynchronous with CPU</li>
<li>Ability to concurrently execute a kernel and a memcpy</li>
<li>Concurrent copies in both directions (D2H, H2D) possible on most GPUs</li>
</ul>
<p>CUDA API <code>cudaMemcpyAsync</code>这个操作与CUDA <code>cudaMemcpy</code> 功能相同，只是它额外接收了一个流参数。通过这种方式，它使得内存拷贝得以异步执行</p>
<p>当我们讨论两个 CUDA memcopy异步操作时，要在时间线上同时执行它们，通常在GPU上唯一可能的方式是，一个操作的方向与另一个相反。同向并发是一个更为晦涩的主题，目前我们对此并不十分关注。</p>
<h3 id="stream-sequence-of-operations-that-execute-in-issue-order-on-gpu"><a class="markdownIt-Anchor" href="#stream-sequence-of-operations-that-execute-in-issue-order-on-gpu"></a> Stream = sequence of operations that execute in issue-order on GPU</h3>
<ul>
<li>Operations from different streams may be interleaved</li>
<li>A kernel and memcpy from different streams can be overlapped</li>
</ul>
<p>流是在GPU上按发出顺序执行的一系列操作。因此，为了实现重叠，我们不能将两个操作同时发送到同一个流中。</p>
<h2 id="stream-semantics"><a class="markdownIt-Anchor" href="#stream-semantics"></a> STREAM SEMANTICS</h2>
<ol>
<li><strong>向同一流发出的两个操作将按照发出顺序执行。</strong> 若操作A在B之前被发出，即在代码中出现在B的发出之前，那么当A和B被发送到同一流中时，它们将按照发出顺序执行。A保证会在B开始之前完成。</li>
<li><strong>向两个不同流发出的操作之间并无预设的顺序关系。</strong></li>
</ol>
<h2 id="stream-creation-and-copycompute-overlap"><a class="markdownIt-Anchor" href="#stream-creation-and-copycompute-overlap"></a> STREAM CREATION AND COPY/COMPUTE OVERLAP</h2>
<p>前提条件:</p>
<ol>
<li>若希望进行一项可重叠的CUDA内存异步复制操作，该操作能与另一操作同时调度并行运行。其中指向主机内存的指针必须指向 pinned memory</li>
<li>这两个操作，假设是一个内核和一个内存复制，必须分别在不同的非零流中发出。</li>
</ol>
<p>代码示例</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cudaStream_t stream1, stream2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明流之后我们需要创建流, 这本质上是在向CUDA运行时发出信号，它类似于管理GPU的操作系统。</span></span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream1); </span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// potentially overlapped with kernel</span></span><br><span class="line"><span class="built_in">cudaMemcpyAsync</span>( dst, src, size, dir, stream1 ); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于尖括号里的stream参数, 如果我们省略该参数，正如我们一直所做的那样，它会将内核启动到空流/stream 0</span></span><br><span class="line">kernel&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream2&gt;&gt;&gt;(..); </span><br><span class="line"></span><br><span class="line"><span class="comment">// test if stream is idle</span></span><br><span class="line"><span class="built_in">cudastreamQuery</span>(stream1);    </span><br><span class="line"></span><br><span class="line"><span class="comment">// force CPU thread to wait. 它表示需等待此流中发出的所有CUDA活动完成为止。</span></span><br><span class="line"><span class="built_in">cudastreamsynchronize</span>(stream2); </span><br><span class="line"></span><br><span class="line"><span class="built_in">cudastreamDestroy</span>(stream2);</span><br></pre></td></tr></table></figure>
<h2 id="example-stream-behavior-for-vector-math"><a class="markdownIt-Anchor" href="#example-stream-behavior-for-vector-math"></a> EXAMPLE STREAM BEHAVIOR FOR VECTOR MATH</h2>
<p>假设我们针对一个长向量执行一种可分离的数学运算。比方说, 我可以对向量元素1至10进行操作，而不涉及向量元素11至20或其他任何元素。<br />
<strong>核心思路</strong><br />
我们将把向量运算分解成多个小块，逐块传输，并在传输完每一块后，启动一个较小的内核进行处理。它仅对数据的一小部分进行操作。然后，我们将把那部分的结果复制回来。<br />
我们希望实现的是利用流，使得大量绿色操作能与大量橙色操作重叠，同时也能与大量蓝色操作重叠。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116202912010.png" alt="image.png" /></p>
<h2 id="default-stream"><a class="markdownIt-Anchor" href="#default-stream"></a> DEFAULT STREAM</h2>
<p>存在一个默认流，当你未使用显式流或创建的流时，即在使用默认流</p>
<p>默认流将同步执行，这意味着它会强制所有先前发出的CUDA活动（无论其在哪个流中发出）完成，然后默认流发出的项才会开始执行。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116205125218.png" alt="image.png" /></p>
<h2 id="cudalaunchhostfunc-stream-callbacks"><a class="markdownIt-Anchor" href="#cudalaunchhostfunc-stream-callbacks"></a> CudaLaunchHostFunc() (STREAM “CALLBACKS”)</h2>
<ul>
<li>
<p>允许定义一个将在 CUDA 流中执行的主机代码函数。</p>
</li>
<li>
<p>遵循流语义：函数将在流的执行到达该点时才会被调用。</p>
</li>
<li>
<p>使用由 GPU 驱动程序生成的线程来执行工作。</p>
</li>
<li>
<p>有一定限制：不要在函数中使用任何 CUDA 运行时 API 调用（或内核启动）。</p>
</li>
<li>
<p>适用于延迟 CPU 工作直到 GPU 结果准备就绪的场景。</p>
</li>
</ul>
<h2 id="cudaevent"><a class="markdownIt-Anchor" href="#cudaevent"></a> CUDAEVENT</h2>
<p><strong>cudaEvent</strong> 可以作为一种流中的“标记”</p>
<ul>
<li>当一个 <strong>cudaEvent</strong> 被发出时，被称为已“记录”（<code>recorded</code>）。</li>
<li>当流执行到达其被记录的位置时，<strong>cudaEvent</strong> 被称为已“完成”（<code>completed</code>）。</li>
</ul>
<p><strong>最常见用途：</strong> 用于计时。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">cudaEvent_t start, stop;                   <span class="comment">// cudaEvent 有自己特定的类型</span></span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);                   <span class="comment">// 必须在使用前创建</span></span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);                    <span class="comment">// 同样为停止事件创建</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start);                    <span class="comment">// 将 “记录” (发出) 放入默认流</span></span><br><span class="line">Kernel&lt;&lt;&lt;b, t&gt;&gt;&gt;(...);                     <span class="comment">// 可用于任何 CUDA 设备上的活动</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop);                     <span class="comment">// 将停止事件记录到流中</span></span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);                <span class="comment">// 等待流执行到达 &quot;stop&quot; 事件</span></span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;float_var, start, stop);  <span class="comment">// 计算 kernel 持续时间</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>cudaEvent</strong> 也可用于安排复杂的并发场景。</p>
<p>基于事件的计时可能会对主机活动或复杂并发场景产生意外结果。</p>
<h1 id="multi-gpu"><a class="markdownIt-Anchor" href="#multi-gpu"></a> MULTI -GPU</h1>
<h2 id="device-management"><a class="markdownIt-Anchor" href="#device-management"></a> DEVICE MANAGEMENT</h2>
<ul>
<li>
<p><strong>不是</strong> OpenMP、MPI 等的替代品。</p>
</li>
<li>
<p><strong>应用程序可以查询和选择 GPU：</strong></p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cudaGetDeviceCount</span>(<span class="type">int</span> *count);  <span class="comment">// 获取可用 GPU 数量</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="type">int</span> device);       <span class="comment">// 设置当前使用的 GPU</span></span><br><span class="line"><span class="built_in">cudaGetDevice</span>(<span class="type">int</span> *device);      <span class="comment">// 获取当前正在使用的 GPU</span></span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(cudaDeviceProp *prop, <span class="type">int</span> device);  <span class="comment">// 获取指定 GPU 的属性</span></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><strong>多个主机线程可以共享一个设备（GPU）。</strong></p>
</li>
<li>
<p><strong>一个主机线程可以管理多个设备：</strong></p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(i); <span class="comment">// 选择当前设备 </span></span><br><span class="line"><span class="built_in">cudaMemcpyPeerAsync</span>(...); <span class="comment">// 用于设备间的点对点内存拷贝	</span></span><br></pre></td></tr></table></figure>
<h2 id="streams"><a class="markdownIt-Anchor" href="#streams"></a> STREAMS</h2>
<p><strong>流（Streams）和 cudaEvent 具有隐式/自动的设备关联：</strong></p>
<ul>
<li>每个设备也都有其独特的默认流。</li>
<li>如果内核启动被发往一个未与当前设备关联的流中，则会失败。</li>
<li><code>cudaStreamWaitEvent()</code> 可用于同步属于不同设备的流；<code>cudaEventQuery()</code> 可测试某个事件是否已“完成”。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);                     <span class="comment">// 设置为设备 0</span></span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream0);           <span class="comment">// 创建与设备 0 关联的流</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);                     <span class="comment">// 设置为设备 1</span></span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream1);           <span class="comment">// 创建与设备 1 关联的流</span></span><br><span class="line">Kernel&lt;&lt;&lt;b, t, <span class="number">0</span>, stream1&gt;&gt;&gt;(…);      <span class="comment">// 在设备 1 上执行内核</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);                     <span class="comment">// 再次切换到设备 0</span></span><br><span class="line">Kernel&lt;&lt;&lt;b, t, <span class="number">0</span>, stream0&gt;&gt;&gt;(…);      <span class="comment">// 在设备 0 上执行内核</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="device-to-device-data-copying"><a class="markdownIt-Anchor" href="#device-to-device-data-copying"></a> DEVICE-TO-DEVICE DATA COPYING</h2>
<ul>
<li>
<p>如果系统拓扑支持，可以通过高速互联（如 PCIe 或 NVLink）直接将数据从一个设备复制到另一个设备。</p>
</li>
<li>
<p>必须先显式地将设备放入对等关系（“clique”）。</p>
</li>
<li>
<p>如果需要，必须为数据传输的双向操作启用“对等访问（peering）”。</p>
</li>
<li>
<p>此后，这些设备之间的内存拷贝将不会通过系统内存缓冲区（利用 GPUDirect 的 P2P 传输）。</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaDeviceCanAccessPeer</span>(&amp;canPeer, <span class="number">0</span>, <span class="number">1</span>);         <span class="comment">// 检查设备 0 和设备 1 是否支持对等访问</span></span><br><span class="line"><span class="built_in">cudaDeviceEnablePeerAccess</span>(<span class="number">1</span>, <span class="number">0</span>);               <span class="comment">// 设备 0 将设备 1 视为“对等”</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);</span><br><span class="line"><span class="built_in">cudaDeviceEnablePeerAccess</span>(<span class="number">0</span>, <span class="number">0</span>);               <span class="comment">// 设备 1 将设备 0 视为“对等”</span></span><br><span class="line"><span class="built_in">cudaMemcpyPeerAsync</span>(dst_ptr, <span class="number">0</span>, src_ptr, <span class="number">1</span>, size, stream0);  <span class="comment">// 从设备 1 到设备 0 的拷贝</span></span><br><span class="line"><span class="built_in">cudaDeviceDisablePeerAccess</span>(<span class="number">0</span>);                 <span class="comment">// 设备 0 不再是设备 1 的对等设备</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="other-concurrency-scenarios"><a class="markdownIt-Anchor" href="#other-concurrency-scenarios"></a> OTHER CONCURRENCY SCENARIOS</h2>
<p><strong>主机/设备并发执行：</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">Kernel&lt;&lt;&lt;b, t&gt;&gt;&gt;(…);    <span class="comment">// 此内核执行可以与以下主机代码重叠</span></span><br><span class="line"><span class="built_in">cpuFunction</span>(…);          <span class="comment">// 此主机代码</span></span><br></pre></td></tr></table></figure>
<p><strong>并发内核：</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">Kernel&lt;&lt;&lt;b, t, <span class="number">0</span>, streamA&gt;&gt;&gt;(…);    <span class="comment">// 这些内核有可能并发执行</span></span><br><span class="line">Kernel&lt;&lt;&lt;b, t, <span class="number">0</span>, streamB&gt;&gt;&gt;(…);</span><br></pre></td></tr></table></figure>
<p><strong>说明：</strong></p>
<ul>
<li><strong>在实际中，同一设备上的并发内核执行较难实现</strong>。</li>
<li>需要内核具备相对较低的资源使用率和相对较长的执行时间。</li>
<li>每个设备的并发内核数量存在硬件限制。</li>
<li>与利用单个内核完全占满设备相比，并发内核执行效率较低。</li>
</ul>
<h2 id="stream-priority"><a class="markdownIt-Anchor" href="#stream-priority"></a> STREAM PRIORITY</h2>
<ul>
<li>
<p><strong>CUDA 流</strong>允许定义可选的优先级（<code>priority</code>）。</p>
</li>
<li>
<p>这仅影响<strong>并发内核</strong>的执行。</p>
</li>
<li>
<p><strong>GPU 块调度器</strong>会尝试优先调度高优先级流（<code>stream</code>）内核的块，而不是低优先级流内核的块。</p>
</li>
<li>
<p>当前实现中只有<strong>两个优先级</strong>。</p>
</li>
<li>
<p>当前实现<strong>不会导致块的抢占（preemption）</strong>。</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取当前设备支持的流优先级范围</span></span><br><span class="line"><span class="type">int</span> priority_high, priority_low;</span><br><span class="line"><span class="built_in">cudaDeviceGetStreamPriorityRange</span>(&amp;priority_low, &amp;priority_high);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建具有最高和最低优先级的流</span></span><br><span class="line">cudaStream_t st_high, st_low;</span><br><span class="line"><span class="built_in">cudaStreamCreateWithPriority</span>(&amp;st_high, cudaStreamNonBlocking, priority_high);</span><br><span class="line"><span class="built_in">cudaStreamCreateWithPriority</span>(&amp;st_low, cudaStreamNonBlocking, priority_low);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>ALBEF</title>
    <url>/2025/01/16/ALBEF/</url>
    <content><![CDATA[<p>ALBEF来自于Align before Fuse，作者团队全自来自于Salesforce Research。</p>
<p>论文地址：<a href="https://arxiv.org/pdf/2107.07651.pdf">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a></p>
<p>论文代码：<a href="https://github.com/salesforce/ALBEF">ALBEF</a></p>
<h1 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h1>
<p>最近图像文本的大规模的特征学习非常火爆，大部分已有的方法都是用一个Transformer模型作为多模态的一个编码器，同时编码视觉的Token和文本的Token，视觉Token就是视觉特征，一般是region-based的图像特征。</p>
<p><strong>本文贡献1–ALign BEfore Fuse：</strong></p>
<p>ViLT和ALBEF都认为不需要目标检测的模型，但ViLT只是说用了目标检测模型以后速度太慢，希望推理时间变得更快，但是ALBEF分析认为<strong>使用预训练的目标检测器之后，视觉特征和文本特征无法align对齐</strong>，因为目标检测器是提前训练好的，只抽取特征，没有再进行end-to-end的训练，所以导致视觉特征和文本特征可能相隔很远，此时将两个特征同时送入多模态的编码器之后，编码器学习图像文本之间的交互信息就会变得很有挑战。<strong>作者提出对比学习的ITC Loss，将图像和文本在Fusing之前实现Align，也就是他们论文的题目ALign BEfore Fuse。</strong></p>
<p><strong>本文贡献2–自训练方式学习：</strong></p>
<p>网上爬取的数据特别noisy，因为图片文本对里面的文本通常具备搜索属性，但不是好的描述性句子，甚至没有描述，这种noisy web data的问题影响模型有效地学习文本图像特征，所以作者<strong>使用Momentum Distillation也就是自训练的方式去学习</strong>，自训练就是用Pseudo Label伪标签去学习。<strong>作者采用Moco提出的Momentum Encoder的形式生成Pseudo Target，从而达到一个自训练的效果。</strong></p>
<p>在图文检索的任务上，ALBEF的效果最好，性能反超之前的方法，尤其是在特别大的数据上训练过的模型CLIP和Align。在VQA和VR任务上，ALBEF也比之前state of art提升2.37%、3.84%的准确度，而且推理时间也更快。模型在4 million的训练数据集下，能做到一个8卡A100训练三四天的时间，训练门槛大大降低。</p>
<span id="more"></span>
<h1 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h1>
<p>参考 ViLT 综述部分</p>
<h1 id="albef方法"><a class="markdownIt-Anchor" href="#albef方法"></a> ALBEF方法</h1>
<h2 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160941204.png" alt="image.png" /></p>
<p>**图像端：**给定任何一张图片，按照Vision Transformer的做法，把它打成patch，然后通过patch embedding layer送给一个Vision Transformer。这里是一个非常标准的12层的Vision Transformer的base模型，如果图片是224x224，那这里的sequence length就是196，然后加上额外的一个CLS token就是197，它的特征维度是768，所以这里绿黄色的特征就是197乘以768。但论文在预训练阶段用的图片是256x256，所以这里绿色的sequence length就会相应的再长一些。它的预训练参数用的DEiT，也就是Data Efficient Vision Transformer在ImageNet 1K数据集上训练出来的初始化参数。</p>
<p>**文本端：**文本端为保持计算量与clip类似，并且增强模态融合的部分，用前六层做文本编码，剩下的六层transformer encoder作为multi-model fusion的过程。文本模型用BERT模型做初始化，它中间的特征维度也是768，它也有一个CLS token代表了整个句子的文本信息。</p>
<p>**momentum model：**ALBEF为了做momentum distillation，而且为了给ITC loss提供更多negative，增加了momentum model，这个模型参数由左边训练的模型参数通过moving average得到的（和MoCo一模一样），通过把moving average的参数设的非常高（论文里是0.995）来保证momentum model不会那么快更新，产生的特征更加稳定，不仅可以做更稳定的negative sample，而且还可以去做momentum distillation。</p>
<h2 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h2>
<ul>
<li>
<p><strong>ITC loss</strong>: 对比学习就是首先定义一个正样本对，然后定义很多负样本对，对比使正样本对之间的距离越来越近，正负样本对之间的距离越来越远。在ALBEF里首先将图像I通过vision transformer之后得到图像的全局特征，图中黄色CLS token作为全局特征，也就是一个768×1的向量，文本这边先做tokenize，将文本text变成tokenization的序列，再输入BERT的前六层，得到了一系列的特征，文本端的CLS token作为文本的全局特征，也是一个768×1的向量。接下来与MoCo相同，图像的特征向量先做一下downsample和normalization，将768×1变成了256×1的向量，文本特征向量也是768变成256×1。正样本这两个特征距离尽可能的近，它的负样本全都存在一个q里，含有65536个负样本（因为它由momentum model产生的，没有gradient，所以它并不占很多内存），正负样本之间的对比学习，使得这两个特征距离尽可能的远。这个过程就是align before fuse的align，也就是说在图像特征和这个文本特征输入Multi-model Fusion的encoder之前，就已经通过对比学习的ITC loss让这个图像特征和文本特征尽可能的拉近，在同一个embedding space里，具体使用了cross entropy loss。</p>
</li>
<li>
<p><strong>ITM loss：Image Text Matching</strong>，属于一个二分类任务，就是给定一个图片，给定一个文本，图像文本通过ALBEF的模型之后输出一个特征，在这个特征之后加一个分类头，也就是一个FC层，然后去判断I和T是不是一个对，</p>
</li>
</ul>
<p>这个loss虽然很合理，但是实际操作的时候发现这个loss太简单，所以这个分类任务，很快它的准确度就提升得很高无法进一步优化。因此ALBEF通过某种方式选择最难的负样本（最接近于正样本的那个负样本），具体来说ALBEF的batch size是512，所以ITM loss正样本对就是512个，对于mini batch里的每一张图像，把这张图片和同一个batch里所有的文本都算一遍cos similarity，然后它在这里选择除了它自己之外相似度最高的文本当做negative，这样ITM loss就非常有难度</p>
<ul>
<li><strong>MLM：Mask Language Modeling</strong>，它把原来完整的句子text T变成一个T’，也就是有些单词被mask掉了, 然后它把缺失的句子和图片一起通过ALBEF的模型，最后把之前的完整的句子给预测出来，它这里也借助了图像的信息去更好的恢复被mask掉的单词。</li>
</ul>
<p>计算ITC loss和ITM loss的时候，输入都是原始的i和t，但是当计算MLM loss的时候，它的输入是原始的i和mask后的T’，所以模型每一个训练的iteration都做了两次模型的forward：一次模型的forward用了原始的i和t，另一次模型的forward用了原始的i和mask后的T’。为了计算不同的loss，这种多次前项在多模态学习中使用普遍。</p>
<p><strong>ALBEF的目标函数:</strong>$\mathcal{L} = \mathcal{L}<em>{itc} + \mathcal{L}</em>{mlm} + \mathcal{L}_{itm} $，也就是itc、mlm和itm的合体</p>
<h2 id="momentum-distillation-动量蒸馏"><a class="markdownIt-Anchor" href="#momentum-distillation-动量蒸馏"></a> momentum distillation 动量蒸馏</h2>
<p><strong>动机：</strong></p>
<p>由于从网上爬下来的数据噪声严重，图像文本对经常是弱相关，甚至不匹配，这种noisy的data会导致计算目标函数时有偏差。比如在算ITC的时候，某个负样本文本很有可能也描述了图像里的很多内容，它可能不是爬下来的image text pair，但文本甚至可能比Ground Truth描述的还好。若此时将其作为一个负样本，就会对ITC的学习造成很大的影响。</p>
<p><strong>改进方式：</strong></p>
<p>作者认为one hot label（就是图片和文本就是一对，其他跟它都不是一对）对于ITC和MLM这两个loss来说不好，因为有的负样本也包含了很多的信息。所以作者采取了自训练方式，先构建一个momentum model然后用这个动量模型去生成pseudo targets伪目标（其实就是一个softmax score），这样它就不再是一个one hot label。</p>
<p>动量模型在已有模型之上做exponential moving average EMA，目的是在原始模型训练的时候，不仅希望模型预测与ground truth的one hot label去尽可能的接近，还希望模型预测与动量模型出来的pseudo targets尽可能的匹配，这样就能达到一个比较好的折中点。因为当one hot label正确时，可以学习到很多信息，但当one hot label是错误的，或者是noisy的时候，作者希望稳定的momentum model能够提供一些改进。</p>
<p>以ITC loss为例，它是基于这个one hot label的，所以这里再算一个pseudo target loss去弥补它的一些缺陷和不足。这个loss跟前面equation1里的ITC loss的不同，这里将ground truth换成这个q，就是pseudo targets，q不再是one hot label，而是softmax score，所以这里计算KL divergence而不是cross entropy。</p>
<p><strong>最终结果：</strong></p>
<p>最终ALBEF的训练loss有五个：两个ITC、两个MLM、一个ITM，其中：</p>
<ul>
<li>
<p>ITC有两个loss：一个是原始的ITC，一个是基于pseudo target的ITC，所以分别加权（1-α）和α的loss weight，最终得到momentum版本的ITC loss。</p>
</li>
<li>
<p>MLM loss有两个loss：一个是原始的MLM，一个是基于pseudo target的MLM，用新生成的pseudo target去代替了原来的ground truth，分别加权（1-α）和α的loss weight，最终得到momentum版本的MLM loss。</p>
</li>
<li>
<p>ITM有一个loss：ITM没有动量版本，因为本身它就是基于ground truth，它就是一个二分类任务，而且在ITM里又做了hard negative，这跟momentum model有冲突。</p>
</li>
</ul>
<h1 id="下游vision-language任务"><a class="markdownIt-Anchor" href="#下游vision-language任务"></a> 下游Vision Language任务</h1>
<p>ALBEF这篇论文做了五个任务：</p>
<ol>
<li><strong>图文检索（图像到文本、文本到图像、图像到图像、文本到文本）：</strong></li>
</ol>
<p>定义：给定一个数据库，去搜索Ground Truth的图像文本对。</p>
<p>衡量指标：因为是检索，衡量的指标就是这个Recall 召回率，一般用的是R1,R5,R10，就是说在你检索回来的一个、五个或者十个Sample里是否有Ground Truth Sample，有就算正确。</p>
<ol>
<li><strong>视觉蕴含(Visual Entailment)：</strong></li>
</ol>
<p>定义：给定一个假设或者一个前提，能否推理出这个前提，如果能推理出来，就是一个蕴含的关系Entailment。如果前后矛盾推不出来，就是矛盾Contradictory。如果没关系，不确定能否推出来，就是中立Neutral。</p>
<p>衡量指标：一般将Visual Entailment变成一个三分类的问题，衡量的指标就是分类准确度。</p>
<ol>
<li><strong>视觉问答（VQA）：</strong></li>
</ol>
<p>定义：给定一个问题一个图片，能否提供一个Answer回答这个问题，一般有两个Setting：一个是看作分类问题，它的这些答案都是固定的，从一个集合中选择一个正确答案，一般称作闭集VQA，例如VQA2.0数据集有一个提前设好的3192个Answer。一个是看作生成问题，需要一个Transformer Decoder做文本生成的任务，一般称作开集VQA，开集VQA它的任务难度大很多，因为有可能生成了正确的或者很相似的答案，但是它跟Ground Truth不一致，还是会被判错。</p>
<p>衡量指标：分类准确度。</p>
<ol>
<li><strong>视觉推理（Visual Reasoning）：</strong></li>
</ol>
<p>定义：预测一个文本能不能描述一张图片，所以它是一个二分类任务问题</p>
<p>衡量指标：分类准确度。</p>
<ol>
<li><strong>Visual Grounding</strong></li>
</ol>
<p>其实Visual Grounding属于它自己的一个领域，很多多模态表征学习的论文里， 都不会去涉及这个的任务. ALBEF虽然做了Answer Generation的问题，但在做推理的时候，还是把生成的答案限制到3192个答案里，所以还是一个分类问题。</p>
<p><strong>消融实验</strong></p>
<p>作者将用了MLM和ITM training loss的模型当做了baseline，因为基本上所有的之前的工作和现在的工作里面都会有这两个loss。</p>
<ul>
<li>
<p>增加ITC也就是Align Before Fuse里的Align，这个提升是非常巨大的，基本是两个多点，三个点，而且是在检索、VE、VR、VQA这么多任务上都有明显的提升，所以CLIP、MoCo这种对比学习的方式非常厉害</p>
</li>
<li>
<p>增加ITM里提出的Hard Negative，大概都有0.5左右的提升</p>
</li>
<li>
<p>增加Momentum Distillation（MoD），在预训练阶段的MoD提升可能只有0.3 0.4个点，但是这个研究方向还是很好的，就是怎么从Noise Data里去学习有效的表征。</p>
</li>
</ul>
<h1 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h1>
<ul>
<li><a href="https://blog.csdn.net/lansebingxuan/article/details/131721728">多模态系列论文–ALBEF 详细解析-CSDN博客</a></li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>07 CUDA Graphs</title>
    <url>/2025/01/24/07-CUDA-Graphs/</url>
    <content><![CDATA[<h1 id="definition-of-a-cuda-graph"><a class="markdownIt-Anchor" href="#definition-of-a-cuda-graph"></a> DEFINITION OF A CUDA GRAPH</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250121205355699.png" alt="image.png" /></p>
<p>图是一种由依赖关系连接的一系列操作, 它独立于其执行过程。</p>
<p>它能够一次性生成，随后多次执行, 得益于其定义与执行的分离。这种重复执行正是关键所在。</p>
<span id="more"></span>
<p>构成CUDA图的节点是任意<strong>异步</strong>CUDA操作。他们不必是Kernel, 可以是:</p>
<ul>
<li><strong>Kernel Launch</strong>: CUDA kernel running on GPU</li>
<li><strong>CPU Function Call</strong>: Callback function on CPU</li>
<li><strong>Memcopy/Memset</strong>: GPU data management</li>
<li><strong>Memory Alloc/Free</strong>: Inline memory allocation</li>
<li><strong>Sub-Graph</strong>: Graphs are hierarchical</li>
</ul>
<h1 id="free-up-cpu-resources"><a class="markdownIt-Anchor" href="#free-up-cpu-resources"></a> FREE UP CPU RESOURCES</h1>
<p><strong>释放 CPU 时间以降低功耗或运行其他工作</strong></p>
<p>当启动一个包含单个操作的图时，会隐式触发我们在创建过程中生成的多项操作，同时保持CPU可用于其他任务。</p>
<p>与之形成对比的是上述流示例，其中CUDA内核启动更为零散，而CPU实际上可能非常忙碌地执行 launch。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250121205627607.png" alt="image.png" /></p>
<h1 id="launch-overhead-reduction"><a class="markdownIt-Anchor" href="#launch-overhead-reduction"></a> LAUNCH OVERHEAD REDUCTION</h1>
<p>假如 kernel 非常小，小于启动开销, 总执行时间主要受CPU启动开销影响</p>
<p><strong>Graph Launch 一次性提交所有工作，降低 CPU 开销</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250121210029596.png" alt="image.png" /></p>
<h1 id="three-stage-execution-model"><a class="markdownIt-Anchor" href="#three-stage-execution-model"></a> THREE-STAGE EXECUTION MODEL</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250121210325113.png" alt="image.png" /></p>
<ol>
<li><strong>定义阶段</strong>: 定义 CUDA 图的结构, 内容. 告诉 CUDA 节点、参数、依赖关系，目标GPU是哪个, 要启动到哪个流等等。</li>
<li><strong>实例化阶段:</strong> 当我们实例化图时, 其性能开销与直接在 CUDA 流中启动内核相似, 但这一成本仅需支付一次. 随后，我们可以多次执行这个图, 从而有效地隐藏掉初始的设置成本。</li>
<li><strong>执行阶段:</strong> 在 CUDA 流中执行图</li>
</ol>
<h1 id="modifying-graphs-in-place"><a class="markdownIt-Anchor" href="#modifying-graphs-in-place"></a> MODIFYING GRAPHS IN-PLACE</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250121211747011.png" alt="image.png" /></p>
<h1 id="programming-model"><a class="markdownIt-Anchor" href="#programming-model"></a> PROGRAMMING MODEL</h1>
<p>定义 CUDA 图有两种方式, 第一种方式是流捕获 (Stream Capture), 第二种方式是直接创建图</p>
<h2 id="stream-capture"><a class="markdownIt-Anchor" href="#stream-capture"></a> Stream Capture</h2>
<ul>
<li><strong>记录了流中的操作，但并未实际向 GPU 发起任何工作任务。</strong> 之后，当完成捕获该图对象时；它便可以被实例化，所有节点及其依赖关系均由 CUDA 定义完毕。</li>
<li>调用库函数时；流捕获同样非常实用, 例如对cuBLAS的通用矩阵乘法调用，或者对cuFFT的任何调用。因为作为用户，我们通常并不清楚在库提供的抽象层级背后具体有哪些工作被启动，以及使用哪些参数等细节来达成结果。流捕获在此非常有助于处理这些细节；</li>
<li>需要注意的是，<strong>如果库函数调用了CUDA流同步<code>cudaStreamSynchronize()</code>，它可能会带来问题</strong>，因为捕获操作实际上并未启动任何任务, 若尝试使用CUDA流进行捕获, 将会遇到运行时错误。</li>
</ul>
<h3 id="例子1-从常见-cuda-stream-创建-cuda-graph-方式"><a class="markdownIt-Anchor" href="#例子1-从常见-cuda-stream-创建-cuda-graph-方式"></a> 例子1 : 从常见 Cuda Stream 创建 Cuda Graph 方式</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124111748043.png" alt="image.png" /></p>
<p>尽管我们仅捕获了Stream 1，但CUDA仍能识别事件记录以等待事件依赖及后续节点。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124112841901.png" alt="image.png" /></p>
<h3 id="例子2-通过-stream-capture-来记录库调用"><a class="markdownIt-Anchor" href="#例子2-通过-stream-capture-来记录库调用"></a> 例子2: 通过 Stream Capture 来记录库调用</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124112821774.png" alt="image.png" /></p>
<h2 id="create-graphs-directly"><a class="markdownIt-Anchor" href="#create-graphs-directly"></a> CREATE GRAPHS DIRECTLY</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124113108339.png" alt="image.png" /></p>
<h2 id="combining-graph-stream-work"><a class="markdownIt-Anchor" href="#combining-graph-stream-work"></a> COMBINING GRAPH &amp; STREAM WORK</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124113355615.png" alt="image.png" /></p>
<h2 id="graph-execution-semantics"><a class="markdownIt-Anchor" href="#graph-execution-semantics"></a> GRAPH EXECUTION SEMANTICS</h2>
<p>工作在流操作之间是有序的，正如在常规CUDA中一样。如果将图表启动到流中，就是另一种可<br />
启动的工作类型。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124113509997.png" alt="image.png" /></p>
<h2 id="graphs-ignore-stream-serialization-rules"><a class="markdownIt-Anchor" href="#graphs-ignore-stream-serialization-rules"></a> GRAPHS IGNORE STREAM SERIALIZATION RULES</h2>
<p>Graph 忽略了流序列化规则，启动流仅用于工作前后顺序的排定。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124113737069.png" alt="image.png" /></p>
<h1 id="cuda-图的限制"><a class="markdownIt-Anchor" href="#cuda-图的限制"></a> CUDA 图的限制</h1>
<h2 id="no-automatic-placement"><a class="markdownIt-Anchor" href="#no-automatic-placement"></a> NO AUTOMATIC PLACEMENT</h2>
<p>在创建图和节点时，需要由程序员决定操作将在何处运行<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124113954383.png" alt="image.png" /></p>
<h1 id="cuda-图的优势"><a class="markdownIt-Anchor" href="#cuda-图的优势"></a> CUDA 图的优势</h1>
<h2 id="rapid-re-issue-of-work"><a class="markdownIt-Anchor" href="#rapid-re-issue-of-work"></a> RAPID RE-ISSUE OF WORK</h2>
<p>图的使用能够优化向GPU派发任务时的开销成本</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124114303819.png" alt="image.png" /></p>
<h2 id="heterogeneous-node-types"><a class="markdownIt-Anchor" href="#heterogeneous-node-types"></a> HETEROGENEOUS NODE TYPES</h2>
<p><strong>CUDA 图是一种异构类型。因此，可以在图中包含 GPU工作、CPU 工作以及数据移动。</strong> 将操作定义为一个统一的工作组的好处之一是，CUDA能够应用一些微妙的优化，而这些优化在仅将每个内核视为独立事件时是无法实现的</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124114358603.png" alt="image.png" /></p>
<h2 id="cross-device-dependencies"><a class="markdownIt-Anchor" href="#cross-device-dependencies"></a> CROSS-DEVICE DEPENDENCIES</h2>
<p><strong>CUDA 可以同步多 GPU</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124114646676.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>BeiTv3</title>
    <url>/2025/01/16/BeiTv3/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161612285.png" alt="image.png" /></p>
<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131609713">https://blog.csdn.net/lansebingxuan/article/details/13160…</a></p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/2208.10442">Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks</a></p>
<p>论文代码：<a href="https://github.com/microsoft/unilm/tree/master/beit3">BEiT-3</a></p>
<h2 id="引言-big-convergence大一统"><a class="markdownIt-Anchor" href="#引言-big-convergence大一统"></a> 引言 Big Convergence（大一统）</h2>
<p>最近不论是在Language、Vision还是在多模态领域，Big Convergence是大势所趋，也就是在超大的数据集上做大规模的预训练，一旦模型训练好之后，它的特征就已经非常好了，可以直接Transfer到下游任务上去，尤其是当模型足够大、数据足够多的时候，有可能预训练出来一个有通用性能的Foundation Model，这个Foundation Model能解决各种的模态或者各种下游任务，非常的强大。</p>
<p>本文将大一统继续向前推进，彻底将多模态尤其是Vision Language预训练得很好，主要是从以下的三个方面实现大一统：</p>
<span id="more"></span>
<p><strong>1. 模型：</strong></p>
<p>从模型角度来说，Transformer非常关键：</p>
<p><strong>1）Transformer框架相比CNN的优势</strong>：未来肯定是多模态的，一个模型做所有的Modality，所有的Task，肯定是一个大一统的框架，CNN不太适合做其他的Modality，而Transformer就适合做很多的Modality。从这一点上对比Transformer就胜出了。</p>
<p><strong>2）多模态学习常用框架</strong>：Transformer刚开始是NLP用的，然后逐渐用到Vision和多模态领域，现在对于Vision Language Modeling多模态学习来说，有几个常用的方式：</p>
<ul>
<li>
<p>Clip的Dual Encoder框架：该框架非常适合做快速的Retrieval。</p>
</li>
<li>
<p>Encoder、Decoder框架：该框架适合做Generation Task，比如BLIP、Coca。</p>
</li>
<li>
<p>Fusion Encoder框架：只用Encoder，但是它有多模态融合部分，ALBEF、VLMO都属于这一类，能做Image Text Encoding。</p>
</li>
</ul>
<p>不论是哪种方式，这些模型在遇到下游任务的时候，因为输入的形式可能会改变或者输出的形式有时候会改变，所以模型需要根据下游任务去做一些改进，因此不是真正意义上的General Purpose Modeling。BEITV3用1个统一的Masked Data Modeling模型，每一个训练好的Transformer Block，SA层，或者Feed Forward Network都可以随意的拼装和组合。</p>
<p><strong>2. 预训练的目标函数</strong></p>
<p>因为掩码学习已经能够很好的学习图像文本或者多模态的特征，因此本文验证只用这一个目标函数训练模型。这种方式的优点如下：</p>
<p>1）当数据和模型变大的时候，用更多的目标函数训练速度肯定会变慢。</p>
<p>2）多个loss的权重调整比较难，有的Loss之间可能互补，有的Loss之间可能互斥，增加了很多的复杂度。而单个loss训练就比较简单。</p>
<p>因此，本文使用了一个Pretraining Task，就是Mask Then Predict，因为图像经过Vision Transformer Embedding层以后，就变成了一个Sequence of Token，因此可以将图像看成一个Foreign Language叫Imaglish，这样就能将文本和图像用同样的方式去处理，本质上就没有任何区别。多模态的图像文本对就可以把它看成是一个Parallel Sentence，就是句子1后面跟了个句子2，那就没有什么不同了，一切都变成NLP（当然这也从侧面说明Mask Data Modeling目标函数非常的强）。</p>
<p><strong>3. 扩大模型和数据集的规模</strong></p>
<p>作者将模型大小扩展到Billions of Parameters，数据集也扩展的非常大，不过用的都是开源数据集。</p>
<h2 id="beit-3预训练框架"><a class="markdownIt-Anchor" href="#beit-3预训练框架"></a> BEIT-3预训练框架</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161621466.png" alt="image.png" /></p>
<p>模型用了Multi-Way Transformer，前面的自注意力全都是Share weights（Shared Multi-Head Self-Attention），只有后面Feed Forward Network不一样，根据不同的Modality训练不同的Vision、Language、Vision Language三个不同的Expert，然后通过调整不同的Input Modality去选择模型分支。Mask Data Modeling目标函数可能遮住了图像，可能是遮住了文本，模型训练学习去恢复它就可以。</p>
<h2 id="下游任务实现框架"><a class="markdownIt-Anchor" href="#下游任务实现框架"></a> 下游任务实现框架</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161635788.png" alt="image.png" /></p>
<p>做下游任务时候：</p>
<ol>
<li>如图3（a），如果用Vision Encoder，就可以做所有图像任务，包括Classification, Detection和Segmentation。 2. 如图3（b），如果用Language Encoder，就可以去做Language的各种任务。 3. 如图3（c），如果用Vision Language Fusion Encoder，就可以做多模态的各种任务Vision Language Understanding Task。 4. 如图3（d），如果用 Dual Encoder，把这两个Vision和Language分开，变成双塔结构，就可以像CLIP一样做比较高效的Image Text Retrieval，如果Funtune，可以再用ITC去Funtune。 5. 如图3（e），如果用Image-to-Text Generation，就可以做生成Captioning任务，给定一句话，将下一个文本Mask掉，然后你Image Grounded的Text Encoder去预测被Mask掉的单词，就可以去做Image Captioning。</li>
</ol>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>BEITv3其实从方法上来说就是之前BEIT、BEITv2、VLBEIT、VLMO等一系列的工作的一个集合体，本身没有提出新的内容，主要就是把它做大做强，展示了一个Unified Framework能达到的性能。</p>
<p>BEiTv3的目标非常明确，就是想做一个更大一统的框架，不论是从模型上统一，而且从训练的目标函数上要统一，还有模型大小，数据集大小，如何scale也要统一，作者称之为Big Convergence。BEiTv3就是把图像也看成了是一种语言（这就是他们题目的意思叫做Image as a Foreign Language），文章把Image叫做Imagelish，文本叫做English，然后把图像文本对叫做Parallel Sentence。因为不论是图像还是文本都可以用Mask Modeling去做,所以就不需要ITC，ITM ，Language Modeling或者Word Patch Alignment等各种Loss，只用一个Loss----- Mask Modeling。模型层面用的是他们之前VLMO提出的MOME，也就是文中的Multi-Way Transformers</p>
<p>总之，BEiTv3用一个非常简单而且非常容易扩展的框架，一个目标函数，Mask Modeling，但是效果非常好。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>BLIP</title>
    <url>/2025/01/16/BLIP/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131594387">https://blog.csdn.net/lansebingxuan/article/details/13159…</a></p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/2201.12086.pdf">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p>
<p>论文代码：<a href="https://github.com/salesforce/BLIP">https://github.com/salesforce/BLIP</a></p>
<p>BLIP是ALBEF原班人马做的, 借鉴了很多ALBEF的思想, <strong>BLIP设计了一个多模态混合编解码器（MED），能够实现 3 种功能：单模态编码、基于图像的文本编码、基于图像的文本解码</strong></p>
<h3 id="研究动机及本文贡献"><a class="markdownIt-Anchor" href="#研究动机及本文贡献"></a> 研究动机及本文贡献</h3>
<ol>
<li>
<p><strong>从模型角度</strong>：最近的一些方法通常有2种模型实现方式，1）Transformer Encoder结果的模型，比如Clip、ALBEF，2）Transformer Encoder、Decoder结构的模型，比如SimVLM。第一种Encoder Only的模型无法直接运用到Text Generation的任务，比如图像生成字幕，因为它只有编码器没有解码器，需要加一些模块做Text Generation的任务；第二种Encoder、Decoder虽然有Decoder可以做生成的任务，但因为没有一个统一的框架，所以它不能直接用来做Image Text Retrieval的任务，因此需要提出一个Unified的统一的框架，用一个模型把所有的任务都解决。BLIP这篇论文就是（利用了很多VLMO里的想法）<strong>把模型设计成一个灵活的框架，从而构造一个Unified Framework</strong>。</p>
</li>
<li>
<p><strong>从数据角度</strong>：目前表现出色的方法比如Clip、ALBEF和SimVLM都是在大规模从网上爬下来的非常Noisy的Image Text Pair数据集上预训练模型，虽然当数据集足够大能够弥补一些嘈杂数据集带来的影响，但BLIP论文指出，使用Noisy的数据集去预训练效果不佳，不是最优解，BLIP论文提出<strong>Captioner和Filter 模块</strong>，Captioner的作用是给定任意一张图片，用Captioner生成一些字幕，从而得到大量的合成数据Synthetic Data，同时去训练一个Filtering Model，把图像和文本不匹配的对从数据集里删掉。作者训练的Captioner模型，可以生成非常好的有描述性的文本，因此训练的filtering模型会选择这个图像文本对去进行模型的训练，而不用原来那个真实的图像文本对去进行训练，这样能够有效地清洗Noisy的DataSet，让模型更好地利用数据集里的图像文本配对信息。</p>
</li>
</ol>
<span id="more"></span>    
<h3 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h3>
<p>由于BLIP论文借鉴了ALBEF和VLMO的思想，因此先简单介绍这两篇论文。</p>
<h4 id="albef模型"><a class="markdownIt-Anchor" href="#albef模型"></a> ALBEF模型</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161154646.png" alt="image.png" /></p>
<p>ALBEF的模型分成三个结构：1）视觉编码器，2）文本编码器，3）多模态编码器。训练步骤如下：</p>
<ol>
<li>
<p>输入一张图像进入Transformer Encoder，它一共有N层；输入一个文本进入这个文本的编码器，它有L层。得到对应的图像文本特征之后，先做一个ITC（Image Text Contrasting）对比学习的Loss，把图像和文本特征分别学好。</p>
</li>
<li>
<p>文本特征继续进入Self-Attention Layer去训练，图像特征通过一个Cross-Attention Layer进来，然后和文本特征去进行融合，经历了N-L层的多模态的编码器之后，得到多模态的特征。</p>
</li>
<li>
<p>用多模态的特征做Image Text Matching任务，从而训练更好的模型。</p>
</li>
</ol>
<p>文本端要把一个N层的Transformer Encoder分成L层和N-L层，因为作者想维持计算量不变，跟Clip相同，左边一个12层的Transformer Encoder，右边也是一个12层的Transformer Encoder，不想增加更多的多模态融合的计算量，但是多模态这部分又特别的重要，相对而言文本这端不那么重要，所以就把这边12层的计算量给分成了两部分。</p>
<h4 id="vlmo模型"><a class="markdownIt-Anchor" href="#vlmo模型"></a> VLMO模型</h4>
<p>`<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161211803.png" alt="image.png" /></p>
<p>`<br />
针对维持计算量的问题，VLMO没有拆分两部分，而是设计一个Mixer of Expert MoE网络，让它变得极其的灵活。它只有一个网络，Self-Attention层全都是共享参数，唯一根据模态不同而改变的地方是Feed Forward Network，用Feed Forward Vision、Feed Forward Attacks和Feed Forward Multi Model去区别不同的Modality，训练不同的Expert。这样就用统一的一个模型，即训练的时候是一个模型，推理的时候可以根据不同的任务选择这个模型中的某一部分去做推理。VLMO用大量的实验证明Self-Attention层确实是可以共享参数的，它跟模态没什么关系。</p>
<h3 id="本文贡献1unified-framework"><a class="markdownIt-Anchor" href="#本文贡献1unified-framework"></a> 本文贡献1–Unified Framework</h3>
<h4 id="blip模型"><a class="markdownIt-Anchor" href="#blip模型"></a> BLIP模型</h4>
<p>BLIP的模型结构称为MED（Mixture of Encoder and Decoder），就是把编码器和解码器混到一起，模型包含四个部分，图像有一个N层的标准的VIT模型，Self-Attention和Feed Forward均采用标准格式，文本有三个模型，分别算三个不同的目标函数，跟VLMO非常像。它根据输入模态的不同、目标函数的不同，它选择一个大模型里不同的部分去做模型的forward。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161220256.png" alt="image.png" /></p>
<ol>
<li>
<p>第一个文本模型是Text Encoder，N层做分类的任务，当得到了文本特征之后，它就去跟这个视觉特征去做ITC loss。</p>
</li>
<li>
<p>第二个文本模型是Image Grounded Text Encoder，是一个多模态的编码器，借助图像的信息去完成一些多模态的任务，得到ITM loss。到这里左边这一部分就是一个ALBEF，但是它跟ALBEF有一点不同，就是它借鉴了VLMO，Self Attention层可以共享参数，就不需要把一个文本模型拆分成两个部分使用，这里<strong>同样的颜色代表共享参数</strong>，图中的SA层是共享参数的。所以第一个文本编码器和第二个文本编码器基本一样，SA和FF全都是一致的，只不过第二个多了一个Cross Attention层。</p>
</li>
<li>
<p>第三个文本模型是Image-grounded Text decoder。目前的结构还是只能做这种VQA VRVE这种Understanding的任务，所以在后面再加一个文本的Decoder，就可以做生成的任务，对于Decoder来说它的输入输出的形式和尤其是第一层的这个Self-Attention不太一样，因为它不能看到完整的句子，必须像训练GPT模型一样把后面的句子都mask掉，只通过前面的信息去推测后面的句子，所以它的第一层用的是Causal的Self-Attention，也就是因果关系的自注意力去做一些因果推理。这里它做的是Causal Self-Attention跟前面的Bidirectional Self-Attention就不一样了。除了第一层的这个自注意力之外，后面的这个Cross-Attention和Feed-forward就跟前面全都是共享参数的。所以它新添加了第三个Text Decoder，但事实上参数量并没有增加多少，只是增加了一些Causal的Self-Attention。最后的目标函数就是用的GPT系列的Language Modeling，也就是给定一些词，去预测剩下的那些词，这篇论文里作者要做生成式的任务，所以更好的选择是使用Language Modeling的目标函数。</p>
</li>
</ol>
<h4 id="blip目标函数"><a class="markdownIt-Anchor" href="#blip目标函数"></a> BLIP目标函数</h4>
<p>BLIP三个目标函数中，前两个跟ALBEF和VLMO都是一样，使用ITC（Image-Text Contrastive loss）和ITM（Image-Text Matching loss），只不过第三个从MLM（Masked Language Modeling）换成了LM（Language Modeling）。</p>
<ol>
<li>
<p>ITC: 使用对比损失来约束image与text的特征，positive靠近，negative远离。</p>
</li>
<li>
<p>ITM: 选取对比计算中的hard negative，要求网络计算其是否匹配，赋予网络具有挑战的任务。</p>
</li>
<li>
<p>MLM: BERT的预训练函数，ALBEF和VLMO也用这个目标函数，类似完形填空，将一个句子某个中间词mask掉，再预测这个中间词，属于双向模型</p>
</li>
<li>
<p>LM：mask掉句子后半部分，然后用前半部分去预测句子后面内容。</p>
</li>
</ol>
<p>其他细节：</p>
<ol>
<li>
<p>对于三个文本模型来说，它们对应的token不一样，第一个文本模型就用的是CLS Token，第二个用的是Encode，第三个用的是Decode，这些模型都很难训练，因为在做每一次Training Iteration的时候，图像端只需要做一次forward，但文本端在要做三次forward，要分别通过这三个模型去得到对应的特征，然后去算对应的目标函数，所以非常耗时。</p>
</li>
<li>
<p>BLIP是ALBEF的原班人马做的,所以用到了很多ALBEF的技巧:1)算ITC的时候也用了Momentum Encoder去做更好的Knowledge Distillation和更好的数据级的清理。2）算ITM Loss的时候，也像ALBEF一样利用ITC算的Similarity Score做Hard Negative Mining，从而每次都用最难的负样本去算ITM，增加Loss的有效性。</p>
</li>
</ol>
<h3 id="本文贡献2cap-filter-model"><a class="markdownIt-Anchor" href="#本文贡献2cap-filter-model"></a> 本文贡献2–Cap Filter Model</h3>
<p>BLIP这篇论文第二个贡献点，也是最重要的贡献称为Cap Filter Model。目前从网页上爬取的数据集最大的问题是图片文本对不匹配，也就是说这里的TW不好，作者用红色表示，然后Coco是手工标注的，作者认为图片文本一定匹配，用绿色来表示。</p>
<h4 id="filter-model"><a class="markdownIt-Anchor" href="#filter-model"></a> filter model</h4>
<ol>
<li>
<p>动机<br />
作者认为，如果用Noisy的数据集去预训练一个模型，效果就不是最好，因此需要清理数据集，从而达到最优解。</p>
</li>
<li>
<p>训练方法<br />
本文训练一个能够提供图像文本之间相似度的模型，相似度高的匹配，相似度不高的不匹配。训练方法是将已经提前预训练好的MED，也就是BLIP模型拿出来，把图像模型和两个文本模型，就是分别做ITC和ITM的那两个文本模型拿出来在Coco数据集上做微调，微调过后的MED就叫做filter。接下来用这个模型计算图像文本的相似度，尤其是image text matching分数，就可以确认图像和文本是不是一个match。若不match就可以把它拿掉。通过filter作者把原始爬下来的noisy的IT的文本对（红色的tw）变成了更干净的图像文本对（绿色的tw），到这里任务就完成了。</p>
</li>
</ol>
<h4 id="captioner-model"><a class="markdownIt-Anchor" href="#captioner-model"></a> captioner model</h4>
<p>作者发现，BLIP模型训练好的decoder非常强，有时候生成的句子比原始的图像文本对好很多，即使原来的图像文本对是一个match，但是新生成的文本更匹配，质量更高。所以作者用生成的文本充当新的训练数据集，具体的，作者在coco数据集上把已经训练好的image grounded text decoder又微调了一下，得到了captioner，然后给定任意一张从网上爬下的图片，用这个captioner给这个图片生成新的字幕，也就是红色这里的ts，经过filter筛选后，添加到数据集中，它是synthetic data。最后通过captioner和filter数据集变大。以cc12million为例，（Iw，Tw）是filter过后的cc12million，它还是原来从网上爬下的图像文本对，只不过是filter过后变少，但质量也变高了。（Iw，Ts）是cc12million合成的新生成的图像文本对。（Ih，Th）是手工标注的Coco数据集。总之数据集不仅变得更大，而且质量变得更高了。再用新的d去预训练BLIP模型，模型的提升非常显著。这就是本文提出的第二个创新点，这个funtune的capfilter模型做到了数据集上的bootstrapping（Bootstrapping算法，指的就是利用有限的样本资料经由多次重复抽样，重新建立起足以代表母体样本分布的新样本）。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161230353.png" alt="image.png" /></p>
<p>图中上面的Tw是直接从网页端下载的文本，下面的Ts是captioner新生成的文本，红色的代表被filter掉的文本，绿色代表filter以后保留下来的文本，也就是跟图片更匹配的文本，可以看出filter和captioner的强大之处</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161239353.png" alt="image.png" /></p>
<p>表1中对比了captioner ，filtering模式到底带来什么样的提升，C代表captioner，F代表filter。可以观察到：</p>
<ol>
<li>
<p>不论是用了filter还是用captioner，效果都会有提升。</p>
</li>
<li>
<p>用captioner以后提升是更加显著的，也就意味着captioner带来的data diversity多样性更让模型受益，因为尤其是对大模型或者大数据集的训练来说，偶尔数据集有点noise无所谓，模型都是能够处理的，但因为模型参数量太大，需要大量大量的数据，所以只要能生成更多更好的数据，它往往就能够受益。</p>
</li>
<li>
<p>captioner和filter同时用效果就达到最好。这个表格里倒数第3，4两行都是打了对号，也就说都用了captioner和filter，但一个叫base一个叫large因为BLIP跟VLMO一样分阶段训练：<br />
1）stage1：用嘈杂的数据集预训练了一个模型。<br />
2）stage2：用CoCo去funtune captioner和filter，再把数据集重新处理，得到一个新的更大的质量更好的数据集。<br />
3）stage3：用新的数据集又pre-train一个BLIP。<br />
这几个步骤互不相干，可以分开训练或者分开使用。因此第二阶段生成新的数据集的时候，可以用更大的模型去生成更好质量更高的数据集，并不一定是backbone这用的模型是base，capfilter模型就一定要用base，生成数据这一步完全是一个额外的步骤，理论上也可以用任何一种方式去生成pseudo-labeling伪标签。</p>
</li>
</ol>
<p>因此，BLIP large训练出来的这个captioner filter生成更好的数据后，可以训练其他模型，比如VLMO，CoCa，BEiT-3，它是一个非常通用的工具。例如，Laion Coco这个数据集刚开始先推出了Laion 400 Million，和OpenAI的Clip的400 Million数据集去对齐，接下来又推出了更大的Laion 2 Billion，Laion 5 Billion这些开源的大规模数据集，极大的促进了这个多模态学习的进展。他们也用BLIP模型和两个Clip模型不停的做filtering和captioning的过程，最后得到Laion Coco 600 Million这个数据集。它具体的做法：</p>
<p>1）给定任何一张图片，它先用最大的BLIP模型去生成40个caption。</p>
<p>2）用Clip去做Ranking看看最后Retrieve谁排前谁排后，这里用OpenAI Vision Transformer Large选最好的5个caption。</p>
<p>3）用OpenAI的Clip模型，但这次是用ResNet50做一次重新的Ranking然后把最好的那个选出来，这样子你就有一个图像文本对，一个图像对应一个文本。</p>
<p>4）用一个比较小的T0的模型去修复了一下语法，文本的标点符号使文本看起来更真实更正确。</p>
<p>所以BLIP是一篇非常好的论文，不只提出了一个模型框架，而是它提出的Caption Filtering这个方法非常有效，而且具有普适性，可以拿它去做很多的工作。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert</title>
    <url>/2025/01/09/Bert/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/pdf/1810.04805">论文链接</a></p>
<h2 id="bert-是什么"><a class="markdownIt-Anchor" href="#bert-是什么"></a> <strong>BERT 是什么</strong></h2>
<p><strong>BERT</strong> (<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers) 是一种基于Transformer架构的<strong>预训练语言模型,</strong> 它的主要模型结构是trasnformer的encoder堆叠而成。</p>
<p>它通过在大规模文本数据上的预训练来捕捉语言的深层双向表征，然后再针对不同的自然语言处理（NLP）任务进行微调（fine-tuning）。BERT的出现标志着NLP领域的一个重要进步，因为它能够更好地理解语言的上下文和语义关系。Bert 训练阶段具体如下：</p>
<p><strong>1）预训练阶段</strong>：BERT通过预训练任务来学习语言的深层表示。这些任务通常包括“遮蔽语言模型”（Masked Language Model，MLM）（类似于完形填空）和“下一句预测”（Next Sentence Prediction，NSP）。在MLM任务中，模型被训练来预测输入句子中被遮蔽的词；而在NSP任务中，模型需要判断两个句子是否是连续的文本序列。</p>
<p><strong>2）微调阶段</strong>：预训练完成后，BERT模型可以通过添加任务特定的输出层来进行微调，以适应不同的NLP任务，如情感分析、问答、命名实体识别等。微调过程利用了预训练阶段学到的语言表征，使得模型能够快速适应新的任务并取得优异的性能。</p>
<span id="more"></span>
<p><strong>Q1: 什么是预训练语言模型？</strong></p>
<p>预训练：预训练是一种迁移学习的概念。所谓预训练模型，举个例子，假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型（一个知识渊博的人，见多识广），当我们需要在特定场景使用时，例如做医学命名实体识别，那么，只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整即可（增加行业知识后，这个知识渊博的人就是行业专家）。预训练语言模型有很多，典型的如ELMO、GPT、BERT等。</p>
<p><strong>Q2: 什么是双向（Bidirectional）？</strong></p>
<p>因为BERT之前的预训练语言模型如ELMO和GPT都是单向的（ELMO可以说是双向的，但其实是两个方向相反的单向语言模型的拼接），而结合上下文信息对自然语言处理是非常重要的。Bidirectional也是Bert的主要创新点。</p>
<p><strong>ELMo和OpenAI GPT的问题</strong></p>
<p>ELMo和GPT最大的问题就是传统的语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子”The animal didn’t cross the street because it was too tired”。我们在编码it的语义的时候需要同时利用前后的信息，因为在这个句子中，it可能指代animal也可能指代street。根据tired，我们推断它指代的是animal，因为street是不能tired。但是如果把tired改成wide，那么it就是指代street了。传统的语言模型，不管是RNN还是Transformer，它都只能利用单方向的信息。比如前向的RNN，在编码it的时候它看到了animal和street，但是它还没有看到tired，因此它不能确定it到底指代什么。如果是后向的RNN，在编码的时候它看到了tired，但是它还根本没看到animal，因此它也不能知道指代的是animal。Transformer的Self-Attention理论上是可以同时attend to到这两个词的，但是根据前面的介绍，由于我们需要用Transformer来学习语言模型，因此必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的。</p>
<p>注意：即使ELMo训练了双向的两个RNN，但是一个RNN只能看一个方向，因此也是无法”同时”利用前后两个方向的信息的。也许有的读者会问，我的RNN有很多层，比如第一层的正向RNN在编码it的时候编码了animal和street的语义，反向RNN编码了tired的语义，然后第二层的RNN就能同时看到这两个语义，然后判断出it指代animal。理论上是有这种可能，但是实际上很难。举个反例，理论上一个三层(一个隐层)的全连接网络能够拟合任何函数，那我们还需要更多层词的全连接网络或者CNN、RNN干什么呢？如果数据不是足够足够多，如果不对网络结构做任何约束，那么它有很多中拟合的方法，其中很多是过拟合的。但是通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得它能够更好的收敛到最佳的参数。我们研究不同的网络结构(包括resnet、dropout、batchnorm等等)都是为了对网络增加额外的(先验的)约束。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109154955337.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155030883.png" alt="image.png" /></p>
<h2 id="bert-的预训练"><a class="markdownIt-Anchor" href="#bert-的预训练"></a> BERT 的预训练</h2>
<p>BERT的预训练阶段包括两个任务，一个是Masked LM ，还有一个是下句预测（Next Sentence Prediction，NSP）。</p>
<p><strong>Task #1：Masked LM</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155111095.png" alt="image.png" /></p>
<p>Masked LM 可以形象地称为完形填空问题，随机掩盖掉每一个句子中15%的词，用其上下文来去判断被盖住的词原本应该是什么。举例来说，有这样一个未标注句子 <code>my dog is hairy</code> ，我们可能随机选择了hairy进行遮掩，就变成 <code>my dog is [mask]</code> ，训练模型去预测 [mask] 位置的词，使预测出 hairy的可能性最大，在这个过程中就将上下文的语义信息学习并体现到模型参数中去了。</p>
<p>这里需要说明，GPT使用统计语言模型，这限制了它只能是单向的，而BERT通过Masked LM能够提取上下文信息。更一般地：</p>
<p><strong>AR模型，auto regressive，自回归模型。</strong> 自回归模型可以类比为早期的统计语言模型（Statistical Language Model），也就是根据上文预测下一个单词，或者根据下文预测前面的单词，只能考虑单侧信息，典型的如GPT，而ELMo 是将两个方向（从左至右和从右至左）的自回归模型进行了拼接，实现了双向语言模型，但本质上仍然属于自回归模型</p>
<p><strong>AE模型，auto encoding，自编码模型。</strong> 从损坏的输入数据（相当于加入噪声）中预测重建原始数据，可以使用上下文的信息。BERT使用的就是AE。劣势是在下游的微调阶段不会出现掩码词，因此[MASK] 标记会导致<strong>预训练和微调阶段不一致</strong>的问题。</p>
<p>所以该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：</p>
<p>80%的时间是采用[mask]，<code>my dog is hairy</code> → <code>my dog is [MASK]</code></p>
<p>10%的时间是随机取一个词来代替mask的词，<code>my dog is hairy</code> -&gt; <code>my dog is apple</code></p>
<p>10%的时间保持不变，<code>my dog is hairy</code> -&gt; <code>my dog is hairy</code></p>
<p><strong>为什么使用这个策略？</strong></p>
<p>（其实我目前还不能很好的理解，先将其他地方看到的说法放在这里）</p>
<p>这是因为transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是&quot;hairy&quot;（这个地方的理解，强行记住了位置和masked的分布，而没有真正理解上下文），从而导致若训练样本和微调的样本mask不一致的情况下，模型预测出现很大的偏差。</p>
<p>如果仅使用[MASK]或者随机的词，那么模型可能学习到的信息都是错误的单词（认为这个地方的单词就是不正确的）；</p>
<p>若仅使用正确的单词，那么模型学到的方法就是直接copy（根据学到的上下文，直接断定），从而学不到完整的上下文信息。</p>
<p>综上三个特点，必须在正确的信息（10%）、未知的信息（80% MASK，使模型具有预测能力）、错误的信息（加入噪声10%，使模型具有纠错能力）都有的情况下，模型才能获取全局全量的信息。</p>
<p><strong>Task #2：Next Sentence Prediction</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155131598.png" alt="image.png" /></p>
<p>很多下游任务（QA和natural language inference）都是基于两个句子之间关系的理解，基于此项任务，为了增强模型对句子之间关系的理解能力。训练数据选择两个句子（50%情况下是真正相连的两个句子，50%是随机拼接的两个句子），判断第二个句子是不是真正的第一个句子的下文。</p>
<p>其输入形式是，开头是一个特殊符号<code>[CLS]</code>，然后两个句子之间用<code>[SEP]</code>隔断：</p>
<p>Input = <code>[CLS] the man went to [MASK] store [SEP]he bought a gallon [MASK] milk [SEP]</code></p>
<p>Label = IsNext</p>
<p>Input = <code>[CLS] the man [MASK] to the store [SEP]penguin [MASK] are flight ##less birds[SEP]</code></p>
<p>Label = NotNext</p>
<p>实际构建预训练任务时，是首选设计好 “下句预测” 任务，生成该任务的标注信息，在此基础上构建 “Masked LM” 任务，生成掩码语言模型的标注信息。考虑到预训练涉及两个句子，BERT 采用如下的输入信息表征方案：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155143822.png" alt="image.png" /></p>
<p><strong>token embedding</strong> ：将各个词转换成固定维度的向量。在BERT中，每个词会被转换成<strong>768维</strong>的向量表示。在实际代码实现中，输入文本在送入token embeddings 层之前要先进行tokenization处理。此外，两个特殊的token会被插入到tokenization的结果的开头 ([CLS])和结尾 ([SEP])</p>
<p><strong>segment embedding：</strong> 用于区分一个token属于句子对中的哪个句子。Segment Embeddings 层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0</p>
<p><strong>position embedding</strong>：Transformers无法编码输入的序列的顺序性，所以要在各个位置上学习一个向量表示来将序列顺序的信息编码进来。加入position embeddings会让BERT理解下面下面这种情况，“ I think, therefore I am ”，第一个 “I” 和第二个 “I”应该有着不同的向量表示。</p>
<p>这3种embedding都是768维的，最后要将其按元素相加，得到每一个token最终的768维的向量表示。</p>
<h2 id="bert-能干什么"><a class="markdownIt-Anchor" href="#bert-能干什么"></a> BERT 能干什么</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155153118.png" alt="image.png" /></p>
<p>首先我们可以看到BERT 具有两种输出，一个是pooler output，对应的[CLS]的输出，以及sequence output，对应的是序列中的所有字的最后一层hidden输出。所以BERT主要可以处理两种，一种任务是分类/回归任务（使用的是pooler output），一种是序列任务（sequence output）。</p>
<ul>
<li>
<p>分类任务</p>
<ul>
<li>Single Sentence Classification tasks</li>
</ul>
</li>
</ul>
<blockquote>
<p>例如：文本分类，我想听音乐，分到音乐这个domain</p>
</blockquote>
<ul>
<li>
<p>Sentence Pair Classification tasks 例如：自然语言推断任务(NLI)，给定前提，推断假设是否成立</p>
</li>
<li>
<p>回归任务 回归任务其实是分类任务的一种特殊形式，最后的输出是一个数值而不是具体的某个类别的概率。</p>
</li>
<li>
<p>具体任务例如：文本相似度，可以判断两个句子是不是类似的，得到具体的分数。</p>
</li>
<li>
<p>序列任务</p>
</li>
<li>
<p>命名实体识别（NER）</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155227065.png" alt="image.png" /></p>
<ul>
<li>
<p>Cloze task（完形填空）其实这就是bert预训练的一种任务。</p>
</li>
<li>
<p>SQuAD(Standford Question Answering Dataset) task</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155235236.png" alt="image.png" /></p>
<p>SQuAD任务传入的是D,Q，其实D是该篇文章,Q是问题，返回的结果是答案开始的位置s以及答案结束的位置e。例如上图第一个问题的答案是gravity, 它的位置是文章中第17个单词，即s=17,e=17</p>
<p>具体做法是：我们学习两个向量，分别是Vs,Ve他们分别和document的sequence output做dot product，然后经过softmax，得到对应的s,e位置。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155245956.png" alt="image.png" /></p>
<h2 id="为什么bert仅用编码器而gpt仅用解码器"><a class="markdownIt-Anchor" href="#为什么bert仅用编码器而gpt仅用解码器"></a> 为什么BERT仅用编码器，而GPT仅用解码器？</h2>
<p>BERT 和 GPT 是两种经典的自然语言处理（NLP）模型，它们分别选择了编码器（Encoder）和解码器（Decoder）架构的不同部分进行专注，主要是因为它们的设计目标和任务类型不同。</p>
<ul>
<li>
<p><strong>任务需求</strong>：BERT专注于理解任务，需要对文本进行全局双向理解，因此使用了编码器。而GPT专注于生成任务，需要逐词预测文本，因此使用了解码器。</p>
</li>
<li>
<p><strong>架构特性</strong>：编码器擅长捕捉全局上下文信息，适合BERT的双向理解任务。解码器擅长逐步生成序列，适合GPT的文本生成任务。</p>
</li>
<li>
<p><strong>训练目标</strong>：BERT的预训练任务（如MLM）需要双向上下文理解，而GPT的语言模型任务需要单向文本生成。</p>
</li>
</ul>
<h3 id="bert仅使用编码器"><a class="markdownIt-Anchor" href="#bert仅使用编码器"></a> BERT：仅使用编码器</h3>
<p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong> 选择了仅使用Transformer的编码器部分，这是因为BERT的设计目的是为了解决需要全面理解句子语义的任务，比如：</p>
<ul>
<li>
<p><strong>句子分类</strong>（例如情感分析）。</p>
</li>
<li>
<p><strong>问答系统</strong>（识别文本中的答案）。</p>
</li>
<li>
<p><strong>命名实体识别</strong>（标记文本中的特定信息，如人名、地点名等）。</p>
</li>
</ul>
<h4 id="1-双向性理解"><a class="markdownIt-Anchor" href="#1-双向性理解"></a> 1. <strong>双向性理解</strong></h4>
<p>BERT 的核心特点是双向性（Bidirectional），即它在处理文本时，同时考虑了每个词左右两侧的上下文信息。为了实现这种双向性，BERT需要依赖编码器架构的自注意力机制（Self-Attention）来捕捉整个输入序列中所有词之间的关系。</p>
<ul>
<li>在BERT中，编码器不仅看一个词的前面的词（如在传统语言模型中那样），还看后面的词。这样，模型在每个位置上都能获得全局的上下文信息，从而更好地理解句子的语义。</li>
</ul>
<h4 id="2-预训练任务"><a class="markdownIt-Anchor" href="#2-预训练任务"></a> 2. <strong>预训练任务</strong></h4>
<p>BERT的预训练任务包括：</p>
<ul>
<li>
<p><strong>掩码语言模型（Masked Language Model, MLM）</strong>：BERT在预训练时，会随机地掩盖输入序列中的一些词，然后训练模型根据上下文预测这些被掩盖的词。由于BERT需要同时考虑词的前后信息，这一任务非常适合编码器架构。</p>
</li>
<li>
<p><strong>下一句预测（Next Sentence Prediction, NSP）</strong>：BERT还通过训练模型预测两个句子是否连续出现，以学习句子间的关系。这也是一种全局理解任务，需要编码器的支持。</p>
</li>
</ul>
<h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h4>
<p>BERT 仅使用编码器，因为它的目标是生成丰富的、上下文感知的词向量表示，适合各种需要深度理解文本语义的NLP任务。</p>
<h3 id="gpt仅使用解码器"><a class="markdownIt-Anchor" href="#gpt仅使用解码器"></a> GPT：仅使用解码器</h3>
<p><strong>GPT（Generative Pre-trained Transformer）</strong> 选择了仅使用Transformer的解码器部分，这是因为GPT的设计目的是生成文本或预测序列中的下一个词，例如：</p>
<ul>
<li>
<p><strong>文本生成</strong>（如文章写作、对话生成）。</p>
</li>
<li>
<p><strong>语言建模</strong>（预测下一个词）。</p>
</li>
<li>
<p><strong>自动补全</strong>（如智能回复）。</p>
</li>
</ul>
<h4 id="1-自回归模型"><a class="markdownIt-Anchor" href="#1-自回归模型"></a> 1. <strong>自回归模型</strong></h4>
<p>GPT 是一种自回归模型（Autoregressive Model），这意味着它在生成每个词时，只依赖于之前生成的词，而不考虑后续词。解码器架构非常适合这种自回归的任务。</p>
<ul>
<li>在解码器中，使用了掩码自注意力机制（Masked Self-Attention），即在预测下一个词时，只允许模型看到当前词之前的词。这样，模型可以按照顺序生成文本。</li>
</ul>
<h4 id="2-单向性生成"><a class="markdownIt-Anchor" href="#2-单向性生成"></a> 2. <strong>单向性生成</strong></h4>
<p>GPT 的生成过程是单向的，即从左到右逐词生成。解码器的架构可以通过掩码注意力机制确保每个生成的词只基于其前面的词，而不是后面的词，这与语言建模的目标一致。</p>
<ul>
<li>每个生成的词都是基于之前所有词的条件下生成的，因此GPT非常适合需要连续生成文本的任务。</li>
</ul>
<h4 id="3-预训练任务"><a class="markdownIt-Anchor" href="#3-预训练任务"></a> 3. <strong>预训练任务</strong></h4>
<p>GPT 的预训练任务是典型的语言建模任务，即给定一个词序列，预测下一个词。这个任务直接利用了解码器架构的优势。</p>
<h4 id="总结-2"><a class="markdownIt-Anchor" href="#总结-2"></a> 总结</h4>
<p>GPT 仅使用解码器，因为它的目标是生成连贯的、上下文相关的文本，适合各种文本生成任务。解码器架构的单向性和自回归特性非常适合这类任务。</p>
<p>参考链接：</p>
<p><a href="https://fancyerii.github.io/2019/03/09/bert-theory/#bert%E7%AE%80%E4%BB%8B">https://fancyerii.github.io/2019/03/09/bert-theory/#bert%E7%AE%80%E4%BB%8B</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/103226488">https://zhuanlan.zhihu.com/p/103226488</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>C++常见编程错误总结</title>
    <url>/2024/12/16/C-%E5%B8%B8%E8%A7%81%E7%BC%96%E7%A8%8B%E9%94%99%E8%AF%AF%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="i-more-efficient-than-i"><a class="markdownIt-Anchor" href="#i-more-efficient-than-i"></a> <ins>i More Efficient Than i</ins></h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193001450.png" alt="image.png" /></p>
<span id="more"></span>
<h1 id="shared_ptr-construction"><a class="markdownIt-Anchor" href="#shared_ptr-construction"></a> shared_ptr Construction</h1>
<p><strong>What’s wrong with this code?</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void f(const shared_ptr&lt;X&gt;&amp; sp, const vector&lt;int&gt;&amp; v);</span><br><span class="line">2. f(shared_ptr&lt;X&gt;(new X(args), &#123;11, 22, 33 &#125;);</span><br></pre></td></tr></table></figure>
<p>The order of the evaluation of function arguments is unspecified in C++. So the compiler can execute “new X(args)” first, create the vector second, and create shared_ptr last. If the vector creation throws, the memory of X is leaked (shared_ptr has not been created yet).<br />
<strong>Fix:</strong><br />
<code>f(make_shared&lt;X&gt;(args), &#123; 11, 22, 33 &#125;);</code></p>
<h1 id="function-argument-evaluation-order-undefined"><a class="markdownIt-Anchor" href="#function-argument-evaluation-order-undefined"></a> Function Argument Evaluation Order Undefined</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. f(v[i], i+)      \\Result is undefined.</span><br><span class="line">2. f(out1(), out2() \\You don&#x27;t know which &quot;out&quot; is executed first.</span><br></pre></td></tr></table></figure>
<h1 id="resource-management"><a class="markdownIt-Anchor" href="#resource-management"></a> Resource Management</h1>
<p>程序里不应该有和内存分配释放相关的裸指针<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193108850.png" alt="image.png" /></p>
<h1 id="multiple-resources"><a class="markdownIt-Anchor" href="#multiple-resources"></a> Multiple Resources</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193121639.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193132627.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193153724.png" alt="image.png" /></p>
<ul>
<li>C++11 15.2: “if the non-delegating constructor for an object has completed execution and a delegating constructor for that object exists with an exception, the object’s destructor will be invoked.&quot;</li>
<li>Just because you can take advantage of this rule doesn’t mean that you should!</li>
</ul>
<h2 id="recommendations"><a class="markdownIt-Anchor" href="#recommendations"></a> Recommendations</h2>
<ul>
<li>new and delete are dangerous. They are easy to cause memory leak.
<ul>
<li>Use make_shared and make_unique.</li>
</ul>
</li>
<li>new[] and delete[] are also dangerous.
<ul>
<li>Use vector and string.</li>
</ul>
</li>
<li>Manual resource management is dangerous.
<ul>
<li>Write a wrapper class for automatic resource management.</li>
</ul>
</li>
<li>Each automatic resource manager…
<ul>
<li>…should acquire/release exactly one resource, or</li>
<li>…should acquire/release multiple resources very carefully.</li>
</ul>
</li>
</ul>
<h1 id="returning-value"><a class="markdownIt-Anchor" href="#returning-value"></a> Returning Value</h1>
<h2 id="recommendations-2"><a class="markdownIt-Anchor" href="#recommendations-2"></a> Recommendations</h2>
<ul>
<li>Don’t return by const value
<ul>
<li>Inhibits move semantics, doesn’t achieve anything useful.</li>
</ul>
</li>
<li>Don’t move() when returning local X by value X (returned object and function return type are same).
<ul>
<li>The NRVO and move semantics are designed to work together.</li>
<li>NRVO applicable -&gt; direct construction is optimal.</li>
<li>NRVO inapplicable -&gt; move semantics is efficient.</li>
</ul>
</li>
<li>Function return type should not be rvalue reference (X&amp;&amp; foo(…)
<ul>
<li>For experts only, extremely rare.</li>
<li>Even the Standardization Committee got burned.</li>
<li>Valid examples: forward, move, declval, get(tuple&amp;&amp;)</li>
</ul>
</li>
<li>Function return type can be lvalue reference (X&amp; foo(…)), but must carefully  check the lifetime of the referenced object:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">()</span> </span>&#123; string s; <span class="keyword">return</span> s; &#125;  <span class="comment">//Wrong</span></span><br><span class="line"><span class="number">2.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string s)</span></span>&#123; <span class="keyword">return</span> s; &#125;     <span class="comment">//Wrong</span></span><br><span class="line"><span class="number">3.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string&amp; s)</span> </span>&#123;<span class="keyword">return</span> s; &#125;     <span class="comment">//Possible wrong, if s is short-lived.</span></span><br><span class="line"><span class="number">4.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string&amp;&amp; s)</span> </span>&#123; <span class="keyword">return</span> s; &#125;    <span class="comment">//Possible wrong, if s is short-lived.</span></span><br><span class="line"><span class="number">5.</span> <span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="number">6.</span>   <span class="function">string&amp; <span class="title">get_string</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> m_str; &#125;     <span class="comment">//Ok, but make sure caller does</span></span><br><span class="line"><span class="number">7.</span>   string m_str;                              <span class="comment">//not hold the reference after A</span></span><br><span class="line"><span class="number">8.</span> &#125;;                                           <span class="comment">//is destroyed.</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>CLIP</title>
    <url>/2025/01/10/CLIP/</url>
    <content><![CDATA[<p><strong>CLIP打通了文本和图像之间的联系，是多模态方面的经典之作。</strong></p>
<span id="more"></span>
<h1 id="对比学习"><a class="markdownIt-Anchor" href="#对比学习"></a> 对比学习</h1>
<h2 id="对比学习背后的直觉"><a class="markdownIt-Anchor" href="#对比学习背后的直觉"></a> 对比学习背后的直觉</h2>
<p>首先，让我们谈谈对比学习背后的直觉。 下面，我们可以看到一个很多孩子玩的传统游戏：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183045154.png" alt="image.png" /></p>
<p>这个游戏的目标是在右侧寻找与左侧的图片相似的动物。 在我们的例子中，孩子必须在右边的四张图片中搜索一张狗的图片。 首先，孩子必须将四种动物中的每一种与一只狗进行比较，然后得出结论，左下角的图像描绘的是一只狗。</p>
<p>根据许多调查，孩子们通过这种方式比阅读有关动物的书更容易学习新概念。 但是为什么这种方法效果更好呢？</p>
<p>事实证明，对于像孩子这样没有先验知识的人来说，通过对比相似和不同的事物来学习新事物比一个一个地学习识别它们更容易。 起初，孩子可能无法识别狗。 但过了一段时间，孩子学会了区分狗的共同特征，比如鼻子的形状和身体姿势。</p>
<h2 id="对比学习的训练目标"><a class="markdownIt-Anchor" href="#对比学习的训练目标"></a> 对比学习的训练目标</h2>
<p><strong>受先前观察的启发，对比学习旨在通过对比相似和不相似的样本来学习数据的低维表示。</strong> 具体来说，它试图在表示空间中使相似的样本彼此靠近，并使用欧氏距离将不同的样本推得更远。</p>
<p>假设我们有三张图片 I_1、I_2 和 I_3。 前两幅图像描绘了一只狗，第三幅图像描绘了一只猫，我们想要为每幅图像（x_1、x_2 和 x_3）学习低维表示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183054187.png" alt="image.png" /></p>
<p>在对比学习中，我们希望最小化相似样本之间的距离，最大化不同样本之间的距离。 在我们的示例中，<strong>我们希望最小化距离 d(x_1, x_2) 并最大化距离 d(x_1, x_3) 和 d(x_2, x_3)</strong>，其中 d() 是像欧几里得这样的度量函数。</p>
<p>与锚样本（I_1）相似的样本定义为正样本（I_2），与锚样本（I_3）不相似的样本定义为负样本。</p>
<h2 id="无监督训练"><a class="markdownIt-Anchor" href="#无监督训练"></a> 无监督训练</h2>
<p>当我们没有标注样本时，可以使用自监督学习，利用数据中的某些特性来生成伪标签。</p>
<p>一个用于无监督对比学习的著名自监督框架是 SimCLR。其主要思想是通过对原始图像应用随机变换（如裁剪、翻转和颜色抖动）来生成正样本对，因为这些变换不会改变图像的标签。</p>
<h1 id="数据收集"><a class="markdownIt-Anchor" href="#数据收集"></a> 数据收集</h1>
<p>对比开放的计算机视觉应用，目前的所有的视觉公开数据集（例如ImageNet等）的应用场景都是非常有限的，为了学习到通用的图像-文本多模态通用特征，首先要做的便是采集足够覆盖开放计算机视觉领域的数据集。这里<strong>OpenAI采集了一个总量超过4亿图像-文本对的数据集</strong>WIT（WebImage Text）。为了尽可能的提高数据集在不同场景下的覆盖度，WIT的首先使用在英文维基百科中出现了超过100次的单词构建了50万个查询，并且使用WordNet进行了近义词的替换。为了实现数据集的平衡，每个查询最多取2万个查询结果。</p>
<h1 id="模型架构"><a class="markdownIt-Anchor" href="#模型架构"></a> 模型架构</h1>
<h2 id="预训练"><a class="markdownIt-Anchor" href="#预训练"></a> 预训练</h2>
<p>CLIP的核心思想是将图像和文本映射到同一个特征空间。这个特征空间是一个抽象的概念，例如当我们看到一条狗的图片的时候，我们心中想的是狗，当我们读到狗的时候我们想的也是狗，那么我们心中想象的狗，便是“特征空间”。</p>
<p>所以CLIP也是由两个编码器组成，如图所示：</p>
<ul>
<li>
<p>图像编码器（VIT和ResNet，都用了）：用于将图像映射到特征空间；</p>
</li>
<li>
<p>文本编码器（Transformer）：用于将文本映射到相同的特征空间。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183110519.png" alt="image.png" /></p>
<p>在模型训练过程中，我们取到的每个batch由N个图像-文本对组成。这N个图像送入到图像编码器中会得到N个图像特征向量(I1,I2,⋯,IN)，同理将这N个文本送入到文本编码器中我们可以得到N个文本特征向量(T1,T2,⋯,TN)。因为只有在对角线上的图像和文本是一对，所以<strong>CLIP的训练目标是让是一个图像-文本对的特征向量相似度尽可能高，而不是一对的相似度尽可能低</strong>，这里相似度的计算使用的是向量内积。通过这个方式，CLIP构建了一个由N个正样本和N^2−N个负样本组成的损失函数。另外，因为不同编码器的输出的特征向量长度不一样，CLIP使用了一个线性映射将两个编码器生成的特征向量映射到统一长度，CLIP的计算过程伪代码如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - 残差网络 或者 ViT</span><br><span class="line"># text_encoder - CBOW 或者 文本Transformer</span><br><span class="line"># I[n, h, w, c] - 训练图像</span><br><span class="line"># T[n, l] - 训练文本</span><br><span class="line"># W_i[d_i, d_e] - 训练图像生成的特征向量</span><br><span class="line"># W_t[d_t, d_e]  - 训练文本生成的特征向量</span><br><span class="line"># t - softmax的温度（temperature）参数</span><br><span class="line"></span><br><span class="line"># 提取多模态的特征</span><br><span class="line">I_f = image_encoder(I) #[n, d_i]</span><br><span class="line">T_f = text_encoder(T) #[n, d_t]</span><br><span class="line"></span><br><span class="line"># 多模态特征向特征空间的映射</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1)</span><br><span class="line"></span><br><span class="line"># 计算余弦相似度</span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"></span><br><span class="line"># 构建损失函数</span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t  = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t)/2</span><br></pre></td></tr></table></figure>
<h2 id="zero-shot-推理"><a class="markdownIt-Anchor" href="#zero-shot-推理"></a> Zero-shot 推理</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183141161.png" alt="image.png" /></p>
<ul>
<li>
<p>考虑到大部分的数据集的标签都是以单词的形式存在的，比如“bird”，“cat”等等，然而在预训练阶段的文本描述大多都是某个短句，为了填补这种数据分布上的差别，作者考虑用指示上下文（guide context） 对标签进行扩展。可以<strong>用a photo of a label作为推理时文本端的输入</strong>，其中的 label 恰恰是需要预测的zero-shot标签。</p>
</li>
<li>
<p>CLIP模型的效果实现了图像和文本向同一个特征空间映射的能力。当进行图像识别时，我们将待识别的图像映射成一个特征向量。同时我们将所有的类别文本转换成一个句子（指示上下文），然后将这个句子映射成另外一组特征向量。<strong>文本特征向量和图像特征向量最相近的那一个便是我们要识别的目标图像的类</strong>。</p>
</li>
<li>
<p>考虑到以单词作为标签存在多义的情况，比如boxer表示斗牛犬、拳击运动；crane同时表示了起重机和鹤。这种词语的多义显然对是因为缺少对标签的上下文描述导致的。为了解决这种问题，<strong>作者在指示上下文中添加了一些提示标签类型的词语</strong>，比如A photo of a label, a type of pet.。作者将这个方法<strong>称之为“prompt engineering”</strong>。在合适地选取了不同的指示上下文，并且将其打分进行ensemble之后。作者发现这些Tricks竟能在zero-shot实验上提高5个绝对百分位。</p>
</li>
<li>
<p><strong>全局学习</strong>：CLIP学习的不再是图像中的一个物体，而是整个图像中的所有信息，不仅包含图像中的目标，还包含这些目标之间的位置，语义等逻辑关系。这便于将CLIP迁移到任何计算机视觉模型上。这也就是为什么CLIP可以在很多看似不相关的下游任务上（OC<strong>R等）取得令人意外的效果。</strong></p>
</li>
<li>
<p><strong>在实际应用中，这个类别的标签也是可以改的，不必非得是 ImageNet 中的1000个类，可以换成任何的单词；这个图片也不需要是 ImageNet 的图片，也可以是任何的图片</strong>，依旧可以通过算相似度来判断这图中含有哪些物体。即使这个类别标签是没有经过训练的，只要图片中有某个物体也是有很大概率判断出来的，这就是 zero-shot。但如果像之前的那些方法，严格按照1000个类去训练分类头，那么模型就只能判断出这1000个类，这1000个类之外的所有内容都将判断不出来。</p>
</li>
</ul>
<h1 id="效果"><a class="markdownIt-Anchor" href="#效果"></a> 效果</h1>
<p>CLIP 彻底摆脱了 categorical label 的限制，无论在训练时，还是在推理时，都不需要有这么一个提前定好的标签列表，任意给出一张图片，都可以通过给模型不同的文本句子，从而知道这张图片里有没有我想要的物体。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183153436.png" alt="image.png" /></p>
<p>CLIP 把视觉的语义和文字的语义联系到了一起，学到的特征语义性非常强，迁移的效果也非常好。如图左侧部分是在 ImageNet 上训练好的 ResNet101，右侧是 CLIP 训练出的 ViT-L，在 ImageNet 上 ResNet 和 CLIP 效果相同，但在 ImageNetV2、ImageNet-R、ObjectNet、ImageNet Sketch、ImageNet-A上，ResNet 的性能明显就不行了，迁移的效果惨目忍睹，但对于 CLIP 来说，它的效果始终都非常好。这也说明了 CLIP 因为和自然语言处理的结合，导致 <strong>CLIP 学出来的视觉特征和我们用语言所描述的某个物体产生了强烈的联系</strong>。</p>
<h1 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h1>
<p><a href="https://www.baeldung.com/cs/contrastive-learning">https://www.baeldung.com/cs/contrastive-learning</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/477760524">https://zhuanlan.zhihu.com/p/477760524</a></p>
<p><a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/h661975/article/details/135116957">【CLIP】多模态预训练模型CLIP论文详解-CSDN博客</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>CoCa</title>
    <url>/2025/01/16/CoCa/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131611916">https://blog.csdn.net/lansebingxuan/article/details/13161…</a></p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/2205.01917v1">CoCa: Contrastive Captioners are Image-Text Foundation Models</a></p>
<p>代码地址：<a href="https://github.com/lucidrains/CoCa-pytorch">CoCa</a></p>
<p>CoCa代表Contrastive Captioners的缩写，代表模型用两个目标函数训练出来的，一个是Contrastive Loss，一个是Captioning Loss。本文因为数据集更大，模型也更大，所以它的效果很好，在多模态所有的任务均SOTA，而且在单模态里，在ImageNet上也得到了90以上的Top1准确度，在视频动作识别领域，在Paper with Code上CoCa在K400、K600、K700这些数据集上排名前三。</p>
<span id="more"></span>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161514502.png" alt="image.png" /></p>
<p>CoCa是ALBEF的一个后续工作，它与ALBEF的模型类似，左边是一个Image Encoder，右边是一个Text <strong>Decoder</strong>，注意，这里是Decoder</p>
<p>不是Encoder。从左右来看还是左边图像分支，右边文本分支，文本分支分两部分，前面用来抽取Unimodel的文本特征，后面做<a href="https://so.csdn.net/so/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81&amp;spm=1001.2101.3001.7020">多模态</a>的特征。整个模型就是用两个目标函数训出来的，一个是ITC，一个是Language Modeling Loss，也就是Contrastive和Captioning，具体步骤如下：</p>
<ol>
<li>
<p>图像通过Image Encoder，得到一系列的Token，文本通过文本的解码器，得到一系列的文本特征。</p>
</li>
<li>
<p>图像的CLS Token和文本的CLS Token计算ITC loss</p>
</li>
<li>
<p>图像其他的Token做Attention Pooling，然后再传到多模态的Text Decoder里做Cross Attention，这样把视觉和文本的特征融合在一起了。多模态的特征用Captioning Loss训练，也就是BLIP、GPT用的Language Modeling Loss。</p>
</li>
</ol>
<p>所以CoCa的布局跟ALBEF是一模一样的，区别是：</p>
<ol>
<li>
<p>在图像分支做Attentional Pooling，这一部分是可学的，这种可学的Pooling方式能够针对不同的任务学到更好的特征。</p>
</li>
<li>
<p>不论是单模态的文本特征的学习还是多模态的特征学习，整个文本端统一都用Decoder训练目标函数，使用Captioning的Loss，文本的输入从一开始前面的Self-Attention Layer就是Causal的（也就是mask住一个句子后半部分，然后用前半部分去预测句子后面内容）。因为作者在超大的几十亿的数据集上去做预训练，所以文本如何mask关系不大，模型应该是什么都见过。</p>
</li>
</ol>
<p>Coca的模型实现并不难，但是想复现它难度非常大。原因是：</p>
<ol>
<li>
<p>模型大：虽然很简单，但它训练出来最大的模型参数量已经达到了2.1 billion，算是视觉或者多模态里面非常大的一个模型（当然在NLP那边已经有几十亿上百亿的模型）</p>
</li>
<li>
<p>训练的数据集∶作者不只用了之前训练Align用的多模态的数据集，同时还把GFT 3 billion（google私有数据）图像分类的数据集转化成了多模态数据集，加在一起有几十亿的训练数据，所以不论是模型还是这个数据都远超之前所有工作的这个scale，效果也是非常明显的。</p>
</li>
</ol>
<h2 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h2>
<ol>
<li>
<p>ITC loss：Contrastive Loss，图像的CLS Token和文本的CLS Token计算ITC loss。</p>
</li>
<li>
<p>LM(Captioning) Loss：单模态、多模态的文本特征学习，计算LM Loss。</p>
</li>
</ol>
<p>文本端统一都用Decoder训练目标函数，并且只用一个Captioning Loss而不用ITM Loss，原因是作者这里想解决训练的效率问题，之前不论是ALBEF还是VLMO，因为算各种的目标函数，往往一个Training Iteration要forward这个模型好几次，无形中增加了模型训练的时间长度，比如训练100个Epoch，其实forward三次之后相当于训练了300个Epoch。作者这里<strong>为了让ITC Loss和Captioning Loss能同时计算，所以文本的输入从刚开始就必须是Causal的，这样通过Unimodal Text Decoder出来的特征能直接做ITC Loss，同样的输入得到的多模态特征也直接能做Captioning Loss</strong>。这样一个Iteration就是只forward一次，训练时间就会降低一些。</p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>其实不论用ALBEF或者CoCa模型结构，还是VLMO、 BLIP，共享参数都是可以的。当把这个模型做大，数据集做大后，模型性能都差不多，其实往往最后拼的都是数据</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>DALL·E 2</title>
    <url>/2025/01/10/DALL%C2%B7E-2/</url>
    <content><![CDATA[<p><a href="https://cdn.openai.com/papers/dall-e-2.pdf">《Hierarchical Text-Conditional Image Generation with CLIP Latents》</a></p>
<p>Paper: <a href="https://cdn.openai.com/papers/dall-e-2.pdf">https://cdn.openai.com/papers/dall-e-2.pdf</a></p>
<p>Project: <a href="https://openai.com/product/dall-e-2">https://openai.com/product/dall-e-2</a></p>
<p>Author: OpenAI</p>
<span id="more"></span>
<p><strong>DALL·E 2能做什么</strong></p>
<ul>
<li>
<p><strong>DALL·E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.</strong></p>
<ul>
<li>
<p><strong>生成原创性的图片</strong></p>
</li>
<li>
<p><strong>组合concepts、attributes，and styles</strong></p>
</li>
</ul>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183739514.png" alt="image.png" /></p>
<ul>
<li><strong>DALL·E 2 can expand images beyond what’s in the original canvas, creating expansive new compositions.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183747747.png" alt="image.png" /></p>
<ul>
<li><strong>DALL·E 2 can make realistic edits to existing images from a natural language caption. It can add and remove elements while taking shadows, reflections, and textures into account.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183754130.png" alt="image.png" /></p>
<ul>
<li><strong>DALL·E 2 can take an image and create different variations of it inspired by the original.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183800592.png" alt="image.png" /></p>
<p>**DALLE 2 的hierarchical：**先生成64<em>64小分辨率图片，再利用一个模型上采样到256</em>256，再利用一个模型上采样到1024*1024。</p>
<p>DALLE 2就是： CLIP模型+GLIDE模型（一个基于diffusion model的文本图像生成方法）；</p>
<h1 id="前置"><a class="markdownIt-Anchor" href="#前置"></a> 前置</h1>
<h2 id="clip"><a class="markdownIt-Anchor" href="#clip"></a> CLIP</h2>
<p>CLIP通过对比学习将图像和文本映射到同一特征空间，实现了图像与文本之间的多模态理解。</p>
<h2 id="gan"><a class="markdownIt-Anchor" href="#gan"></a> GAN</h2>
<p>GAN的思想其实就是左右手互博，同时训练两个网络：生成器G(generator) 和判别器D(discriminator)。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183810503.png" alt="image.png" /></p>
<ul>
<li>
<p><strong>生成器</strong>: 给定一个随机噪声Z, 生成一个比较真实的图片X’</p>
</li>
<li>
<p><strong>判别器</strong>: 把生成的图片X’给判别器. 同时再给真实的图片X给这个判别器, 然后让这个判别器去看, 到底哪个是真图片, 哪个是假图片.</p>
</li>
</ul>
<p>其实就是一个0-1的这个二分类问题.</p>
<p>通过generator和discriminator这两个网络之间互相较量, 然后判别器不停地提高自己, 生成器也不停地提高自己, 所以说最后能生成比较真实的图片.</p>
<p>**优点：**因为GAN的目标就是以假乱真，所以其生成图像的保真度比较高</p>
<p><strong>缺点：</strong></p>
<ol>
<li>
<p>训练不够稳定，因为要同时训练两个网络；</p>
</li>
<li>
<p>因为是以保真度为目标，所以生成图像的多样性较差；</p>
</li>
<li>
<p>不是概率模型，它的生成都是隐式完成的。你不知道它做了什么，不知道遵循了什么分布，因此GAN在数学上不如后续的VAE、扩散模型优美。</p>
</li>
</ol>
<h2 id="aeauto-encoder"><a class="markdownIt-Anchor" href="#aeauto-encoder"></a> AE（Auto-encoder）</h2>
<p>自编码器是很早的技术，目的是将高维的信息通过encoder压缩到一个低维的code内，然后再使用decoder对其进行重建，关于它的细节可参阅文章：<a href="https://blog.csdn.net/DUDUDUTU/article/details/129286398?spm=1001.2014.3001.5502">Auto-encoder系列</a>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183819046.png" alt="image.png" /></p>
<p>给定一个输入X, 通过一个编码器, 然后就能得到一个特征Z, 这个特征的维度一般都会小很多, 所以也叫bottleneck. 然后再从bottleneck开始, 通过一个解码器, 最后得到一个图像X’.</p>
<p>训练的目标函数: 我们希望这个图像X’能尽可能的重建之前的这个X.</p>
<p>因为是自己重建自己, 所以说这也就是为什么叫auto-encoder.</p>
<h2 id="daedenoising-auto-encoder"><a class="markdownIt-Anchor" href="#daedenoising-auto-encoder"></a> DAE（Denoising Auto-encoder）</h2>
<p>先把原图进行了一定程度的打乱变成了一个Xc, 也就是corrupted x, 然后把这个经过扰乱过后的输入传给编码器, 后续都是一样的, 我们仍然希望这个图像X’能尽可能的重建之前的这个X, 而不是扰动后的Xc.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183829472.png" alt="image.png" /></p>
<p>这个改进证明非常的有用, 会让这个训练出来的模型非常的稳健, 也不容易过拟合. 主要原因是图像这边像素的冗余性太高了, 即使把原来的这个图片做一些污染, 其实模型还是能抓住它的本质, 然后去把它重建出来的. 这个其实也就有点何恺明MAE (Masked AutoEncoders)的意思也就是masked auto-encoder, 这个掩码自编码器在训练的时候能够mask掉75%, 还能把这个图像很好的重建出来, 也就说明了图像的冗余性确实是高, 也就从侧面证明了这种denoising auto-encoder或者masked auto-encoder的有效性</p>
<h2 id="vaevariational-auto-encoder"><a class="markdownIt-Anchor" href="#vaevariational-auto-encoder"></a> VAE（Variational Auto-Encoder）</h2>
<p>无论是AE，DAE，还是MAE，核心上都是学习bottleneck处这个特征的，然后去做目标检测、分割、分类这些下游任务，并不是去做生成的。原因就是中间特征Z并不是一个概率分布，我们没法对它采样，它是一个用于重建的特征。因此就有了VAE，中间不再生成一个特征，而是生成一个分布（高斯）。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183837941.png" alt="image.png" /></p>
<p>因为VAE学到的是一个概率分布，从分布里去抽样生成图像的多样性就会好很多。</p>
<p>VAE的本质思想是学习一个分布，而不是一个特征。后续的诸多工作也是基于这一本质思想进行展开的。</p>
<p>关于VAE的细节也可参阅文章：<a href="https://blog.csdn.net/DUDUDUTU/article/details/129286398?spm=1001.2014.3001.5502">Auto-encoder系列</a>。</p>
<h2 id="vq-vaevector-quantized-variational-auto-encoder"><a class="markdownIt-Anchor" href="#vq-vaevector-quantized-variational-auto-encoder"></a> VQ-VAE（Vector Quantized Variational Auto-Encoder）</h2>
<p>VQ-VAE 的核心思想是将连续的高维向量编码（例如语音信号、图像、视频等）离散化，从而减少模型的复杂度和存储需求。具体来说，VQ-VAE 通过将连续的编码向量映射到一组离散的“码本”（codebook）中的最近邻，从而将高维连续编码转换为低维的离散编码。在解码器中，这些离散编码被解码回原始的高维向量。</p>
<p>VQ-VAE就是将特征进行量化的VAE。相对于VAE，优化起来相对容易。codebook可以理解为聚类中心，有K*D个聚类中心。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183912860.png" alt="image.png" /></p>
<p>但是VQ-VAE学习的是一个固定的codebook，因此无法像VAE一样做随机采样，因此VQ-VAE更像是VQ-AE。若想像VAE那样，就需要有一个prior网络。在VQ-VAE中，prior网络是一个用于生成离散码本（codebook）的神经网络。离散码本是一组预定义的离散向量，用于将连续的向量空间映射到一个离散的向量空间。这种离散化的表示方式使得VQ-VAE可以对输入数据进行高效地编码和解码。</p>
<p>具体来说，prior网络的输入是由编码器生成的连续向量，输出是一个由离散向量组成的码本。prior网络的训练目标是最小化输入向量和最近的码本向量之间的欧几里得距离，从而实现向量量化。在训练过程中，prior网络不仅学习生成码本，还学习将连续向量映射到码本中最近的向量。通过将编码器生成的连续向量量化为码本中最近的向量，VQ-VAE可以保留输入数据的局部结构信息，并且可以在编码和解码过程中实现高效的计算。因此，prior网络在VQ-VAE中起着非常重要的作用。</p>
<p>VQ-VAE 已经在许多领域得到了应用，包括图像生成、音频压缩、语音识别、自然语言处理等。DALL·E 第一版就是基于VQ-VAE做的。</p>
<h2 id="dall-e"><a class="markdownIt-Anchor" href="#dall-e"></a> DALL-E</h2>
<p>对于一个图像文本对，文本用BPE编码生成256维的特征，图像用现成的VQ-VAE（codebook）编码为32*32维的特征，然后将这两个特征拼接成一个1280维的序列，再输入至GPT中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183921032.png" alt="image.png" /></p>
<p>训练时，将1280维序列的部分遮住，让GPT进行预测。</p>
<p>推理时，只需要输入文本的特征（256维），让GPT自回归的做预测就行。</p>
<h2 id="diffusion-model-扩散模型"><a class="markdownIt-Anchor" href="#diffusion-model-扩散模型"></a> Diffusion model (扩散模型)</h2>
<h3 id="扩散模型的方法"><a class="markdownIt-Anchor" href="#扩散模型的方法"></a> 扩散模型的方法</h3>
<p>扩散模型分为 forward diffusion 和 reverse diffusion 两个阶段。对于一个图像，forward diffusion 做的就是往<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中不断添加噪声，一共添加T次，最后得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>−</mo><mi>N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X_{t} - N(0,z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183928002.png" alt="image.png" /></p>
<p>为什么叫“扩散”，这启发于热力学。热力学中有一个名词叫“diffusion”，如果有两个物质分别是高密度和低密度的，那高密度的物质会慢慢地向低密度的做扩散，最后达到一种平衡。在Diffusion model 中，这种“平衡”的体现就是 forward diffusion 过程最后得到的趋近于各向同性的正态分布。</p>
<p>reverse diffusion过程要做的就是训练一个模型，使得从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>恢复至<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，然后再训练一个模型，使得从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>恢复至<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{t-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>，以此类推直至恢复至<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。在reverse diffusion 过程中所有使用到的模型都是共享参数的，也就是说整体其实只有一个模型，只是需要抽样生成很多次。 因此扩散模型目前最大的不足就是：相比于其他生成模型，训练很慢，且推理是最慢的。因为对于一个随机采样的噪声，要往前推T次才能生成Image。此外，目前reverse diffusion 过程中的网络大部分选用的都是U-Net。</p>
<h3 id="扩散模型的发展历程"><a class="markdownIt-Anchor" href="#扩散模型的发展历程"></a> 扩散模型的发展历程</h3>
<p>扩散模型这个想法在2015年就有人提出来了，但是并不能训练很好，生成图像的效果并不如其他的生成模型，比如GAN等。直至2020年有一篇论文——**DDPM对扩散模型的思想进行了改进，使得训练更方便，其核心思想是：在 reverse diffusion 过程中，给定，不去直接预测（训练起来很难），而是预测使变为的噪声。**这将扩散模型的问题简化了很多，直接让网络去预测一个噪声即可（类似于ResNet中去预测一个残差），比如对于正态分布而言，就是预测均值和方差，DDPM作者还发现，将方差设为一个常数，只去预测均值的话，就已经可以使扩散模型生成很好的图像。</p>
<p><strong>给U-Net中加入temporal embedding，目的是使 reverse diffusion 是一个 coarse-to-fine 的过程，让模型知道目前处于哪一步</strong>，希望在最开始先生成一些粗糙的信息，最后快结束时再生成一些细致的信息，即高频的信息（比如物体的边边角角）。损失函数就是：，就是U-Net网络，t是temporal embedding，是前向过程中添加的噪声，是已知的，因此可以拿来当作Ground truth，目的是希望U-Net在每一步预测的噪声与前向过程中的相同。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183935982.png" alt="image.png" /></p>
<p>DDPM与VAE其实是有相似之处，比如DDPM 也可以看做的一个 encoder-decoder 结构，分别对应forward、reverse的过程。但他们也有不同，如下：</p>
<ul>
<li>
<p>DDPM的前向过程是固定的，而VAE的encoder是学习的；</p>
</li>
<li>
<p>DDPM的每一步的特征维度都是相同的，而VAE的bottle neck的维度是比输入小很多的。</p>
</li>
<li>
<p>扩散模型有 step 的概念，有很多步；VAE没有；</p>
</li>
</ul>
<p>在DDPM 证明了扩散模型可以work很好之后，后续出现了很多工作，比如2021年OpenAI的这篇论文**《Diffusion Models Beat GANs on Image Synthesis》**。在这篇论文出现之前扩散模型生成的图像已经很逼真了，但是在各项指标上还比不过GAN，然后这篇论文提出了一个Classifier guidance方法来引导模型生成图片，使得生成的效果更好，并且只做25次采样即可实现。</p>
<p>Classifier guided diffusion的意思是：<strong>在训练diffusion的同时，再去训练一个图片分类器</strong>(classifier，一般是在ImageNet的加了noise的图片上训练)。这个classifier的作用是：有了后，就可以丢给classifier计算一个类别损失，也就是计算出了一个梯度来辅助U-Net的训练过程。这个梯度中暗含了中是否有这个物体，或者说当前的这个物体真不真实的信息，以此来告诉U-Net要在生成的图片中在颜色、纹理等要跟真实的物体匹配上。</p>
<p>这个操作的核心思想是利用梯度来引导diffusion的生成，它牺牲了一定的diversity，来换取生成图片的逼真性。这种思想提出来之后，后续有人思考不用classifier来当作引导信号，而是用CLIP来进行引导，这样文本和图像就可以联系起来了，不再用梯度，而是用文本来引导引导diffusion的采样和生成。所有的引导都是目标函数中的y，也就是输入不只是和t，同时还有一个condition（y），至于是什么就看你往里加入什么。</p>
<p>但是Classifier guided diffusion这类方法也有一个缺陷，就是：必须用另外一个模型来进行guidance，要么是pre-trained，要么是再训练一个，这样的话成本较高且不可控。因此就出现了后续的Classifier free guidance方法，GLIDE、DALLE·2、Imagen都是基于这类方法的思想。Classifier free guidance方法不想用之前的那种guidance，而是希望找到另外一种指导信号。它的思想就是在训练时同时做两个输出，一个有条件的，一个没条件的，最后就会知道二者的差距是多少，这样在测试时，在没有条件引导的情况下，也可以推测出有条件引导的输出是多少。但是这种训练成本也很高，同时要有两个输出。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183944101.png" alt="image.png" /></p>
<p>GLIDE采用了classifier free guidance的扩散模型，可以实现很好的图像生成。OpenAI也因此摒弃了DALL·E用VQ-VAE的思路，在DALL·E 2中转而使用扩散模型来进行图像生成，也就是基于GLIDE，并在其之前加入了prior网络，以及一些层级生成的技巧等。</p>
<h1 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h1>
<p><strong>像CLIP这些对比学习的模型已经可以学习到很稳健的图像特征，既能捕捉语义信息，又能捕捉风格信息</strong>。这种良好特征只用来做分类就很可惜，因此为了借助于这种良好特征来完成图像生成任务，我们提出了一个<strong>两阶段的模型：a prior （给定一个文本，先用现成的CLIP模型生成对应的textual embedding，然后接下来用这个textual embedding生成image embedding的过程就叫做prior），and a decoder（基于image embedding生成图像）</strong>。此外，由于是text-to-image任务，因此可以基于CLIP，可以实现zero-shot。</p>
<p>作者发现： 显式地生成图像特征的这种方式（就是先从文本生成image embedding，再将其解码为Image），可以很显著地提升生成图像的diversity（多样性）。</p>
<h1 id="dalle-2-模型解读"><a class="markdownIt-Anchor" href="#dalle-2-模型解读"></a> DALL·E 2 模型解读</h1>
<h2 id="总览"><a class="markdownIt-Anchor" href="#总览"></a> 总览</h2>
<p>对于一段话，DALLE 2是能够捕捉其中的层次关系，并在画图时也考虑进来的。比如给出下面这句话，DALLE 2 生成的图片，是真的把熊画在滑板之上了，也就是说捕捉到并解码出目标之间的关联关系的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183953965.png" alt="image.png" /></p>
<p>在DALLE 2这篇论文里，CLIP这个模型是锁住的，只是被用，没有被训练。DALLE 2模型的整体框架如下，虚线之上是CLIP 模型，虚线之下才是DALLE 2模型。unCLIP是指：CLIP的目的是从文本、图像中获取特征，本文的方法是为了从特征中还原出图像，因此叫unCLIP。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184000723.png" alt="image.png" /></p>
<p>DALLE 2的训练数据是图像文本对(x, y)，过程分以下两个阶段：</p>
<ul>
<li>
<p><strong>Prior阶段</strong>：**对于一个文本x，经由fixed CLIP可以产生一个texual embedding（蓝色）<strong>和Image embedding（红色），产生的texual embedding经过 autoregressive或difussion的prior模型生成Image embedding，这一个阶段</strong>用刚才CLIP中生成的Image embedding对其做监督。**这样做的目的是：等在做推理的时候，即使只有文本特征，也可以生成比较好的图像特征。</p>
</li>
<li>
<p><strong>Decoder阶段</strong>：用于从Image embedding中解码出图像。</p>
</li>
</ul>
<p>如果DALLE 2的输入是图像的话，会通过CLIP生成文本特征，然后再经过prior、decoder生成图像。</p>
<h2 id="训练过程"><a class="markdownIt-Anchor" href="#训练过程"></a> 训练过程</h2>
<p>DALL·E 2是将其子模块分开训练的，最后将这些训练好的子模块拼接在一起，最后实现由文本生成图像的功能。</p>
<p><strong>1. 训练CLIP，使其能够编码文本和对应图像</strong></p>
<p>这一步是与CLIP模型的训练方式完全一样的，目的是能够得到训练好的text encoder和img encoder。这么一来，文本和图像都可以被编码到相应的<a href="https://zhida.zhihu.com/search?content_id=205119088&amp;content_type=Article&amp;match_order=1&amp;q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&amp;zhida_source=entity">特征空间</a>。对应上图中的虚线以上部分。</p>
<p><strong>2. 训练prior，使文本编码可以转换为图像编码</strong></p>
<p>论文中对于该步骤作用的解释为：</p>
<blockquote>
<p>A prior <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(z_i|y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>that produces CLIP image embeddings<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>conditioned on captions<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>.</p>
</blockquote>
<p>实际的训练过程为：将CLIP中训练好的text encoder拿出来，输入文本<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>，得到文本编码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。同样的，将CLIP中训练好的img encoder拿出来，输入图像<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>得到图像编码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。我们希望prior能从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>获取相对应的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。假设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>经过prior输出的特征为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">z_i&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.010556em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-2.441336em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>，那么我们自然希望<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">z_i&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.010556em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-2.441336em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>越接近越好，这样来更新我们的prior模块。最终训练好的prior，将与CLIP的text encoder串联起来，它们可以根据我们的输入文本<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>生成对应的图像编码特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>了。关于具体如何训练prior，可以精度一下原文，作者使用了<a href="https://zhida.zhihu.com/search?content_id=205119088&amp;content_type=Article&amp;match_order=1&amp;q=%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95&amp;zhida_source=entity">主成分分析法</a>PCA来提升训练的稳定性。</p>
<p>在DALL·E 2 模型中，作者团队尝试了两种先验模型：自回归式Autoregressive (AR) prior 和扩散模型Diffusion prior。实验效果上发现两种模型的性能相似，而因为扩散模型效率较高，因此最终选择了扩散模型作为prior模块。</p>
<p><strong>3. 训练decoder生成最终的图像</strong></p>
<p>论文中对于该步骤作用的解释为：</p>
<blockquote>
<p>A decoder<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x|z_i,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> that produces images <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> conditioned on CLIP image embeddings <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (and optionally text captions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> ).</p>
</blockquote>
<p>也就是说我们要训练decoder模块，从<a href="https://zhida.zhihu.com/search?content_id=205119088&amp;content_type=Article&amp;match_order=1&amp;q=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&amp;zhida_source=entity">图像特征</a>﻿<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>还原出真实的图像<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>，如下图左边所示。这个过程与<a href="https://zhida.zhihu.com/search?content_id=205119088&amp;content_type=Article&amp;match_order=1&amp;q=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;zhida_source=entity">自编码器</a>类似，从中间特征层还原出输入图像，但又不完全一样。我们需要生成出的图像，只需要保持原始图像的显著特征就可以了，这样以便于多样化生成，例如下图右边的示例。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184009679.png" alt="image.png" /></p>
<p>DALL-E 2使用的是改进的GLIDE模型 [2]。这个模型可以根据CLIP图像编码的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，还原出具有相同与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>有相同语义，而又不是与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>完全一致的图像。</p>
<h2 id="推理过程由文本生成图像过程"><a class="markdownIt-Anchor" href="#推理过程由文本生成图像过程"></a> 推理过程（由文本生成图像过程）</h2>
<p>经过以上三个步骤的训练，已经可以完成DALL·E 2预训练模型的搭建了。我们这事丢掉CLIP中的img encoder，留下CLIP中的text encoder，以及新训练好的prior和decoder。这么一来流程自然很清晰了：由text encoder将文本进行编码，再由prior将文本<a href="https://zhida.zhihu.com/search?content_id=205119088&amp;content_type=Article&amp;match_order=1&amp;q=%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2&amp;zhida_source=entity">编码转换</a>为图像编码，最后由decoder进行解码生成图像。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184021236.png" alt="image.png" /></p>
<h1 id="应用"><a class="markdownIt-Anchor" href="#应用"></a> 应用</h1>
<p>DALL·E 2的几种应用场景：</p>
<ul>
<li><strong>text-to-image</strong></li>
</ul>
<p>DALL·E 2可以生成各式各样的沙发，变换颜色、样式、位置</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184027705.png" alt="image.png" /></p>
<ul>
<li><strong>image-to-image</strong></li>
</ul>
<p>给定一个图像，可以生成很多风格类似的图像（整体语义信息不变）：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184033514.png" alt="image.png" /></p>
<ul>
<li><strong>通过对两个图像的特征做内插，生成新图像</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184039857.png" alt="image.png" /></p>
<ul>
<li><strong>通过对两个文本的特征做内插，生成新图像</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184046392.png" alt="image.png" /></p>
<h1 id="不足"><a class="markdownIt-Anchor" href="#不足"></a> 不足</h1>
<ul>
<li><strong>DALL-E 2不能很好的将attribute绑定到 object 上。</strong></li>
</ul>
<p>比如下面这个图，object就是方块，属性就是颜色。给一句话：“a red cube on top of a blue cube”，GLIDE的效果要比DALL-E 2好很多。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184054203.png" alt="image.png" /></p>
<p>作者解释很可能是使用CLIP的原因。虽然使用CLIP后可以是图像和文本的联系更紧密，这使得更容易去做文本生成图像的任务。但是CLIP模型学习的时候，只是考虑相似性，它只是去找红方块、蓝方块来把相似性提升到最高就行了，但是CLIP模型不了解&quot;on top of&quot;这种东西，不了解什么叫做“上下左右”，它从头到尾都在找物体间的相似性。</p>
<p>因此在做CLIP特征做下游任务的时候，就不能很好的区分object和其对应的attribute。</p>
<p>此外，在对图片进行重建时，decoder会将object的attribute混淆。</p>
<ul>
<li><strong>直接生成文字，效果很差</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184100962.png" alt="image.png" /></p>
<p>作者解释可能是使用BPE编码的原因，这种是词根词缀编码。但可能还有其他原因。</p>
<h1 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h1>
<ul>
<li>
<p><a href="https://blog.csdn.net/DUDUDUTU/article/details/129438597">DALL·E 2 论文阅读笔记_dalle论文-CSDN博客</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/526438544">https://zhuanlan.zhihu.com/p/526438544</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT系列</title>
    <url>/2025/01/09/GPT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p>GPT整体的模型架构都是以Transformer的解码器为模块进行堆叠而成。主要的创新点集中在模型训练策略，还有就是讲究一个大力出奇迹。</p>
<h2 id="论文时间轴"><a class="markdownIt-Anchor" href="#论文时间轴"></a> 论文时间轴</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170449688.png" alt="image.png" /></p>
<span id="more"></span>
<p><strong>GPT</strong>：Transformer解码器，在没有标号的大量的文本数据上，训练一个语言模型，来获得预训练模型，后续在子任务上做微调，得到每一个任务所用的分类器。</p>
<p><strong>BERT</strong>：Transformer编码器，收集了一个更大的数据集，用来做预训练，效果比GPT好。BERT有两个模型，BERT-base模型与GPT模型参数相差不大，BERT-Large比BERT-base模型大。</p>
<blockquote>
<p>Transformer和BERT来自Google，想要解决的问题都比较小；Transformer就想解决机器翻译这样的问题，从一个序列到另外一个序列；BERT想把计算机视觉中成熟的那一套预训练模型应用到NLP中。在同样大小的模型上，BERT的效果是要好于GPT的。所以，后续的工作，非常愿意使用BERT，而不是GPT。</p>
</blockquote>
<p><strong>GPT-2</strong>：原作者吸取教训，收集更大的数据集，训练了一个更大的模型（15亿参数），GPT-2的模型比BERT-large要大。继续使用Transformer的解码器，发现非常适合做Zero Shot，步子跨的太大，效果上不是那么好。</p>
<p><strong>GPT-3</strong>：GPT-3对GPT-2的改进就是数据和模型都大了100倍（1750亿参数）。大力出奇迹，效果非常惊艳。几乎没有什么团队去复现结果。</p>
<p><strong>InstructGPT</strong> ：使用了指示学习（Instruction Learning）和人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）来指导模型的训练。<strong>ChatGPT</strong> (也被称为<strong>GPT3.5</strong>)就是使用了InstructGPT的模型结构和训练方法。</p>
<p><strong>GPT-4</strong>：改进了 GPT-3.5 的架构，参数数量显著增加（具体参数量未公开，传言有1.8万亿参数），并引入了更多的训练数据和新的优化方法。这使得 GPT-4.0 在处理复杂任务和理解更细微的上下文时表现更好。</p>
<h2 id="gpt"><a class="markdownIt-Anchor" href="#gpt"></a> GPT</h2>
<p>论文标题：“Improving Language Understanding by Generative Pre-Training”，2018.6.</p>
<p>在自然语言处理领域，有很多各式各样的的任务，如问答，文本分类等。然而，现有的无标签文本非常多，而有标签的文本很少，这使得在这些有标签文本训练一个好的分辨模型很难，因为数据集太少。因此GPT第一个版本主要就是为了解决这个问题而提出的一套针对语言模型的预训练方法，使得大量的无标签数据能够被使用，并且对预训练好的模型加以微调使其适用于许多的下游任务。在微调时，构造与子任务相关的输入，从而之只需要很少改变模型架构。</p>
<h3 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h3>
<p>如何利用无标签的数据，当时最成功的还是词嵌入模型（Word Embeddings）。</p>
<p>使用无标记数据时，需要的两个困难：</p>
<ul>
<li>
<p>损失函数设计困难：不清楚什么样的优化目标对文本有效</p>
</li>
<li>
<p>特征迁移困难：怎么样把学习到的问题表示，传递到下游的子任务上；没有一种表示，能够迁移到所有子任务上</p>
</li>
</ul>
<p>提出一个半监督的方法（预训练+微调）：在没有标号的数据上，训练一个比较大的语言模型，然后再再子任务上微调。</p>
<ul>
<li>半监督学习：有一些标记的数据，但还有大量没有标记的相似数据，怎么把这些没有标记的数据用过来。</li>
</ul>
<p>后续的BERT、GPT的方法，预训练 + 微调，不再叫做半监督学习，而叫做自监督学习。</p>
<p>GPT架构使用Transformer块，相比于RNN，在做迁移学习时，Transformer块学到的特征更加稳健。作者猜测Transformer里面有更结构化的记忆，使得能够处理更长的问题信息，从而能偶抽取出句子层面、段落层面的语义信息。在迁移时，使用任务相关的输入表示。</p>
<h3 id="训练"><a class="markdownIt-Anchor" href="#训练"></a> 训练</h3>
<h4 id="无监督预训练预训练"><a class="markdownIt-Anchor" href="#无监督预训练预训练"></a> 无监督预训练：预训练</h4>
<ul>
<li>
<p>什么最大似然函数<br />
GPT 的目标是最大化以下联合概率：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>x</mi><mi>T</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mn>2</mn><mo>∣</mo><mi>x</mi><mn>1</mn><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mn>3</mn><mo>∣</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo><mo>⋯</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi>T</mi><mo>∣</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>x</mi><mi>T</mi><mtext>−</mtext><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(X)=P(x1,x2,…,xT)=P(x1)⋅P(x2∣x1)⋅P(x3∣x1,x2)⋯P(xT∣x1,x2,…,xT−1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></p>
<p>最大似然估计的目标是通过选择模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 来最大化训练数据的似然函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\theta) = \prod_{t=1}^{T} P(x_t | x_1, x_2, \dots, x_{t-1}; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></p>
<p>为了方便计算，通常将这个似然函数取对数，转化为对数似然（Log-Likelihood）：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log L(\theta) = \sum_{t=1}^{T} \log P(x_t | x_1, x_2, \dots, x_{t-1}; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></p>
</li>
</ul>
<p><strong>GPT的目标函数</strong></p>
<p>给定一个未监督的语料信息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo>=</mo><mrow><mo fence="true">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mi>n</mi></msub><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{U}=\left\{u_{1}, \ldots, u_{n}\right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.09931em;">U</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span>，使用标准的语言模型，使下面这个似然函数最大化：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">U</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mi>P</mi><mrow><mo fence="true">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>u</mi><mrow><mi>i</mi><mo>−</mo><mi>k</mi></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mi mathvariant="normal">Θ</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{1}(\mathcal{U})=\sum_i \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathcal" style="margin-right:0.09931em;">U</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0497100000000001em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Θ</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></p>
<p>其中，_k_为上下文窗口大小，Θ为模型参数。</p>
<p>简单来说就是根据上文的k个单词，预测下一个最大概率的单词<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">u_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>使用多层Transformer decoder块作为语言模型（标准的语言模型，根据已有的词进行下一个词的预测，不能使它看到所有的词，所以只有解码器）。</p>
<p>模型输入输出如下所示</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>h</mi><mn>0</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>U</mi><msub><mi>W</mi><mi>e</mi></msub><mo>+</mo><msub><mi>W</mi><mi>p</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>h</mi><mi>l</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi mathvariant="normal">transformer_block</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>h</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">∀</mi><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>h</mi><mi>n</mi></msub><msubsup><mi>W</mi><mi>e</mi><mi>T</mi></msubsup><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} h_{0} &amp; =U W_{e}+W_{p} \\ h_{l} &amp; =\operatorname{transformer\_ block}\left(h_{l-1}\right) \forall i \in[1, n] \\ P(u) &amp; =\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.551331000000001em;vertical-align:-2.0256655000000006em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5256655em;"><span style="top:-4.6856655em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.1856655em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.6343344999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0256655000000006em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5256655em;"><span style="top:-4.6856655em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.1856655em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mop"><span class="mord mathrm">t</span><span class="mord mathrm">r</span><span class="mord mathrm">a</span><span class="mord mathrm">n</span><span class="mord mathrm">s</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">m</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathrm">b</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">c</span><span class="mord mathrm">k</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∀</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">]</span></span></span><span style="top:-1.6343344999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0256655000000006em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>u</mi><mrow><mo>−</mo><mi>k</mi></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mrow><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">U=\left(u_{-k}, \ldots, u_{-1}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span>为上下文tokens向量，𝑛为transformer的层数，𝑊为词嵌入矩阵维度，𝑊𝑝为位置编码矩阵。</p>
<h4 id="有监督微调"><a class="markdownIt-Anchor" href="#有监督微调"></a> 有监督微调</h4>
<p>在得到预训练模型后，就使用有标签的数据进行微调。具体来说每一次我给你一个长为m的一个词的序列，然后告诉你这个序列它对应的标号是y，也就是每次给定一个序列预测他这个y。微调优化目标是最小化分类目标函数。</p>
<p>NLP领域四大常见的应用：</p>
<ul>
<li>
<p>分类：给定一段话或文本，判断所属标号；如用户的评价是正面还是负面的。一段文本，在前面加入开始词元[Start]，在后面加入抽取词元[Extract]，做成一个序列，放入Transformer解码器中，模型对最后一个词抽取的特征[Extract]放入线性层进行分类。</p>
</li>
<li>
<p>蕴含：给一段话，再给一个假设，判断前面一段话是否蕴含提出的假设。如文本：a给b送一朵玫瑰；假设：a喜欢b。判断前面文本是否支持假设。即两端文本做三分类问题，支持，不支持，既不支持也不反对。将两端问题串成一个序列，使用开始符，分隔符，抽取符。</p>
</li>
<li>
<p>相似：判断两段文字是不是相似。如一个搜索词与文档是不是相似，或两个文档相似去重。相似是对称的，a和b相似，则b和a也相似。所以，做了两个序列，两个序列的文字顺序不同，再使用开始符，分隔符，抽取符。得到两段输出后，经过Transformer，在做加号，最后经过线性层输出，得到是相似还是不是相似的二分类问题。</p>
</li>
<li>
<p>多选题：问一个问题，给几个答案，选择最可能的几个答案。如果有n个答案，构造n个序列，前面是问题，每一个答案作为第二个序列；每个序列进入Transformer块，最后经过线性层，输出答案的置信度；最后做一个softmax。</p>
</li>
</ul>
<p>复用预训练的Transformer的结构，加一个线性层，不同的任务需要不同的输入。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170604083.png" alt="image.png" /></p>
<h2 id="gpt2"><a class="markdownIt-Anchor" href="#gpt2"></a> GPT2</h2>
<p>论文标题：Language Models are Unsupervised Multitask Learners，2019</p>
<p>自从GPT提出后，Google紧随其后提出了BERT预训练模型，采用更大的模型和更大的数据，在各方面效果都要优于GPT。作为竞争对手，GPT当然是要反击的，那怎么做呢？如果单纯加大模型于增加数据量，或许能击败BERT，但是却少了些新意，因此GPT2从另外一个角度，除了加大模型和数据量，还引入了zero-shot设定，就是在做下游任务是，不需要下游任务的任何标签信息，也不需要重新训练一个模型，即在更难的一个设定上体现他的一个新意度。</p>
<h3 id="引言-2"><a class="markdownIt-Anchor" href="#引言-2"></a> 引言</h3>
<p>现在主流的机器学习系统训练方式为：一个任务收集一个数据集，然后再上面做模型训练和预测，因现在模型泛化性能不是很好。</p>
<p>多任务学习：训练一个模型时，同时看多个数据集，通过多个损失函数，来达到一个模型能够在多个任务上都能用。但是NLP中使用的不多，主要使用的还是预训练+微调方式。但还是有两个问题：第一个是对每一个下游任务，需要重新训练模型；二是需要收集有标号的数据才行。这样导致，拓展到新的任务上，还是有成本的。</p>
<p><strong>GPT-2还是在做语言模型，在到下游任务时，会使用zero-shot的设定（不需要下游任务的标注信息，不引入模型没有见过的特殊符号），这样训练一个模型，任何地方都可以用。</strong></p>
<h3 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h3>
<h4 id="gpt-1和gpt-2区别"><a class="markdownIt-Anchor" href="#gpt-1和gpt-2区别"></a> GPT-1和GPT-2区别</h4>
<ul>
<li>GPT-1时，根据不同的下游任务，会调整输入信息，会加入开始符、分隔符、结束符等信息，然后在使用有标记的数据进行微调，让模型去认识这些符号。而GPT-2做下游任务时，不再加入开始符、分隔符、结束符等模型未见过的信息，而是采用zero-shot的设定。GPT-2下游任务模型的输入，和预训练时，模型看到的输入是一样的。</li>
</ul>
<p>例如：</p>
<ul>
<li>
<p>英语翻译为法语：translate to french, english text, french text</p>
</li>
<li>
<p>做阅读理解：answer the question, document, question, answer</p>
</li>
</ul>
<p>这个提示符后面叫做Prompt。</p>
<h4 id="训练数据"><a class="markdownIt-Anchor" href="#训练数据"></a> 训练数据</h4>
<p>BERT训练数据：Wikipedia</p>
<p>Common Crawl项目：一个公开的爬虫项目，不断抓取网页放在AWS上，是能下载到最大的文本数据集，TB级别的数据量。但作者认为这个不好用，因为信噪比比较低，抓取的网页，较多是没有信息的网页。GPT-2使用Reddit里面的数据，选取最近3词评论以上的数据，得到4500万分链接，将数据抽取出来，得到一个数据集，约800万文本，40GB的文字。</p>
<h4 id="gpt-2模型大小"><a class="markdownIt-Anchor" href="#gpt-2模型大小"></a> GPT-2模型大小</h4>
<p>GPT2也是基于Transformer解码器的架构，作者设计了4种大小的模型，参数结构如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170614852.png" alt="image.png" /></p>
<p>可以看到，最大模型的参数量已经去到了15个亿。还有一个细节就是GPT2调整了transformer解码器结构：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。</p>
<h4 id="训练方式"><a class="markdownIt-Anchor" href="#训练方式"></a> 训练方式</h4>
<p>采用预训练+zero-shot的训练范式。为实现zero-shot，GPT2在做下游任务时，输入就不能像GPT那样在构造输入时加入开始、中间和结束的特殊字符，因为这些特殊字符是模型在预训练时没有见过的。正确的输入应该和预训练模型看到的文本一样，更像一个自然语言。比如在做机器翻译时，直接可以输入“请将下面一段英文翻译成法语，英文文本”，由于在训练时可能已经存在很多这样的翻译文本样例，因此模型就可以实现输出一段法语。</p>
<h2 id="gpt3"><a class="markdownIt-Anchor" href="#gpt3"></a> GPT3</h2>
<p>论文题目：Language Models are Few-Shot Learners，2020</p>
<p>GPT-3：自回归语言模型，有1750亿个可学习参数，比之前非稀疏模型在可学习参数上要大10倍。GPT-3作用到子任务上，不做任何的梯度更新或是微调；GPT-3可以生成一些人类难以区分的文章。</p>
<h3 id="引言-3"><a class="markdownIt-Anchor" href="#引言-3"></a> 引言</h3>
<p>GPT2实验采用了zero-shot设定，在新意度上很高，但是有效性却比较低。而GPT3则是尝试解决GPT2的有效性，因此回到了GPT提到的Few-shot设置，即模型在做下游任务时，可以看到一些任务的样例，而不是像GPT2那样啥样例都不给。此外，GPT3还是只采用无监督预训练方式，那么传统的二阶段训练方式（预训练+微调）有什么问题？二阶段训练方式在预训练好一个模型后，还需要一个与任务相关的数据集，以及跟任务相关的微调方式。去除这种方式是可取的，有以下几个原因：</p>
<ul>
<li>
<p>微调需要一个较大的有标签数据，对于一些如问答型任务，做标签是很困难的；</p>
</li>
<li>
<p>当一个样本没有出现在数据分布里是，微调模型的泛化能力不一定好，即尽管微调效果好，也不一定说明预训练的模型泛化能力好，因为极有可能微调是过拟合了预训练的训练数据，或者说预训练训练数据和下游任务数据有一定重合性，所以效果会好一点；</p>
</li>
<li>
<p>以人类角度来阐述为什么不用微调，就是说人类做任务不需要一个很大的数据集进行微调，比如一个人有一定的语言功底，让你去做一个别的事情，可能就是告诉你怎么做并提供几个样例就可以了，GPT3就是采用一样的思想。</p>
</li>
</ul>
<p>总的来说，GPT3就是一个参数特别大，效果也很好的一个模型。</p>
<p><strong>多任务预训练（上下文训练</strong>，类似于 meta-learning 元学习的想法，元学习的核心思想在于通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果**）**</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170624378.png" alt="image.png" /></p>
<p>模型评估方式（3种）：</p>
<ul>
<li>
<p>few-shot learning：语境学习允许尽可能多的示例活动将适合模型的上下文窗口(通常10 - 100)</p>
</li>
<li>
<p>one-shot learning：只提供一个示例，如英语翻译德语时，只提供第一个词怎么翻译，后续让模型自己翻译</p>
</li>
<li>
<p>zero-shot：一个示例样本都没有</p>
</li>
</ul>
<p>在三个设定下，模型的学习区别，x轴为语言模型的大小，其中虚线是每个子任务，做平均变成了实线。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170634563.png" alt="image.png" /></p>
<h3 id="模型-2"><a class="markdownIt-Anchor" href="#模型-2"></a> 模型</h3>
<h4 id="训练方式-2"><a class="markdownIt-Anchor" href="#训练方式-2"></a> 训练方式</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170649230.png" alt="image.png" /></p>
<ul>
<li>
<p>Fine-tuning：训练完成预训练模型后，在每一个子任务上提供训练样本；微调对数据量的要求少于从0开始训练；</p>
</li>
<li>
<p>Zero-shot：任务描述和prompt之间没有任何样本</p>
</li>
<li>
<p>One-shot：任务描述和prompt之前，插入一个样本进来。样本只做预测，不做训练，模型在前向推理时，使用注意力机制，处理比较长的信息，从中间抽取出有用的信息。</p>
</li>
<li>
<p>Few-shot：任务描述和prompt之前，插入多个样本进来。多个不一定有用，可能模型不能处理很长的数据。</p>
</li>
</ul>
<p>几种训练方式简单概括如下：</p>
<ul>
<li>
<p>fine-tuning：预训练 + 微调计算loss更新梯度，然后预测。会更新模型参数</p>
</li>
<li>
<p>zero-shot：预训练 + task description + prompt，直接预测。不更新模型参数</p>
</li>
<li>
<p>one-shot：预训练 + task description + example + prompt，预测。不更新模型参数</p>
</li>
<li>
<p>few-shot：预训练 + task description + examples + prompt，预测。不更新模型参数</p>
</li>
</ul>
<h4 id="模型架构"><a class="markdownIt-Anchor" href="#模型架构"></a> 模型架构</h4>
<p>GPT-3的模型和GPT-2的模型是一样的，稍微有点改动，把transformer换成了Sparse Transformer中的结构，并设计8个不同大小的模型。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170702577.png" alt="image.png" /></p>
<ul>
<li>
<p>整体：GPT3模型偏扁</p>
</li>
<li>
<p>Batch Size：使用相对比较大的批量大小，计算性能更好，每台机器的并行度更高，通讯量变低，降低批量里的噪音分布式比较好，小的模型批量大小更容易过拟合—些。</p>
</li>
<li>
<p>过拟合：模型越来越大的时候过拟合没有那么的严重，搜索范围更广，可能存在一个比较简单的模型架构，SDG可以帮助找到那个模型，使泛化精度更好一些。</p>
</li>
<li>
<p>学习率模型批量大小增大学习率下降。</p>
</li>
</ul>
<p><strong>Sparse Transformer</strong></p>
<ul>
<li>
<p>Self-Attention：每个 token 之间两两计算 attention，复杂度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mtext>²</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n²)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord">²</span><span class="mclose">)</span></span></span></span></p>
</li>
<li>
<p>Sparse Attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mtext>∗</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n∗logn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></p>
</li>
<li>
<p>Sparse Attention 除了相对距离不超过k以及相对距离为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mtext>，</mtext><mn>2</mn><mi>k</mi><mtext>，</mtext><mn>3</mn><mi>k</mi><mtext>，</mtext><mo>…</mo></mrow><annotation encoding="application/x-tex">k，2k，3k，…</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord cjk_fallback">，</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord cjk_fallback">，</span><span class="mord">3</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord cjk_fallback">，</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span></span></span></span>的 token，其他所有token的注意力都设为0：</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170712746.png" alt="image.png" /></p>
<h2 id="instructgptchatgptgpt35"><a class="markdownIt-Anchor" href="#instructgptchatgptgpt35"></a> InstructGPT/ChatGPT/GPT3.5</h2>
<p>InstructGPT 使用了指示学习（Instruction Learning）和人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）来指导模型的训练。ChatGPT (也被称为GPT3.5) 和InstructGPT在模型结构，训练方式上都完全一致，它们不同的仅仅是采集数据的方式上有所差异。</p>
<p>预训练模型自诞生之始，一个备受诟病的问题就是预训练模型的偏见性。因为预训练模型都是通过海量数据在超大参数量级的模型上训练出来的，对比完全由人工规则控制的专家系统来说，预训练模型就像一个黑盒子。没有人能够保证预训练模型不会生成一些包含种族歧视，性别歧视等危险内容，因为它的几十GB甚至几十TB的训练数据里几乎肯定包含类似的训练样本。这也就是InstructGPT和ChatGPT的提出动机，论文中用3H概括了它们的优化目标：</p>
<ul>
<li>
<p>有用的（Helpful）;</p>
</li>
<li>
<p>可信的（Honest）;</p>
</li>
<li>
<p>无害的（Harmless）。</p>
</li>
</ul>
<h3 id="指示学习instruct-learning和提示prompt-learning学习"><a class="markdownIt-Anchor" href="#指示学习instruct-learning和提示prompt-learning学习"></a> 指示学习（Instruct Learning）和提示（Prompt Learning）学习</h3>
<p>指示学习是谷歌Deepmind的Quoc V.Le团队在2021年的一篇名为《Finetuned Language Models Are Zero-Shot Learners》文章中提出的思想。指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。不同的是Prompt是激发语言模型的<strong>补全能力</strong>，例如根据上半句生成下半句，或是完形填空等。Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。我们可以通过下面的例子来理解这两个不同的学习方式：</p>
<ol>
<li>
<p>提示学习：给女朋友买了这个项链，她很喜欢，这个项链太____了。</p>
</li>
<li>
<p>指示学习：这句话的情感是非常正向的：给女朋友买了这个项链，她很喜欢。</p>
</li>
</ol>
<p>指示学习的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示学习都是针对一个任务的。泛化能力不如指示学习。我们可以通过下图来理解微调，提示学习和指示学习。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170721839.png" alt="image.png" /></p>
<h3 id="人类反馈的强化学习-rlhf"><a class="markdownIt-Anchor" href="#人类反馈的强化学习-rlhf"></a> 人类反馈的强化学习 （RLHF）</h3>
<p>因为训练得到的模型并不是非常可控的，模型可以看做对训练集分布的一个拟合。那么反馈到生成模型中，训练数据的分布便是影响生成内容的质量最重要的一个因素。有时候我们希望模型并不仅仅只受训练数据的影响，而是人为可控的，从而保证生成数据的有用性，真实性和无害性。论文中多次提到了对齐（Alignment）问题，我们可以理解为模型的输出内容和人类喜欢的输出内容的对齐，人类喜欢的不止包括生成内容的流畅性和语法的正确性，还包括生成内容的有用性、真实性和无害性。</p>
<p>我们知道强化学习通过奖励（Reward）机制来指导模型训练，奖励机制可以看做传统模型训练机制的损失函数。奖励的计算要比损失函数更灵活和多样（AlphaGO的奖励是对局的胜负），这带来的代价是奖励的计算是不可导的，因此不能直接拿来做反向传播。强化学习的思路是通过对奖励的大量采样来拟合损失函数，从而实现模型的训练。同样人类反馈也是不可导的，那么我们也可以将人工反馈作为强化学习的奖励，基于人类反馈的强化学习便应运而生。</p>
<p>RLHF最早可以追溯到Google在2017年发表的《Deep Reinforcement Learning from Human Preferences》，它通过人工标注作为反馈，提升了强化学习在模拟机器人以及雅达利游戏上的表现效果。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170730546.png" alt="image.png" /></p>
<p>InstructGPT/ChatGPT中还用到了强化学习中一个经典的算法：OpenAI提出的最近策略优化（Proximal Policy Optimization，PPO）<a href="https://zhuanlan.zhihu.com/p/590311003#ref_7">[7]</a>。PPO算法是一种新型的Policy Gradient算法，Policy Gradient算法对步长十分敏感，但是又难以选择合适的步长，在训练过程中新旧策略的的变化差异如果过大则不利于学习。PPO提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。其实TRPO也是为了解决这个思想但是相比于TRPO算法PPO算法更容易求解。</p>
<h3 id="instructgptchatgpt原理解读"><a class="markdownIt-Anchor" href="#instructgptchatgpt原理解读"></a> InstructGPT/ChatGPT原理解读</h3>
<p>有了上面这些基础知识，我们再去了解InstructGPT和ChatGPT就会简单很多。简单来说，InstructGPT/ChatGPT都是采用了<strong>GPT-3</strong>的网络结构，通过<strong>指示学习</strong>构建训练样本来训练一个反应预测内容效果的奖励模型（RM），最后通过这个奖励模型的打分来指导强化学习模型的训练。InstructGPT/ChatGPT的训练流程如图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170739026.png" alt="image.png" /></p>
<p>InstructGPT的计算流程：（1）有监督微调（SFT）；（2）奖励模型（RM）训练；（3）通过PPO根据奖励模型进行强化学习。</p>
<p>从上图中我们可以看出，InstructGPT/ChatGPT的训练可以分成3步，其中第2步和第3步是的奖励模型和强化学习的SFT模型可以反复迭代优化。</p>
<ol>
<li>
<p>根据采集的SFT数据集对GPT-3进行有监督的微调（Supervised FineTune，SFT）；</p>
</li>
<li>
<p>收集人工标注的对比数据，训练奖励模型（Reword Model，RM）；</p>
</li>
<li>
<p>使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型。</p>
</li>
</ol>
<p>我们将分别介绍InstructGPT/ChatGPT的数据集采集和模型训练两个方面的内容。</p>
<h4 id="数据集采集"><a class="markdownIt-Anchor" href="#数据集采集"></a> 数据集采集</h4>
<p>InstructGPT/ChatGPT的训练分成3步，每一步需要的数据也有些许差异，下面我们分别介绍它们。</p>
<h5 id="sft数据集"><a class="markdownIt-Anchor" href="#sft数据集"></a> SFT数据集</h5>
<p>SFT数据集是用来训练第1步有监督的模型，即使用采集的新数据，按照GPT-3的训练方式对GPT-3进行微调。因为GPT-3是一个基于提示学习的生成模型，因此SFT数据集也是由提示-答复对组成的样本。SFT数据一部分来自使用OpenAI的PlayGround的用户，另一部分来自OpenAI雇佣的40名标注工（labeler）。并且他们对labeler进行了培训。在这个数据集中，标注工的工作是根据内容自己编写指示，并且要求编写的指示满足下面三点：</p>
<ul>
<li>
<p><strong>简单任务</strong>：labeler给出任意一个简单的任务，同时要确保任务的多样性；</p>
</li>
<li>
<p><strong>Few-shot任务</strong>：labeler给出一个指示，以及该指示的多个查询-响应对；</p>
</li>
<li>
<p><strong>用户相关的</strong>：从接口中获取用例，然后让labeler根据这些用例编写指示。</p>
</li>
</ul>
<h5 id="rm数据集"><a class="markdownIt-Anchor" href="#rm数据集"></a> RM数据集</h5>
<p>RM数据集用来训练第2步的奖励模型，我们也需要为InstructGPT/ChatGPT的训练设置一个奖励目标，要尽可能全面且真实的对齐我们需要模型生成的内容。很自然的，我们可以通过人工标注的方式来提供这个奖励，通过人工对可以给那些涉及偏见的生成内容更低的分从而鼓励模型不去生成这些人类不喜欢的内容。InstructGPT/ChatGPT的做法是先让模型生成一批候选文本，让后通过labeler根据生成数据的质量对这些生成内容进行排序。</p>
<h5 id="ppo数据集"><a class="markdownIt-Anchor" href="#ppo数据集"></a> PPO数据集</h5>
<p>InstructGPT的PPO数据没有进行标注，它均来自GPT-3的API的用户。既又不同用户提供的不同种类的生成任务，其中占比最高的包括生成任务（45.6%），QA（12.4%），头脑风暴（11.2%），对话（8.4%）等。</p>
<h5 id="数据分析"><a class="markdownIt-Anchor" href="#数据分析"></a> 数据分析</h5>
<p>因为InstructGPT/ChatGPT是在GPT-3基础上做的微调，而且因为涉及了人工标注，它们数据总量并不大。</p>
<p>论文的附录A对数据的分布进行了更详细的讨论，这里我列出几个可能影响模型效果的几项：</p>
<ul>
<li>
<p>数据中96%以上是英文，其它20个语种例如中文，法语，西班牙语等加起来不到4%，这可能导致InstructGPT/ChatGPT能进行其它语种的生成，但效果应该远不如英文；</p>
</li>
<li>
<p>提示种类共有9种，而且绝大多数是生成类任务，可能会导致模型有覆盖不到的任务类型；</p>
</li>
<li>
<p>40名外包员工来自美国和东南亚，分布比较集中且人数较少， InstructGPT/ChatGPT的目标是训练一个价值观正确的预训练模型，它的价值观是由这40个外包员工的价值观组合而成。而这个比较窄的分布可能会生成一些其他地区比较在意的歧视，偏见问题。</p>
</li>
</ul>
<p>此外，ChatGPT的博客中讲到ChatGPT和InstructGPT的训练方式相同，不同点仅仅是它们采集数据上有所不同，但是并没有更多的资料来讲数据采集上有哪些细节上的不同。考虑到ChatGPT仅仅被用在对话领域，这里我猜测ChatGPT在数据采集上有两个不同：1. 提高了对话类任务的占比；2. 将提示的方式转换Q&amp;A的方式。当然这里也仅仅是猜测，更准确的描述要等到ChatGPT的论文、源码等更详细的资料公布我们才能知道。</p>
<h4 id="训练任务"><a class="markdownIt-Anchor" href="#训练任务"></a> 训练任务</h4>
<p>我们刚介绍到InstructGPT/ChatGPT有三步训练方式。这三步训练会涉及三个模型：SFT，RM以及PPO，下面我们详细介绍它们。</p>
<h5 id="有监督微调sft"><a class="markdownIt-Anchor" href="#有监督微调sft"></a> 有监督微调（SFT）</h5>
<p>这一步的训练和GPT-3一致，而且作者发现让模型适当过拟合有助于后面两步的训练。</p>
<h5 id="奖励模型rm"><a class="markdownIt-Anchor" href="#奖励模型rm"></a> 奖励模型（RM）</h5>
<p>因为训练RM的数据是一个labeler根据生成结果排序的形式，所以它可以看做一个回归模型。RM结构是将SFT训练后的模型的最后的嵌入层去掉后的模型。它的输入是prompt和Reponse，输出是奖励值。具体的讲，对弈每个prompt，InstructGPT/ChatGPT会随机生成 K 个输出（ 4≤K≤9 ），然后它们向每个labeler成对的展示输出结果，也就是每个prompt共展示 CK2 个结果，然后用户从中选择效果更好的输出。在训练时，InstructGPT/ChatGPT将每个prompt的 CK2 个响应对作为一个batch，这种按prompt为batch的训练方式要比传统的按样本为batch的方式更不容易过拟合，因为这种方式每个prompt会且仅会输入到模型中一次。</p>
<p>奖励模型的损失函数表示为式(1)。这个损失函数的目标是最大化labeler更喜欢的响应和不喜欢的响应之间的差值。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170755396.png" alt="image.png" /></p>
<p>其中 rθ(x,y) 是提示 x 和响应 y 在参数为 θ 的奖励模型下的奖励值， yw 是labeler更喜欢的响应结果， yl 是labeler不喜欢的响应结果。 D 是整个训练数据集。</p>
<h5 id="强化学习模型ppo"><a class="markdownIt-Anchor" href="#强化学习模型ppo"></a> 强化学习模型（PPO）</h5>
<p>强化学习和预训练模型是最近两年最为火热的AI方向之二，之前不少科研工作者说强化学习并不是一个非常适合应用到预训练模型中，因为很难通过模型的输出内容建立奖励机制。而InstructGPT/ChatGPT反直觉的做到了这点，它通过结合人工标注，将强化学习引入到预训练语言模型是这个算法最大的创新点。</p>
<p>PPO的训练集完全来自API。它通过第2步得到的奖励模型来指导SFT模型的继续训练。很多时候强化学习是非常难训练的，InstructGPT/ChatGPT在训练过程中就遇到了两个问题：</p>
<ol>
<li>
<p>问题1：随着模型的更新，强化学习模型产生的数据和训练奖励模型的数据的差异会越来越大。作者的解决方案是在损失函数中加入KL惩罚项 βlog⁡(πϕRL(y∣x)/πSFT(y∣x)) 来确保PPO模型的输出和SFT的输出差距不会很大。</p>
</li>
<li>
<p>问题2：只用PPO模型进行训练的话，会导致模型在通用NLP任务上性能的大幅下降，作者的解决方案是在训练目标中加入了通用的语言模型目标 γEx∼Dpretrain [log⁡(πϕRL(x))] ，这个变量在论文中被叫做PPO-ptx。</p>
</li>
</ol>
<p>综上，PPO的训练目标为</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170802276.png" alt="image.png" /></p>
<h3 id="instructgptchatgpt的性能分析"><a class="markdownIt-Anchor" href="#instructgptchatgpt的性能分析"></a> InstructGPT/ChatGPT的性能分析</h3>
<p>不可否认的是，InstructGPT/ChatGPT的效果是非常棒的，尤其是引入了人工标注之后，让模型的“价值观”和的正确程度和人类行为模式的“真实性”上都大幅的提升。那么，仅仅根据InstructGPT/ChatGPT的技术方案和训练方式，我们就可以分析出它可以带来哪些效果提升呢？</p>
<h4 id="优点"><a class="markdownIt-Anchor" href="#优点"></a> 优点</h4>
<ul>
<li>
<p><strong>InstructGPT/ChatGPT的效果比GPT-3更加真实</strong>：这个很好理解，因为GPT-3本身就具有非常强的泛化能力和生成能力，再加上InstructGPT/ChatGPT引入了不同的labeler进行提示编写和生成结果排序，而且还是在GPT-3之上进行的微调，这使得我们在训练奖励模型时对更加真实的数据会有更高的奖励。作者也在TruthfulQA数据集上对比了它们和GPT-3的效果，实验结果表明甚至13亿小尺寸的PPO-ptx的效果也要比GPT-3要好。</p>
</li>
<li>
<p><strong>InstructGPT/ChatGPT在模型的无害性上比GPT-3效果要有些许提升</strong>：原理同上。但是作者发现InstructGPT在歧视、偏见等数据集上并没有明显的提升。这是因为GPT-3本身就是一个效果非常好的模型，它生成带有有害、歧视、偏见等情况的有问题样本的概率本身就会很低。仅仅通过40个labeler采集和标注的数据很可能无法对模型在这些方面进行充分的优化，所以会带来模型效果的提升很少或者无法察觉。</p>
</li>
<li>
<p><strong>InstructGPT/ChatGPT具有很强的Coding能力</strong>：首先GPT-3就具有很强的Coding能力，基于GPT-3制作的API也积累了大量的Coding代码。而且也有部分OpenAI的内部员工参与了数据采集工作。通过Coding相关的大量数据以及人工标注，训练出来的InstructGPT/ChatGPT具有非常强的Coding能力也就不意外了。</p>
</li>
</ul>
<h4 id="缺点"><a class="markdownIt-Anchor" href="#缺点"></a> 缺点</h4>
<ul>
<li>
<p><strong>InstructGPT/ChatGPT会降低模型在通用NLP任务上的效果</strong>：我们在PPO的训练的时候讨论了这点，虽然修改损失函数可以缓和，但这个问题并没有得到彻底解决。</p>
</li>
<li>
<p><strong>有时候InstructGPT/ChatGPT会给出一些荒谬的输出</strong>：虽然InstructGPT/ChatGPT使用了人类反馈，但限于人力资源有限。影响模型效果最大的还是有监督的语言模型任务，人类只是起到了纠正作用。所以很有可能受限于纠正数据的有限，或是有监督任务的误导（只考虑模型的输出，没考虑人类想要什么），导致它生成内容的不真实。就像一个学生，虽然有老师对他指导，但也不能确定学生可以学会所有知识点。</p>
</li>
<li>
<p><strong>模型对指示非常敏感</strong>：这个也可以归结为labeler标注的数据量不够，因为指示是模型产生输出的唯一线索，如果指示的数量和种类训练的不充分的话，就可能会让模型存在这个问题。</p>
</li>
<li>
<p><strong>模型对简单概念的过分解读</strong>：这可能是因为labeler在进行生成内容的比较时，倾向于给给长的输出内容更高的奖励。</p>
</li>
<li>
<p><strong>对有害的指示可能会输出有害的答复</strong>：例如InstructGPT/ChatGPT也会对用户提出的“AI毁灭人类计划书”给出行动方案（这个是因为InstructGPT/ChatGPT假设labeler编写的指示是合理且价值观正确的，并没有对用户给出的指示做更详细的判断，从而会导致模型会对任意输入都给出答复。虽然后面的奖励模型可能会给这类输出较低的奖励值，但模型在生成文本时，不仅要考虑模型的价值观，也要考虑生成内容和指示的匹配度，有时候生成一些价值观有问题的输出也是可能的。</p>
</li>
</ul>
<h2 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h2>
<p><a href="https://dongnian.icu/paper_reading/2.5.GPT_GPT-2_GPT-3/index.html">https://dongnian.icu/paper_reading/2.5.GPT_GPT-2_GPT-3/index.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/590311003">https://zhuanlan.zhihu.com/p/590311003</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM的Base和Instruct等多版本有什么区别</title>
    <url>/2025/01/26/LLM%E7%9A%84Base%E5%92%8CInstruct%E7%AD%89%E5%A4%9A%E7%89%88%E6%9C%AC%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>最近使用Qwen2-7b推理, 发现模型在输入完成后继续输出无关内容</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126102030547.png" alt="image.png" /></p>
<p>在github issue上有人问类似问题, 使用instruct模型后问题解决</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126102038416.png" alt="image.png" /></p>
<p>LLM通常会发布多个版本，如 <strong>Base 模型</strong> 和 <strong>Instruct 模型</strong>，它们在训练目标和应用场景上存在显著的区别。下面详细解释这两者的不同之处：</p>
<span id="more"></span>
<h4 id="1-base-模型"><a class="markdownIt-Anchor" href="#1-base-模型"></a> 1. <strong>Base 模型</strong>：</h4>
<ul>
<li>
<p><strong>定义</strong>：Base 模型是未经专门任务微调的基础模型，它通常是在大规模数据上进行自监督学习后得到的。Base 模型通过学习语言中的统计模式来理解语言结构，但并没有针对特定任务进行优化。</p>
</li>
<li>
<p><strong>训练目标</strong>：Base 模型的目标是预测下一个 token（即词或字符片段），它通过广泛的文本数据来学习语言的通用特征，但并不具备特定任务的指令执行能力。</p>
</li>
<li>
<p><strong>特点</strong>：</p>
<ul>
<li>
<p><strong>广泛但不具体</strong>：它理解语言的基础结构和广泛知识，但对用户给出的具体指令反应不够好。</p>
</li>
<li>
<p><strong>需要微调</strong>：为了在特定任务上表现出色，Base 模型需要进一步的任务微调。</p>
</li>
</ul>
</li>
<li>
<p><strong>应用场景</strong>：</p>
<ul>
<li>
<p>作为基础模型，可以用来微调针对特定任务的数据集。</p>
</li>
<li>
<p>用于研究探索，开发者可以根据不同任务的需求进行定制化训练。</p>
</li>
</ul>
</li>
</ul>
<h4 id="2-instruct-模型"><a class="markdownIt-Anchor" href="#2-instruct-模型"></a> 2. <strong>Instruct 模型</strong>：</h4>
<ul>
<li>
<p><strong>定义</strong>：Instruct 模型是在 Base 模型的基础上，通过**指令微调（Instruction Tuning）**得来的版本。这类模型专门被设计成能够按照用户的指令执行任务，例如生成、回答问题、翻译等。</p>
</li>
<li>
<p><strong>训练目标</strong>：Instruct 模型不仅学会了语言模式，还被训练去理解并按照用户输入的明确指令执行相应的任务。这通过<strong>监督学习</strong>来实现，模型接受大量人类指令及其对应的输出进行微调，使得它能更好地处理明确的任务请求。</p>
</li>
<li>
<p><strong>特点</strong>：</p>
<ul>
<li>
<p><strong>任务导向</strong>：模型不仅理解语言，还能理解任务需求，并生成相关的输出。</p>
</li>
<li>
<p><strong>指令响应能力强</strong>：Instruct 模型能够按照用户的请求完成诸如生成文本、回答问题等任务，表现出比 Base 模型更好的指令执行能力。</p>
</li>
<li>
<p><strong>用户友好</strong>：相比 Base 模型，它的设计更倾向于真实的应用场景，通常不需要进一步微调即可直接用于任务执行。</p>
</li>
</ul>
</li>
<li>
<p><strong>应用场景</strong>：</p>
<ul>
<li>
<p>聊天机器人和对话系统。</p>
</li>
<li>
<p>特定任务的自动化（如文案生成、问答系统等）。</p>
</li>
<li>
<p>需要精准任务执行的 AI 应用。</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-对比总结"><a class="markdownIt-Anchor" href="#3-对比总结"></a> 3. <strong>对比总结</strong>：</h4>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>特性</td>
<td>Base 模型</td>
<td>Instruct 模型</td>
</tr>
<tr>
<td><strong>训练目标</strong></td>
<td>语言建模（预测下一个词）</td>
<td>任务执行（按指令生成结果）</td>
</tr>
<tr>
<td><strong>优化方式</strong></td>
<td><a href="https://so.csdn.net/so/search?q=%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">自监督学习</a></td>
<td>人类指令数据监督微调</td>
</tr>
<tr>
<td><strong>应用场景</strong></td>
<td>作为基础模型，需微调至特定任务</td>
<td>按指令生成文本、回答问题等任务</td>
</tr>
<tr>
<td><strong>灵活性</strong></td>
<td>适合进一步任务微调</td>
<td>直接用于指令响应任务，用户友好</td>
</tr>
<tr>
<td><strong>指令响应</strong></td>
<td>无指令优化，需上下文理解</td>
<td>针对明确的用户指令做出响应</td>
</tr>
<tr>
<td><strong>可定制性</strong></td>
<td>灵活，可针对不同任务定制</td>
<td>直接适用于常见任务，不需要再微调</td>
</tr>
</tbody>
</table>
<h4 id="4-实际例子"><a class="markdownIt-Anchor" href="#4-实际例子"></a> 4. <strong>实际例子</strong>：</h4>
<ul>
<li>
<p><strong>Base 模型</strong>：GPT-3 的基础版本或 LLaMA 的基础模型就是典型的 Base 模型，它们被广泛用于不同的实验性任务，因为它们具备对语言广泛的理解。</p>
</li>
<li>
<p><strong>Instruct 模型</strong>：如 OpenAI 的 <code>text-davinci-003</code> 或 LLaMA-2-Chat。这些模型被专门微调过，能够理解和执行复杂指令，适用于问答、文案生成等任务。</p>
</li>
</ul>
<h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结：</h4>
<ul>
<li>
<p><strong>Base 模型</strong> 更像是一块未经雕刻的毛坯，拥有广泛的语言知识但不擅长直接执行指令。</p>
</li>
<li>
<p><strong>Instruct 模型</strong> 是在 Base 模型上微调后的版本，专门设计用于听从和执行用户指令，适合用于对话系统、生成任务等具体应用场景。</p>
</li>
</ul>
<p>Instruct 模型通常更加实用，而 Base 模型则更适合研究人员或开发者进行进一步微调和定制。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA系列</title>
    <url>/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165618253.png" alt="image.png" /></p>
<span id="more"></span>
<h1 id="前置知识"><a class="markdownIt-Anchor" href="#前置知识"></a> 前置知识</h1>
<ul>
<li>
<p><strong>Transformer模型的结构以及注意力机制的工作原理</strong>。</p>
</li>
<li>
<p><strong>Transformer模型是如何训练和推理的</strong>。</p>
</li>
<li>
<p><strong>线性代数</strong>：矩阵乘法，点积。</p>
</li>
<li>
<p><strong>复数</strong>：欧拉公式（非必需，了解更好）。</p>
</li>
</ul>
<h1 id="主题"><a class="markdownIt-Anchor" href="#主题"></a> 主题</h1>
<ul>
<li>
<p><strong>原始Transformer与LLaMA的架构差异</strong>。</p>
</li>
<li>
<p><strong>RMS Normalization</strong></p>
</li>
<li>
<p><strong>Rotary Positional Embeddings</strong></p>
</li>
<li>
<p><strong>KV-Cache</strong></p>
</li>
<li>
<p><strong>Multi-Query Attention</strong></p>
</li>
<li>
<p><strong>Grouped Multi-Query Attention</strong></p>
</li>
<li>
<p><strong>SwiGLU Activation Function</strong></p>
</li>
</ul>
<h1 id="transformer-vs-llama"><a class="markdownIt-Anchor" href="#transformer-vs-llama"></a> Transformer vs LLaMA</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165628374.png" alt="image.png" /></p>
<ul>
<li>
<p><strong>llama只有encoder</strong>, 因为它基于next prediction token task训练, 只需要自注意力机制来预测下一个 token</p>
</li>
<li>
<p>**所有的归一化被移到了block之前(**Attention, Feed Forward)</p>
</li>
<li>
<p>llama不再使用Transformer的Postional Encoding, 而是<strong>使用Rotary Positional Embeddings, 并且只应用在Query, Key上</strong></p>
</li>
<li>
<p><strong>Attention层增加了KV cache, Grouped Multi-Query Attention</strong></p>
</li>
<li>
<p><strong>Feed Forward block中的激活函数从ReLU变为SwiGLU</strong></p>
</li>
</ul>
<h1 id="llama1-llama2模型参数量"><a class="markdownIt-Anchor" href="#llama1-llama2模型参数量"></a> LLaMA1, LLaMA2模型参数量</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165639343.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165646305.png" alt="image.png" /></p>
<h1 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h1>
<h3 id="embedding"><a class="markdownIt-Anchor" href="#embedding"></a> Embedding</h3>
<p>句子通过分词器(Tokenizer)变成token(字典中的位置), token通过embedding层被映射到一个向量中(Transformer为512维, 而llama为4096维), 用来表示这个token的特征, 这些embedding是可学习的,是模型参数的一部分</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165655594.png" alt="image.png" /></p>
<h3 id="normalization"><a class="markdownIt-Anchor" href="#normalization"></a> Normalization</h3>
<h4 id="回顾神经网络的数学概念"><a class="markdownIt-Anchor" href="#回顾神经网络的数学概念"></a> <strong>回顾神经网络的数学概念</strong></h4>
<p>假设我们有一个线性层 nn.Linear(in_features=3, out_features=5, bias=True), 输入为(10, 3). 这意味着我们有10个item, 每个item有3个feature.</p>
<p>对于这个带有bias的线性层, 我们有两个权重矩阵, 一个是大小为(5, 3)的W, 一个是大小为(1, 5)的b</p>
<p>如下图所示, item1的第一个输出feature的计算方法为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>∗</mo><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>∗</mo><msub><mi>w</mi><mn>2</mn></msub><mo>+</mo><msub><mi>a</mi><mn>3</mn></msub><mo>∗</mo><msub><mi>w</mi><mn>3</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1 * w_1 + a_2 * w_2 + a_3 * w_3 + b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165721292.png" alt="" /></p>
<p>我们有以下结论:</p>
<ul>
<li>
<p>神经元对数据项的输出取决于输入数据项的特征（以及神经元的参数）。</p>
</li>
<li>
<p>我们可以将一个神经元的输入视为前一个线性层的输出。</p>
</li>
<li>
<p><strong>如果前一层在经过梯度下降更新权重后，其输出发生了剧烈变化，那么下一层的输入也会发生剧烈变化，因此在下一步梯度下降时，下一层的权重也会被迫进行大幅调整。</strong></p>
</li>
<li>
<p><strong>这种神经网络内部节点（神经元）的分布发生变化的现象称为Internal Covariate Shift(内部协变量偏移)。我们希望避免这种情况，因为它会使网络训练变得更慢，迫使神经元因为前一层输出的剧烈变化而大幅调整其权重。</strong></p>
</li>
</ul>
<h4 id="回顾transformer中的layer-normalization"><a class="markdownIt-Anchor" href="#回顾transformer中的layer-normalization"></a> <strong>回顾Transformer中的Layer Normalization</strong></h4>
<ul>
<li>
<p>对于每个item,我们独立统计两个统计量,均值和方差</p>
</li>
<li>
<p>每个item都会用其归一化后的值进行更新，使其变成均值为0、方差为1的正态分布。</p>
</li>
<li>
<p>参数 gamma 和 beta 是可学习的参数，它们允许模型根据损失函数的需求“放大”每个特征的尺度或对特征进行平移。</p>
</li>
<li>
<p>和传统的batch norm相比, layer norm按行(即item)进行计算统计, 而batch norm按列(即feature)计算统计, 好处是可以更好的处理语言模型中,每次input的sequence length不同的情况</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165751368.png" alt="image.png" /></p>
<h4 id="llama使用的normalization-root-mean-square-normalization"><a class="markdownIt-Anchor" href="#llama使用的normalization-root-mean-square-normalization"></a> <strong>LLaMA使用的normalization: Root Mean Square Normalization</strong></h4>
<ul>
<li>
<p>RMS的作者认为, <strong>导致Layer norm起作用的更多的是由于re-scaling(除以方差), 而不是re-centering(减去均值)</strong>. 因此他们想要找到一个不依赖均值的统计量.</p>
</li>
<li>
<p><strong>RMS提出使用均方根归一化, 即每个item除以均方根,在乘以一个可学习的参数gamma, 如下图所示</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164739603.png" alt="image.png" /></p>
<ul>
<li><strong>RMS只需要计算一个统计量, 因此计算量更少, 同时在实践中表现很好</strong></li>
</ul>
<h3 id="positional-encoding"><a class="markdownIt-Anchor" href="#positional-encoding"></a> Positional Encoding</h3>
<p><a href="https://www.bilibili.com/video/BV1xR1RY9ECm">位置编码有什么用？简单讲解位置编码原理 + 源码解读（绝对 / 相对 / RoPE）_哔哩哔哩_bilibili</a></p>
<h4 id="回顾transformer中的positional-encoding"><a class="markdownIt-Anchor" href="#回顾transformer中的positional-encoding"></a> <strong>回顾Transformer中的Positional Encoding</strong></h4>
<ul>
<li>
<p><strong>我们希望每个词都能携带一些关于其在句子中位置的信息。</strong></p>
</li>
<li>
<p>我们希望模型将出现在彼此相邻的词视为“接近”，而将距离较远的词视为“远离”。</p>
</li>
<li>
<p>通过下图的公式, 计算出句子中每个词, 每一维embedding所对应的Positional Encoding,</p>
</li>
<li>
<p><strong>我们只需要计算一次位置编码，然后在训练或推理过程中都可以重复使用它</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165800937.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165809840.png" alt="image.png" /></p>
<h4 id="absolute-positional-encodings-vs-relative-positional-encodings"><a class="markdownIt-Anchor" href="#absolute-positional-encodings-vs-relative-positional-encodings"></a> Absolute Positional Encodings v.s. Relative Positional Encodings</h4>
<ul>
<li>
<p><strong>绝对位置编码</strong>是固定的向量，它们被添加到一个token的embedding中，用来表示其在句子中的绝对位置。因此，它<strong>一次只处理一个token</strong>。你可以将其类比为地图上的（纬度，经度）对：地球上的每个点都有一个唯一的坐标对。绝对位置编码的优势在于它不依赖于序列中的其他元素，可以独立地表示每个位置的信息。</p>
</li>
<li>
<p><strong>相对位置编码</strong>则一次处理两个token，并在我们计算注意力时起作用。因为注意力机制捕捉的是两个词之间相关性的“强度”，相对位置编码则告诉注意力机制这两个词之间的距离。因此，**给定两个token，我们创建一个向量来表示它们之间的距离。**相对于绝对位置编码，相对位置编码更关注序列中位置之间的相对顺序和距离，它可以更好地处理长序列中的位置信息, 也具有更好的外推性。</p>
</li>
</ul>
<blockquote>
<p><strong>备注：什么是大模型外推性？（Length Extrapolation）</strong></p>
<p>**外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。**例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。</p>
</blockquote>
<ul>
<li>如下图所示, 相对位置编码没有完整建模整个序列的位置信息，而是在算当前位置的Attention的时候，考虑了当前位置和被Attention位置之间的相对距离（这一操作可以体现<strong>在计算Attention过程中，在计算中引入一个相对位置向量进行学习</strong>)</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165820235.png" alt="image.png" /></p>
<h4 id="rotary-position-embeddings"><a class="markdownIt-Anchor" href="#rotary-position-embeddings"></a> Rotary Position Embeddings</h4>
<p><strong>旋转位置编码的出发点: 把绝对位置编码和相对位置编码相结合. 具体来说, 我们希望给每个位置分配一个绝对位置编码, 然后不同位置的qk内积又直接包含位置差的信息(注意是“直接”，显式地包含).</strong></p>
<p><strong>也就是我们需要找到满足这个公式</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>f</mi><mi>q</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">⟩</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><mi>m</mi><mo>−</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m - n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span><strong>的函数f, 同时希望编码本身有一些其他的性质(如如远程衰减、易于外推等)</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165828163.png" alt="image.png" /></p>
<p><strong>回顾transformer用到的三角式位置编码, 虽然是绝对位置编码, 但是旋转矩阵的引入也让它包含了一定的相对位置信息</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165836993.png" alt="image.png" /></p>
<p><strong>先研究2D情况</strong></p>
<p>利用三角函数运算, 可以将m-n的旋转矩阵分解为m和n的矩阵乘积, 也就实现了相对位置到绝对位置的转变</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165851458.png" alt="image.png" /></p>
<p>如果左右乘上相应位置的q, k, 就得到了最终的内积形式, 这正是我们追求的目标形式<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>f</mi><mi>q</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">⟩</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><mi>m</mi><mo>−</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m - n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">⟨</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165901170.png" alt="image.png" /></p>
<p><strong>扩展到全维度</strong></p>
<p>总结起来就是给词向量左乘一个旋转矩阵</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165910905.png" alt="image.png" /></p>
<p><strong>优化</strong></p>
<p>由于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">R_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>比较稀疏, 直接用矩阵乘法实现效率较低, 论文中提供了一种高效的实现方法, 用矩阵元素相乘替代矩阵乘法, 最后奇偶维度错位相加.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165918431.png" alt="image.png" /></p>
<p><strong>总结</strong></p>
<ul>
<li>
<p>RoPE就像绝对位置编码一样，我们计算一次，然后可以对我们将要训练模型的所有句子重复使用。</p>
</li>
<li>
<p>使用RoPE编码的两个token之间的关系“强度”会随着它们之间距离的增加而在数值上变小, 称为long-term decay</p>
</li>
<li>
<p>RoPE只应用在Q, K上</p>
</li>
</ul>
<blockquote>
<p>在自注意力机制中，<strong>Query</strong> 和 <strong>Key</strong> 的内积决定了不同位置的词元之间的相关性（或称为“注意力权重”）。注意力机制的目标是根据这些权重，确定每个词元在多大程度上应该关注序列中的其他词元。因此，位置编码需要引入到 Query 和 Key 中，以影响注意力计算时如何考虑不同词元之间的相对位置。</p>
<p><strong>Value</strong> 并不直接参与注意力权重的计算，它只是在计算出注意力权重之后，根据这些权重聚合输入内容。因此，Value 中不需要加入位置编码。在自注意力机制中，Attention 输出是通过加权求和值得出的，而 Value 是被这些权重加权的部分。因为位置编码的目的是为了告诉模型不同位置之间的关系，而不是影响如何组合内容，所以没有必要将位置编码应用于 Value 上。</p>
</blockquote>
<h3 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> Self-Attention</h3>
<p><strong>自注意力机制允许模型将词与其他词关联起来。</strong></p>
<p>假设我们考虑一个序列长度为6, embedding维度为512的输入, 由于是自注意力, QKV都是相同的</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">softmax(\frac{QK^T}{\sqrt{d_k}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5864385em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622307142857143em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222307142857144em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17776928571428574em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>捕捉了两个token之间的相关性</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165928491.png" alt="image.png" /></p>
<p>上一步的结果再和V做矩阵乘, 得到与输入维度相同的输出矩阵, <strong>每行(即经过计算后的的新的embedding)不仅捕捉了词语的含义（由嵌入表示）或在句子中的位置（由位置编码表示），还捕捉了每个词与其他词的相互作用</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165937525.png" alt="image.png" /></p>
<p>给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165943584.png" alt="image.png" /></p>
<p><strong>Multi-Head Attention</strong></p>
<p>沿着d模型维度将QKV分割成较小的矩阵，并在这些较小的矩阵之间计算注意力. 所以每个头都在观察完整的句子，但是是每个embedding的不同方面。这样做的原因是<strong>我们希望每个头观察同一个词的不同方面</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165951462.png" alt="image.png" /></p>
<p>再给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170001367.png" alt="image.png" /></p>
<h3 id="kv-cache"><a class="markdownIt-Anchor" href="#kv-cache"></a> KV cache</h3>
<h4 id="next-token-prediction-task"><a class="markdownIt-Anchor" href="#next-token-prediction-task"></a> Next Token Prediction Task</h4>
<p>首先来了解一下LLama的训练（下词预测任务）：seq2seq的生成，但迭代T次，seq_len逐渐增加。</p>
<p>即输入序列的第一个token将映射到输出序列的第一个token, [SOS] -&gt; Love, 第二次迭代中将用[SOS] Love预测that, 以此类推</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170012232.png" alt="image.png" /></p>
<h4 id="motivation-behind-kv-cache"><a class="markdownIt-Anchor" href="#motivation-behind-kv-cache"></a> Motivation behind KV cache</h4>
<p>在推理的每一步，我们只关心模型输出的最后一个词元，因为我们已经有了之前的词元。然而，模型需要访问所有之前的词元来决定输出哪个词元，因为它们构成了模型的上下文（或称为“提示”）。<br />
有没有办法让模型在推理时对已经看到的词元进行更少的计算呢？</p>
<h4 id="kv-cache-2"><a class="markdownIt-Anchor" href="#kv-cache-2"></a> KV cache</h4>
<p>下句预测时的Self-Attention：</p>
<ul>
<li>timpstep=1时seq_len=1，给[SOS]时，预测Love；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170018823.png" alt="image.png" /></p>
<ul>
<li>timpstep=2时<code>seq_len=2</code>，给[SOS] 和 Love时，预测that</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170024662.png" alt="image.png" /></p>
<ul>
<li>timpstep=4时<code>seq_len=4</code>，给[SOS] 和 Love 和 can 和 quickly时，预测seize…</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170032874.png" alt="image.png" /></p>
<p>再来分析一下，<strong>每次个timestep的self-attention中我们到底需要哪些</strong>：因为我们只关注<strong>最后一个token的</strong><code>**attention_output**</code>，如下图timestep=4，我们只需要attention_output的第4个token。</p>
<p>因此我们只需要<strong>Q的最后一个token</strong>和<strong>K的所有token</strong>相乘，得到最后一个token的<code>attention_score</code>，然后用<strong>V的所有token</strong>再与<code>attention_score</code>点积(相乘求和)，得到最后一个token的<code>attention_output</code>：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170040017.png" alt="image.png" /></p>
<p>由上分析可知，<strong>每个timestep，我们的Q只需要新增的那个token即可，而K和V要缓存之前timestep的token，保证token是全的</strong>。<strong>每次计算出来的attention_output就是那个新增的token的attention。</strong> 这样就可以节省大量计算开销。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170046580.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170053043.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170058416.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170105884.png" alt="image.png" /></p>
<p><strong>参数量的计算方法:</strong> <a href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a></p>
<h3 id="grouped-multi-query-attention"><a class="markdownIt-Anchor" href="#grouped-multi-query-attention"></a> Grouped Multi-Query Attention</h3>
<p>回顾原始的<strong>多头注意力Multi-Head Attention</strong>：时间开销的瓶颈在于<strong>矩阵的运算</strong><code>**matrix computation**</code>。</p>
<ul>
<li>
<p>多头注意力机制如同在原始论文《Attention is all you need》中所展示的。</p>
</li>
<li>
<p>通过设定 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m = n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span>（查询的序列长度 = 键和值的序列长度）。</p>
</li>
<li>
<p>执行的算术运算数量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bnd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>所涉及的总内存量，由计算中涉及的所有张量（包括派生张量）之和给出，为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>b</mi><mi>h</mi><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bnd + bhn^2 + d^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>总内存与算术运算数量之间的比率为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>b</mi><mi>n</mi></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">O\left( \frac{1}{k} + \frac{1}{bn} \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span>。</p>
</li>
<li>
<p>在这种情况下，该比率远小于 1，这意味着我们进行的内存访问量远少于算术运算的数量，因此内存访问<strong>不是</strong>这里的瓶颈。</p>
</li>
</ul>
<p>当我们<strong>使用KV-Cache</strong>后：时间开销的瓶颈在于<strong>内存的访问</strong><code>**memory access**</code>。</p>
<ul>
<li>
<p>使用 KV 缓存来减少执行的操作次数。</p>
</li>
<li>
<p>通过设定 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m = n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span>（查询的序列长度 = 键和值的序列长度）。</p>
</li>
<li>
<p>执行的算术运算次数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bnd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><msup><mi>n</mi><mn>2</mn></msup><mi>d</mi><mo>+</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bn^2d + nd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>总内存量与算术运算次数之间的比率为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi mathvariant="normal">/</mi><mi>d</mi><mo>+</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n/d + 1/b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>当 n ≈ d（序列长度接近嵌入向量的维度大小）或 b ≈ 1（批次大小为 1）时，比率变为 1，此时内存访问成为算法的瓶颈。对于批次大小通常没有问题，因为它通常远大于 1，而对于 n/d 项，我们需要减少序列长度。但还有更好的方法……</p>
</li>
</ul>
<h5 id="multi-query-attentionmqa"><a class="markdownIt-Anchor" href="#multi-query-attentionmqa"></a> Multi Query Attention（MQA）</h5>
<p>为了提升attention计算效率，<strong>多查询注意力（</strong><code>**Multi Query Attention，MQA**</code><strong>）</strong> 是多头注意力的一种变体。其主要区别在于，在<strong>多查询注意力中</strong><code>**多个不同的注意力head**</code><strong>共享</strong><code>**一个**</code><strong>k和v的集合，每个head只单独保留了一份q参数。</strong> 具体操作上，<code>**去除 K和V 的head维度，只为Q保留head维度**</code>。因此这就是被叫做Multi Query Attention的原因。</p>
<ul>
<li>
<p>我们从 K 和 V 中移除了 h 维度，同时保留了 Q 的 h 维度。这意味着所有不同的查询头将共享相同的键和值。</p>
</li>
<li>
<p>执行的算术运算次数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bnd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>b</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>b</mi><msup><mi>n</mi><mn>2</mn></msup><mi>k</mi><mo>+</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(bnd + bn^2k + nd^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">b</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>总内存量与算术运算次数之间的比率为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>d</mi><mo>+</mo><mi>n</mi><mi mathvariant="normal">/</mi><mi>d</mi><mi>h</mi><mo>+</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1/d + n/dh + 1/b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal">d</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span>。</p>
</li>
<li>
<p>与之前的方法相比，我们通过一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span> 因子减少了昂贵的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">n/d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal">d</span></span></span></span> 项。</p>
</li>
<li>
<p>性能提升显著，而模型的质量只出现了少许下降。</p>
</li>
</ul>
<p>因此<code>K和V的矩阵的数量仅为1个</code>（不分head），大幅度减少了显存占用，使其更高效。<strong>由于多查询注意力改变了注意力机制的结构，因此模型通常需要从训练开始就支持多查询注意力。</strong></p>
<p>研究结果表明，可以通过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可以达到不错的效果。包括Falcon、SantaCoder、StarCoder等在内很多模型都采用了多查询注意力机制。</p>
<h5 id="grouped-multi-query-attentiongmqa"><a class="markdownIt-Anchor" href="#grouped-multi-query-attentiongmqa"></a> Grouped Multi-Query Attention(GMQA)</h5>
<p>就是在 Multi-Query Attention的基础上，对input进行分组，<strong>如下图2个head分为1组，每组都有自己的K，V，每个组包含2个Q。</strong> （与MQA的区别在于：<strong>MQA的KV只有1份；GQA的KV有</strong><code>**group**</code><strong>份(LLama-70B中是</strong><code>**kv_heads**</code><strong>=8，即每个kv对应8个q)</strong>）</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170116630.png" alt="image.png" /></p>
<p><strong>参数量的计算方法:</strong> <a href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a></p>
<h3 id="swiglu-function"><a class="markdownIt-Anchor" href="#swiglu-function"></a> SwiGLU Function</h3>
<p>SwiGLU 激活函数是Shazeer 在文献中提出，并在PaLM等模中进行了广泛应用，并且取得了不错的效果，<strong>相较于ReLU 函数在大部分评测中都有不少提升</strong>。在LLaMA 中全连接层使用带有SwiGLU 激活函数的FFN（Position-wise Feed-Forward Network）的计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164815049.png" alt="image.png" /></p>
<p>其中，<strong>σ(x) 是Sigmoid 函数</strong>。下图给出了Swish 激活函数在参数β 不同取值下的形状。可以看到当β 趋近于0 时，Swish 函数趋近于线性函数y = x，当β 趋近于无穷大时，Swish 函数趋近于ReLU 函数，β 取值为1 时，Swish 函数是光滑且非单调。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170126831.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>MAE (Masked AutoEncoders)</title>
    <url>/2025/01/10/MAE-Masked-AutoEncoders/</url>
    <content><![CDATA[<h2 id="mae-是什么"><a class="markdownIt-Anchor" href="#mae-是什么"></a> MAE 是什么</h2>
<p>MAE (Masked AutoEncoders) 是可拓展自监督视觉学习器，思想是随机掩盖一些图像块，然后重建丢失的像素。MAE 使用一个非对称的解码编码器对可见的图形块进行处理，同时使用一个轻量级的解码器从潜在表示和掩码块重建原始图像；如果遮挡输入图像的大部分（例如75%），就是变成了一个自监督任务。这两种方法可以有效地训练大规模模型：可以加速训练和提高精度。MAE可用于ViT下游识别任务的fine-tune，还可以用于目标检测、实例分割以及语义分割等任务的迁移学习。</p>
<p><strong>和VIT的区别：</strong></p>
<ol>
<li>
<p>需要盖住更多的块，使得剩下的块块与块之间的冗余度没那么高</p>
</li>
<li>
<p>使用Transformer架构的解码器，直接还原原始的像素信息</p>
</li>
</ol>
<span id="more"></span>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182334475.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182342364.png" alt="image.png" /></p>
<h2 id="模型架构"><a class="markdownIt-Anchor" href="#模型架构"></a> 模型架构</h2>
<p><strong>编码器</strong></p>
<p>编码器是 ViT。 它接受张量形状为 (batch_size, RGB_channels, height, width) 的图像。 通过执行线性投影为每个Patch获得嵌入， 这是通过 2D 卷积层来完成。 然后张量在最后一个维度被展平（压扁），变成 (batch_size, encoder_embed_dim, num_visible_patches)，并 转置为形状（batch_size、num_visible_patches、encoder_embed_dim）的张量。正如原始 Transformer 论文中提到的，位置编码添加了有关每个Patch位置的信息。 作者使用“sine-cosine”版本而不是可学习的位置嵌入。 与 Transformer 类似，每个块由norm层、多头注意力模块和前馈层组成。 中间输出形状是（batch_size、num_visible_patches、encoder_embed_dim）。这部分仅用于下游任务的微调。 论文的模型遵循 ViT 架构，该架构具有用于分类的类令牌（patch）。 因此，他们添加了一个虚拟令牌，但是论文中也说到他们的方法在没有它的情况下也可以运行良好，因为对其他令牌执行了平均池化操作。 在这里也包含了实现的平均池化版本。 之后，添加一个线性层作为分类器。 最终的张量形状是 (batch_size, num_classes)。</p>
<p><strong>解码器</strong></p>
<p>与编码器类似，解码器由一系列transformer 块组成。 在解码器的末端，有一个由norm层和前馈层组成的分类器。 输入张量的形状为 batch_size, num_patches,decoder_embed_dim) 而最终输出张量的形状为 (batch_size, num_patches, 3 * patch_size ** 2)。</p>
<p><strong>把所有东西放在一起——MAE架构</strong></p>
<p>MAE 用于对掩码图像进行预训练。首先，屏蔽的输入被发送到编码器。然后，它们被传递到前馈层以更改嵌入维度以匹配解码器。 在传递给解码器之前，被掩码的Patch被输入进去。 位置编码再次应用于完整的图像块集，包括可见的和被掩码遮盖的。</p>
<p>在论文中，作者对包含所有Patch的列表进行了打乱，以便正确插入Patch的掩码。 这部分在本篇文章中没有完成，因为在 PyTorch 上实现并不简单。所以这里使用的是位置编码在被添加到Patch之前被相应地打乱的做法。</p>
<h2 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h2>
<p>让我们看看原始论文中报道的预训练阶段的重建图像。看起来MAE在重建图像方面做得很好，即使80%的像素被遮蔽了。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182351798.png" alt="image.png" /></p>
<h2 id="讨论"><a class="markdownIt-Anchor" href="#讨论"></a> 讨论</h2>
<ul>
<li>MAE的Decoder和Bert，Vit有什么区别</li>
</ul>
<p>用于重建的decoder在图像和文本任务发挥的角色有区别，从句子中预测单词属于高语义任务，encoder和decoder的gap小，所以BERT的decoder部分微不足道（只需要一个MLP），而对图像重建像素属于低语义任务（相比图像分类），decoder需要发挥更大作用：将高语义的中间表征恢复成低语义的像素值。</p>
<h2 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h2>
<p><a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>MOCO</title>
    <url>/2025/01/15/MOCO/</url>
    <content><![CDATA[<p>论文地址 : <a href="https://arxiv.org/abs/1911.05722">https://arxiv.org/abs/1911.05722</a></p>
<p>论文代码(官方) : <a href="https://github.com/facebookresearch/moco">https://github.com/facebookresearch/moco</a></p>
<p>视频解读 : <a href="https://www.bilibili.com/video/av422340209">MoCo 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<span id="more"></span>
<h1 id="对比学习"><a class="markdownIt-Anchor" href="#对比学习"></a> 对比学习</h1>
<p>参考 CLIP 对比学习章节</p>
<h1 id="moco实现思路"><a class="markdownIt-Anchor" href="#moco实现思路"></a> MoCo实现思路</h1>
<p>传统的对比学习可以归纳成一种任务：给定一个query和若干keys，找出这个query和以下那个key相似。</p>
<p>如图所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250115201923281.png" alt="image.png" /></p>
<p>其中keys集合的大小就是Batch Size的大小。（Keys集合也称为“字典”）</p>
<p>如果想要训练出好的模型，那我们就需要如下两点：</p>
<ul>
<li>
<p>字典越大越好</p>
</li>
<li>
<p>字典中的key的特征要保持一致性。换句话说：这些正负样本要保证是从一个Encoder中提取的特征。即使不是从一个Encoder提取的，也要是从相似的Encoder中提取的。</p>
</li>
</ul>
<p>而之前的对比学习方法对于这两点没有办法做到兼容。例如对于上述提到的SimCLR的字典大小就是Batch Size大小，所以受限于显卡内存无法做到太大。 而一些其他论文虽然构建了一个大词典，但是词典中的特征向量都是来自不同阶段的Encoder，所以一致性不高。（不同阶段是指在训练过程中，Encoder参数是不断变化的）</p>
<blockquote>
<p>与之前对比学习论文不同，在MoCo中，锚点称为query，而正负样本称为Key。</p>
</blockquote>
<h1 id="moco架构"><a class="markdownIt-Anchor" href="#moco架构"></a> MoCo架构</h1>
<p>MoCo主要就是解决了两个问题：</p>
<ol>
<li>
<p>字典可以搞的很大</p>
</li>
<li>
<p>字典中的key的一致性很高。</p>
</li>
</ol>
<p>MoCo的架构如下:</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250115201931470.png" alt="image.png" /></p>
<p>MoCo架构与之前对比学习类似，query和keys（就是锚点和正负样本）分别经过两个不同的Encoder，然后再计算相似度，最终预测字典中哪个key和query是一对儿。</p>
<p>MoCo与之前对比学习方法的区别主要在keys这边，主要有两点：</p>
<ul>
<li>
<p><strong>Queue</strong>: MoCo维护了一个先进先出队列(FIFO queue)，这个队列就是字典，里面存储的是图片的特征向量，所以可以很大。而队列的维护方式就是每次入队最新一批图片的特征向量，出队最早一批图片的特征向量。</p>
</li>
<li>
<p><strong>Momentum Encoder</strong>：为了保证queue中key的一致性，字典这边的encoder必须要缓慢的更新。所以作者采用了动量的方式。简单来说，就是Momentum Encoder = 0.99 * Momentum Encoder + 0.01 * Encoder。每次Encoder更新后，只拿它参数的0.01来更新Momentum Encoder，这样就可以保证Momentum Encoder更新缓慢了。</p>
</li>
</ul>
<p>举个例子来描述整个过程：假设字典(队列)大小为65536, batch size为128, Encoder编码的特征向量大小为256.</p>
<ol>
<li>
<p>初始化Encoder：首先初始化两个Encoder，一个作为Encoder，一个作为Momentum Encoder。然后让两个Encoder的参数保持一致。</p>
</li>
<li>
<p>初始化队列：拿65536张图片通过Momentum Encoder进行特征提取，然后放入队列。其中每个图片的特征为256.</p>
</li>
<li>
<p>接下来开始训练：</p>
<ol>
<li>
<p>从数据集中随机采样128张图片</p>
</li>
<li>
<p>将这128张图片进行随机数据增强后送给Encoder，计算出其特征向量。</p>
</li>
<li>
<p>将这128张图片再次随机数据增强后送给Momentum Encoder，计算其特征向量。此时我们就得到了两组特征向量，每组都有128个256维的特征向量。我们这里将其命名为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">x_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">x_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中元素“对应相乘”，得到128个数值。也就是得到了128个锚点与其对应的正样本的内积（相当于将向量内积作为相似度）</p>
</li>
<li>
<p>将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">x_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和队列中的向量“两两相乘”，得到128x65536个数值。也就是128个锚点和队列中所有负样本的内积。</p>
</li>
<li>
<p>将锚点和与正样本的内积和与负样本的内积concat到一起，得到128x(1+65536)个内积。</p>
</li>
<li>
<p>使用CrossEntropyLoss求出锚点与正样本对应的loss。即锚点与正样本越接近loss越小，锚点与负样本远离loss越大。</p>
</li>
<li>
<p>反向传播更新encoder参数</p>
</li>
<li>
<p>使用momentum机制更新Momentum Encoder，即Momentum Encoder = 0.99 * Momentum Encoder + 0.01 * Encoder</p>
</li>
<li>
<p>将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>入队，响应的最早的一批128个特征向量出队。</p>
</li>
</ol>
</li>
<li>
<p>重复3的训练过程</p>
</li>
</ol>
<h1 id="moco代码实现"><a class="markdownIt-Anchor" href="#moco代码实现"></a> MOCO代码实现</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先假设存在这么几个对象</span></span><br><span class="line"><span class="comment"># f_q, f_k: query和key的Encoder，也就是Encoder和Momentum Encoder</span></span><br><span class="line"><span class="comment"># queue: 字典队列。为一个Shape为CxK的Tensor。其中C为特征向量的维度，K为词典的大小。例如：256x65536</span></span><br><span class="line"><span class="comment"># m: momentum超参。例如 0.99</span></span><br><span class="line"><span class="comment"># t: temperature。温度超参。例如：0.05</span></span><br><span class="line"></span><br><span class="line">f_k.params = f_q.params <span class="comment"># 初始f_k的参数和f_q保持一致</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># 从dataloader中拿出 N 个数据。x为一个batch的图片，例如x的shape为(128, 3, 224, 224)</span></span><br><span class="line">	x_q = aug(x) <span class="comment"># 对 x 进行随机的数据增强</span></span><br><span class="line">	x_k = aug(x) <span class="comment"># 对 x 进行另一种随机的数据增强</span></span><br><span class="line">	</span><br><span class="line">	q = f_q.forward(x_q) <span class="comment"># 使用Encoder提取x_q的特征。q的shape为 NxC。 例如128x256, 即128张图片，每张图片的特征向量为256维</span></span><br><span class="line">	k = f_k.forward(x_k) <span class="comment"># 使用Momentum Encoder提取x_k的特征，q的shape也为 NxC</span></span><br><span class="line">	k = k.detach() <span class="comment"># Momentum Encoder不计算梯度</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 求出锚点与正样本的内积。l_pos的shape为 Nx1。即每个锚点和对应正样本的向量内积（两两相乘）。例如 shape为128x1</span></span><br><span class="line">	l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 让锚点和队列中所有的负样本计算内积。l_neg的shape为 NxK。例如 shape为128x65536</span></span><br><span class="line">	l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将正样本和负样本concat到一起，正样本放在最前面。logits的shape为 Nx(K+1)。例如 128x(65536+1)</span></span><br><span class="line">	logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	labels = zeros(N) <span class="comment"># 构造Label，因为正样本都是在0这个位置，所以构造N个0就行了。</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 使用CrossEntropyLoss计算损失，这里需要除以温度</span></span><br><span class="line">	loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 反向传播并更新f_q的参数。</span></span><br><span class="line">	loss.backward()</span><br><span class="line">	update(f_q.params)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用momentum的方式更新f_k</span></span><br><span class="line">	f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 更新字典</span></span><br><span class="line">	enqueue(queue, k) <span class="comment"># 将当前的 N 个样本的特征向量入队。</span></span><br><span class="line">	dequeue(queue) <span class="comment"># 同时再将最早的 N 个样本出队</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>对比学习</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title>SwinTranformer</title>
    <url>/2025/01/10/SwinTranformer/</url>
    <content><![CDATA[<p>《<a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>》作为2021 ICCV最佳论文，屠榜了各大CV任务，性能优于DeiT、ViT和EfficientNet等主干网络，已经替代经典的CNN架构，成为了<strong>计算机视觉领域通用的通用骨干网络</strong>。它基于了ViT模型的思想，创新性的引入了<strong>滑动窗口机制</strong>，让模型能够学习到跨窗口的信息，同时也。同时通过<strong>下采样层</strong>，使得模型能够处理超分辨率的图片，节省计算量以及能够关注全局和局部的信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182640059.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2>
<h5 id="从cnn到transformer"><a class="markdownIt-Anchor" href="#从cnn到transformer"></a> 从CNN到Transformer</h5>
<ul>
<li>
<p><strong>CNN的局限性：</strong> 虽然CNN在图像识别任务中取得了巨大成功，但它们通常局限于局部感受野，这限制了它们捕获长距离依赖的能力。此外，CNN在处理高分辨率图像时面临着效率和性能的挑战。</p>
</li>
<li>
<p><strong>Transformer的优势：</strong> Transformer结构，最初用于NLP任务，以其自注意力机制著称，可以有效处理长距离的依赖关系。这一特性使得Transformer在理解复杂的、全局性的数据结构方面表现出色。</p>
</li>
</ul>
<h5 id="从transformer到swin-transformer"><a class="markdownIt-Anchor" href="#从transformer到swin-transformer"></a> 从Transformer到Swin Transformer</h5>
<ul>
<li>
<p><strong>尺度变化和分辨率问题：</strong> 在计算机视觉领域，尤其是处理高分辨率图像时，面临的主要挑战之一是视觉实体尺度的大变化和像素的高分辨率。这些特点与语言处理中的情况截然不同，因为在文本中，词汇的“分辨率”（即明确性和区分度）相对较低。</p>
</li>
<li>
<p><strong>传统Transformer结构的局限：</strong> 由于这些差异，传统的Transformer结构（如在自然语言处理中使用的那样）直接应用于视觉任务时会遇到效率和性能的挑战。尤其是在处理需要细致像素级预测的高分辨率图像时，传统Transformer的全局自注意力机制导致计算复杂度过高，不适合直接应用于视觉任务。</p>
</li>
</ul>
<p>Swin Transformer 提出了一种基于<strong>滑动窗口机制，具有层级设计（下采样层）</strong> 的模型结构。其中<strong>滑窗操作</strong>包括<strong>不重叠的 local window，和重叠的 cross-window</strong>。将注意力计算限制在一个窗口（window size固定）中，<strong>一方面能引入 CNN 卷积操作的局部性，另一方面能大幅度节省计算量</strong>，它只和窗口数量成线性关系。通过<strong>下采样</strong>的层级设计，能够逐渐增大感受野，从而使得注意力机制也能够注意到<strong>全局</strong>的特征。</p>
<h2 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182648835.png" alt="image.png" /></p>
<p>整个模型采取层次化的设计，一共包含 4 个 Stage，除第一个 stage 外，每个 stage 都会先通过 <strong>Patch Merging</strong> 层缩小输入特征图的分辨率，进行<strong>下采样操作</strong>，像 CNN 一样逐层扩大感受野，以便获取到全局的信息。</p>
<ul>
<li>
<p>在输入开始的时候，做了一个<code>Patch Partition</code>，即ViT中<code>Patch Embedding</code>操作，通过 <strong>Patch_size</strong> 为4的卷积层将图片切成一个个 <strong>Patch</strong> ，并嵌入到<code>Embedding</code>，将 <strong>embedding_size</strong> 转变为48（可以将 CV 中图片的<strong>通道数</strong>理解为NLP中token的<strong>词嵌入长度</strong>），以swin-s为例，具体是将图像先分割成4 × 4 的小块，然后将每一个小块通过映射成一个像素点，进行了通道上的扩充，输入的224 × 224 图像经过这一步操作就变成了56 × 56的特征图。</p>
</li>
<li>
<p>随后在第一个Stage中，通过<code>Linear Embedding</code>调整通道数为C。然后Transformer的输入和输出维度是相同的，所以进入下一个stage的大小是不变的。</p>
</li>
<li>
<p>在每个 Stage 里（除第一个 Stage ），均由<code>Patch Merging</code>和多个<code>Swin Transformer Block</code>组成。</p>
</li>
<li>
<p>其中<code>Patch Merging</code>模块主要在每个 Stage 一开始降低图片分辨率，进行下采样的操作。H和W各变为二分之一，总共是缩小4分之一，然后通道数增加了4倍（用空间换深度），经过一个全连接层再调整通道维度为原来的2倍，也就是说每经过一个stage，总的数据量变为原来的1/2。可以参考下面的示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182658508.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182704756.png" alt="image.png" /></p>
<ul>
<li>而<code>Swin Transformer Block</code>具体结构如右图所示，主要是<code>LayerNorm</code>，<code>Window Attention</code> ，<code>Shifted Window Attention</code>和<code>MLP</code>组成 。</li>
</ul>
<h3 id="window-attention-和-shift-window-attention"><a class="markdownIt-Anchor" href="#window-attention-和-shift-window-attention"></a> <strong>Window Attention 和 Shift Window Attention</strong></h3>
<p>这是这篇文章的关键。传统的Transformer都是基于全局来计算注意力的，因此计算复杂度十分高。而Swin Transformer则将注意力的计算限制在每个窗口内，进而减少了计算量。</p>
<p>window attention就是按照一定的尺寸将图像划分为不同的window，每次transformer的attention只在window内部进行计算。</p>
<p>那么如果只有window attention就会带来每一个像素点的感受野得不到提升的问题，为了更好的和其他 window 进行信息交互，所以它又设计了一个shift window attention（下移两个patchs）的方法，就是换一下window划分的方式，让每一个像素点做attention计算的window块处于变化之中。那么就起到了提升感受野的作用。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182712651.png" alt="image.png" /></p>
<h2 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h2>
<ul>
<li>
<p><a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/367111046">https://zhuanlan.zhihu.com/p/367111046</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2024/12/23/Transformer/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。</p>
<p>在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。</p>
<p>Attention is All You Need：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<span id="more"></span>
<h2 id="1-transformer-整体结构"><a class="markdownIt-Anchor" href="#1-transformer-整体结构"></a> 1. Transformer 整体结构</h2>
<p>首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204611805.png" alt="image.png" /></p>
<p>Transformer 的整体结构，左图Encoder和右图Decoder</p>
<p>可以看到 <strong>Transformer 由 Encoder 和 Decoder 两个部分组成</strong>，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p>
<p>**第一步：**获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204624351.png" alt="image.png" /></p>
<p>Transformer 的输入表示</p>
<p>**第二步：**将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。单词向量矩阵用 Xn×d 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204644319.png" alt="image.png" /></p>
<p>Transformer Encoder 编码句子信息<br />
<strong>第三步</strong>：将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204703053.png" alt="image.png" /></p>
<p>Transofrmer Decoder 预测<br />
上图 Decoder 接收了 Encoder 的编码矩阵 <strong>C</strong>，然后首先输入一个翻译开始符 “<Begin>”，预测第一个单词 “I”；然后输入翻译开始符 “<Begin>” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p>
<h2 id="2-transformer-的输入"><a class="markdownIt-Anchor" href="#2-transformer-的输入"></a> 2. Transformer 的输入</h2>
<p>Transformer 中单词的输入表示 <strong>x</strong>由<strong>单词 Embedding</strong> 和<strong>位置 Embedding</strong> （Positional Encoding）相加得到。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204753829.png" alt="image.png" /></p>
<p>Transformer 的输入表示</p>
<h3 id="21-单词-embedding"><a class="markdownIt-Anchor" href="#21-单词-embedding"></a> 2.1 单词 Embedding</h3>
<p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p>
<h3 id="22-位置-embedding"><a class="markdownIt-Anchor" href="#22-位置-embedding"></a> 2.2 位置 Embedding</h3>
<p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。**因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。**所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p>
<p><strong>方法一：使用[0,1]范围分配</strong></p>
<p>这个方法的分配方式是，将0-1这个范围的，将第一个token分配0，最后一个token分配去1，其余的token按照文章的长度平均分配。具体形式如下：</p>
<blockquote>
<p>我喜欢吃洋葱 【0 0.16 0.32…1】</p>
<p>我真的不喜欢吃洋葱【0 0.125 0.25…1】</p>
</blockquote>
<p>问题：我们可以看到，如果句子长度不同，那么位置编码是不一样，所以无法表示句子之间有什么相似性。</p>
<p><strong>方法二：1-n正整数范围分配</strong></p>
<p>这个方法比较直观，就是按照输入的顺序，一次分配给token所在的索引位置。具体形式如下：</p>
<blockquote>
<p>我喜欢吃洋葱 【1，2，3，4，5，6】</p>
<p>我真的不喜欢吃洋葱【1，2，3，4，5，6，7】</p>
</blockquote>
<p>问题：往往句子越长，后面的值越大，数字越大说明这个位置占的权重也越大，这样的方式无法凸显每个位置的真实的权重。</p>
<p><strong>方法三：三角函数表示</strong></p>
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204808623.png" alt="image.png" /></p>
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p>
<ul>
<li>
<p>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</p>
</li>
<li>
<p>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</p>
</li>
</ul>
<p>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <strong>x</strong>，<strong>x</strong> 就是 Transformer 的输入。</p>
<h2 id="3-self-attention自注意力机制"><a class="markdownIt-Anchor" href="#3-self-attention自注意力机制"></a> 3. Self-Attention（自注意力机制）</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204817764.png" alt="image.png" /></p>
<p>Transformer Encoder 和 Decoder</p>
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 <strong>Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p>
<h3 id="31-self-attention-结构"><a class="markdownIt-Anchor" href="#31-self-attention-结构"></a> 3.1 Self-Attention 结构</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204827884.png" alt="image.png" /></p>
<p>Self-Attention 结构</p>
<p>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵<strong>Q(查询),K(键值),V(值)</strong>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q,K,V</strong>正是通过 Self-Attention 的输入进行线性变换得到的。</p>
<h3 id="32-q-k-v-的计算"><a class="markdownIt-Anchor" href="#32-q-k-v-的计算"></a> 3.2 Q, K, V 的计算</h3>
<p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵<strong>WQ,WK,WV</strong>计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204837811.png" alt="image.png" /></p>
<p>Q, K, V 的计算</p>
<h3 id="33-self-attention-的输出"><a class="markdownIt-Anchor" href="#33-self-attention-的输出"></a> 3.3 Self-Attention 的输出</h3>
<p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204845263.png" alt="image.png" /></p>
<p>Self-Attention 的输出</p>
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 dk 的平方根。<strong>Q</strong>乘以<strong>K</strong>的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<strong>Q</strong>乘以 KT ，1234 表示的是句子中的单词。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204851643.png" alt="image.png" /></p>
<p>Q乘以K的转置的计算</p>
<p>得到QKT 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204858967.png" alt="image.png" /></p>
<p>对矩阵的每一行进行 Softmax</p>
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204907248.png" alt="image.png" /></p>
<p>Self-Attention 输出</p>
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 Z1 等于所有单词 i 的值 Vi 根据 attention 系数的比例加在一起得到，如下图所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204917105.png" alt="image.png" /></p>
<p>Zi 的计算方法</p>
<h3 id="34-multi-head-attention"><a class="markdownIt-Anchor" href="#34-multi-head-attention"></a> 3.4 Multi-Head Attention</h3>
<p>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204924298.png" alt="image.png" /></p>
<p>Multi-Head Attention</p>
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204931261.png" alt="image.png" /></p>
<p>多个 Self-Attention</p>
<p>得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204940131.png" alt="image.png" /></p>
<p>Multi-Head Attention 的输出</p>
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的。</p>
<p>具体来说，单头自注意力机制将所有位置的特征向量都看作等价的，而多头自注意力机制能够在不同的“视角”下对输入进行建模。通过将输入的特征向量划分成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span>个多头，模型能够在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span>个不同的子空间上计算注意力，从而能够学习到多个不同的、互补的特征表示，从而更加<strong>全面地捕捉输入序列的语义信息</strong>。例如，对于一句话来说，不同的多头可以学习到句子的不同方面，如主语、宾语、谓语、修饰语等，从而能够更好地表示句子的语义信息。</p>
<p>此外，多头自注意力机制还可以<strong>并行计算</strong>，因为每个头的注意力计算是独立的，可以并行地进行。这在计算效率上有一定的优势，可以加速模型的训练和推理过程。</p>
<p>需要注意的是，在多头自注意力机制中，头的数量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span>需要根据任务的复杂度和数据集的规模进行调整，过多或过少的头都可能会影响模型的性能。通常情况下，头的数量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span>在4-16之间较为常见。</p>
<h2 id="4-encoder-结构"><a class="markdownIt-Anchor" href="#4-encoder-结构"></a> 4. Encoder 结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205007335.png" alt="image.png" /></p>
<p>Transformer Encoder block</p>
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, <strong>Add &amp; Norm, Feed Forward, Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p>
<h3 id="41-add-norm"><a class="markdownIt-Anchor" href="#41-add-norm"></a> 4.1 Add &amp; Norm</h3>
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205016584.png" alt="image.png" /></p>
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X</strong> 维度是一样的，所以可以相加)。</p>
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，将子层的输出和输入进行残差连接（residual connection），并进行元素级别的加法，得到增强了的特征表示。这个残差连接可以有效地防止梯度消失问题，避免训练过程中信息的损失。在 ResNet 中经常用到：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205023109.png" alt="image.png" /></p>
<p>残差连接</p>
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，进行归一化操作，以缩放增强后的特征表示，并减少内部协变量位移（Internal Covariate Shift），从而加快模型的收敛速度和提高性能。</p>
<h3 id="42-feed-forward"><a class="markdownIt-Anchor" href="#42-feed-forward"></a> 4.2 Feed Forward</h3>
<p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205033278.png" alt="image.png" /></p>
<p>Feed Forward</p>
<p><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与<strong>X</strong>一致。</p>
<p>Feed Forward层由两个全连接层组成，中间用一个非线性激活函数进行连接，如ReLU（Rectified Linear Unit）激活函数。在Transformer模型中，第一个全连接层将特征表示映射到一个更高维度的空间，可以增强模型的非线性建模能力，提高模型的性能和泛化能力；第二个全连接层将其再次映射回原始的维度，从而降低模型的计算复杂度和内存占用。</p>
<h3 id="43-组成-encoder"><a class="markdownIt-Anchor" href="#43-组成-encoder"></a> 4.3 组成 Encoder</h3>
<p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵 X(n×d) ，并输出一个矩阵 O(n×d) 。通过多个 Encoder block 叠加就可以组成 Encoder。</p>
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205045993.png" alt="image.png" /></p>
<p>Encoder 编码句子信息</p>
<h2 id="5-decoder-结构"><a class="markdownIt-Anchor" href="#5-decoder-结构"></a> 5. Decoder 结构</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205051556.png" alt="image.png" /></p>
<p>Transformer Decoder block</p>
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p>
<ul>
<li>
<p>包含两个 Multi-Head Attention 层。</p>
</li>
<li>
<p>第一个 Multi-Head Attention 层采用了 Masked 操作。</p>
</li>
<li>
<p>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</p>
</li>
<li>
<p>最后有一个 Softmax 层计算下一个翻译单词的概率。</p>
</li>
</ul>
<h3 id="51-第一个-multi-head-attention"><a class="markdownIt-Anchor" href="#51-第一个-multi-head-attention"></a> 5.1 第一个 Multi-Head Attention</h3>
<p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。</p>
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “<Begin>” 预测出第一个单词为 “I”，然后根据输入 “<Begin> I” 预测下一个单词 “have”。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205108154.png" alt="image.png" /></p>
<p>Decoder 预测</p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “<Begin> I have a cat <end>”。</strong></p>
<p>**第一步：**是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 “<Begin> I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205119517.png" alt="image.png" /></p>
<p>输入矩阵与 Mask 矩阵</p>
<p><strong>第二步：<strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵</strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和 KT 的乘积 QKT 。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205125576.png" alt="image.png" /></p>
<p>Q乘以K的转置</p>
<p><strong>第三步：<strong>在得到 QKT 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用</strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205132032.png" alt="image.png" /></p>
<p>Softmax 之前 Mask</p>
<p>得到 <strong>Mask</strong> QKT 之后在 <strong>Mask</strong> QKT上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p>**第四步：**使用 <strong>Mask</strong> QKT与矩阵 <strong>V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 Z1 是只包含单词 1 信息的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205141462.png" alt="image.png" /></p>
<p>Mask 之后的输出</p>
<p><strong>第五步：<strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 Zi ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出Zi 然后计算得到第一个 Multi-Head Attention 的输出</strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p>
<h3 id="52-第二个-multi-head-attention"><a class="markdownIt-Anchor" href="#52-第二个-multi-head-attention"></a> 5.2 第二个 Multi-Head Attention</h3>
<p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C</strong> 计算的。</p>
<p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出 <strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p>
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p>
<h3 id="53-softmax-预测输出单词"><a class="markdownIt-Anchor" href="#53-softmax-预测输出单词"></a> 5.3 Softmax 预测输出单词</h3>
<p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205150944.png" alt="image.png" /></p>
<p>Decoder Softmax 之前的 Z</p>
<p>Softmax 根据输出矩阵的每一行预测下一个单词：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205156309.png" alt="image.png" /></p>
<p>Decoder Softmax 预测</p>
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p>
<h2 id="6-transformer-总结"><a class="markdownIt-Anchor" href="#6-transformer-总结"></a> 6. Transformer 总结</h2>
<ul>
<li>
<p>Transformer 与 RNN 不同，可以比较好地并行训练。</p>
</li>
<li>
<p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</p>
</li>
<li>
<p>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q, K, V</strong>矩阵通过输出进行线性变换得到。</p>
</li>
<li>
<p>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>VLMO</title>
    <url>/2025/01/16/VLMO/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131749829">https://blog.csdn.net/lansebingxuan/article/details/13174…</a></p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/2111.02358.pdf">VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a></p>
<p>论文代码：<a href="https://github.com/microsoft/unilm/tree/master/vlmo">VLMO</a></p>
<p>VLMo提出了一种统一的视觉语言预训练模型（VLMO），该模型既可以用作双编码器，对检索任务的图像和文本进行单独编码，也可以用作融合编码器，对分类任务的图像-文本对的深度交互进行建模。</p>
<span id="more"></span>
<h2 id="研究动机"><a class="markdownIt-Anchor" href="#研究动机"></a> 研究动机</h2>
<p>现在多模态学习领域有两个主流的模型结构：</p>
<ol>
<li>
<p>双塔结构：图像有一个模型，文本有一个模型，双塔完全分开，互不干扰，模态之间的交互用非常简单的Cosine Similarity来完成，比如CLIP Align这种dual-encoder。<br />
结构优点：对检索任务极其有效，因为它可以提前把特征都抽好，接下来直接算Similarity矩阵乘法就可以，极其适合大规模的图像文本的检索，非常具有商业价值。<br />
结构的缺点：只计算Cosine Similarity无法做多模态之间深度融合，难一些的任务性能差。</p>
</li>
<li>
<p>单塔结构，Fusion Encoder的方式，先把图像和文本分开处理一下，但是当做模态交互的时候，用一个Transformer Encoder做模态之间的交互。<br />
结构优点：这个结构弥补双塔模式的缺陷，在VR、VE、VQA任务上效果特别好。<br />
结构缺点：当做检索任务的时候，因为只有一个模型，所以必须同时做推理，当图像文本对特别多，数据集特别大的时候，需要将所有all possible图像文本对全都要同时编码，然后计算Similarity Score，才能做检索，所以推理时间就会非常慢。</p>
</li>
</ol>
<p>本文的研究动机是：</p>
<ol start="3">
<li>
<p>结合上面两种结构的优点，设计模型网络，在Feed Forward FC层，每个模态就会对应自己不同的Expert，就是视觉有视觉的Vision Expert，Language有Language的Expert，Multi-model有Multi-model对应的Expert，这样在训练的时候输入哪个模态的数据，就训练哪个模态的Expert。推理的时候能根据现在输入的数据决定使用什么模型结构。</p>
</li>
<li>
<p>多模态的训练数据集不够多，但是在单模态里，就是视觉或者NLP里，可用的数据很多，基于这个研究动机，VLMo的作者提出了stagewise pre-training strategy，就是分阶段去训练，先把vision expert在视觉数据集这边训好，再把language expert在language的数据集上训好，这个时候模型本身的参数非常好的被初始化了，再到多模态的数据上做pre-training，效果就会好很多。</p>
</li>
</ol>
<h2 id="本文贡献1mome模型"><a class="markdownIt-Anchor" href="#本文贡献1mome模型"></a> 本文贡献1：MOME模型</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161108321.png" alt="image.png" /></p>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<p>VLMo论文的模型核心是一个transformer encoder的结构，但是它在每个transformer block里面做了一些改动，也就是他们提出的MoME transformer(mixture-of-modality-expert)。</p>
<p>标准的transformer block结构：一个标准的transformer block里先有一个Layer Norm，接下来是一个MSA multi-head self-attention，然后是Layer Norm，然后是一个FFN feed-forward network，最后有一个residual。</p>
<p>本文的transformer block结构：本文结构中layer norm、MSA、layer norm、residual connection这些都是一样的，唯一一个不一样的就是<strong>feed-forward network</strong>，它不是一个feed-forward network，而是针对不同的输入、不同的modality，有vision FFN、language FFN和vision language FFN，也就是switching modality expert，从而构建出MoME transformer block，也就是VLMo整个的模型结构。</p>
<p>注意：虽然的FFN层没有share weights，各自modality有各自的FFN层，但之前的self-attention层是share weights，也就是不论图像、文本还是图像文本信号，输入任何的token sequence，self-attention的model weights都是一样的，这个也是transformer架构的优势，或者说多模态学习接下来的趋势，目前有很多工作证明，同样的self-attention weights可以用来做不同的图像文本音频视频任务，不需要重新去训练自注意力参数。</p>
<h3 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h3>
<ol>
<li>
<p>Image Text Contrastive <strong>ITC</strong></p>
</li>
<li>
<p>Image Text Matching <strong>ITM</strong>：一个二分类任务</p>
</li>
<li>
<p>Mask Language Modeling <strong>MLM</strong>：预测这些被mask掉的单词</p>
</li>
</ol>
<p>本文从ALBEF里借鉴用这三个loss训练模型以及hard negative mining的思想。例如当计算ITC contrastive loss的时候，VLMo类似CLIP模型，图像端只有图像，输入ViT，FFN都用的Vision FFN，如果用Vision Transformer Base，就是12层的一个transformer，文本端就是文本的token单独输入language model，后面用的是language expert是一个12层的BERT base，当计算ITM或者mask language modeling 的时候，模型变成fusion encoder的形式，图像和文本的一起输入multi-head self-attention（<strong>这里的self-attention与之前的self-attention和后面的self-attention都是share weights，都是一样的，不论哪个modality，自注意力的参数都不变，都是共享的</strong>）。</p>
<p>在前面的L-F的transformer block里，模型对视觉和language信号分别做训练，也就是分别用Vision Expert（VE）和Language Expert(LE)，只有在最后的F层，才用Vision Language Expert训练。例如，如果用的是transformer base模型，前十层做VE和LE，后面F就是2，也就是后面只有两层transformer block做模态之间的融合。</p>
<h3 id="结构优点"><a class="markdownIt-Anchor" href="#结构优点"></a> 结构优点</h3>
<p>VLMo的优点是灵活，训练的时候可以通过各种modality选择训练哪个modality expert；推理的时候可以选择使用哪些模型参数。<strong>如果做检索任务，就像CLIP一样就用视觉和文本两个模型就可以，如果做Vision Language分类任务VR、VE、VQA，就用下面的Fusion Encoder模式。</strong></p>
<h3 id="结构缺点"><a class="markdownIt-Anchor" href="#结构缺点"></a> 结构缺点</h3>
<p>代价是VLMo里至少做了两次甚至三次的前向过程，VLMo的base模型在4 million的setting下训练，用64张V100的卡需要训练两天，训练量与ViLT同级别，比ALBEF慢。</p>
<h2 id="本文贡献2分阶段的训练策略"><a class="markdownIt-Anchor" href="#本文贡献2分阶段的训练策略"></a> 本文贡献2：分阶段的训练策略</h2>
<p>因为作者希望利用Unimodality里大量的图片文本去做预训练进而得到更好的模型初始化，所以先做Vision Pre-training，然后做Language Pre-training，最后做Vision Language Pre-training。</p>
<p>具体地，做Vision Pre-training的时候，是Unsupervised，用了他们自己的BEIT中的Mask Image Modeling，做Language Modeling的时候，使用Mask Language Modeling，Vision Language Pre-training用的上面三个目标函数。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161118723.png" alt="image.png" /></p>
<p>图中蓝色的虚线代表是Frozen FFN，橘黄色的虚线代表Frozen Self-attention。</p>
<ol>
<li>
<p>第一阶段做Vision Pre-training的时候，因为刚训练完，不需要freeze，所有参数随机初始化，所以12层的Transformer Block，包括前面自注意力和后面的Vision Expert都打开训练。</p>
</li>
<li>
<p>第二阶段做文本预训练的时候，Vision Expert被冻住，因为训练文本数据不需要训练Vision Expert，所以Vision Expert的FFN层参数就固定下来了，该阶段去训练Language Expert，并且Self-Attention也冻住了，因此<strong>在视觉数据上训练好一个模型，在视觉Token Sequence上训练好一个自注意力模型，可以直接拿来对文本数据进行建模，不需要fine-tune</strong>。但这个过程反过来不行，就是先在Language上去训练然后再在Vision上冻住去做结果不太好，但是先用Vision训练然后再在Text上直接去用Self-Attention，很多工作都证明是有效的。</p>
</li>
<li>
<p>第三阶段做多模态，所有训练参数都打开训练，包括Self-Attention，后面三个Expert都打开做fine-tune。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT (Vision Transformer)</title>
    <url>/2025/01/10/ViT-Vision-Transformer/</url>
    <content><![CDATA[<h2 id="vit-是什么"><a class="markdownIt-Anchor" href="#vit-是什么"></a> VIT 是什么</h2>
<p>在计算机视觉领域中，多数算法都是保持CNN整体结构不变，在CNN中增加attention模块或者使用attention模块替换CNN中的某些部分。有研究者提出，没有必要总是依赖于CNN。因此，作者提出<strong>ViT (Vision Transformer)</strong> 算法，仅仅使用Transformer结构也能够在图像分类任务中表现很好。</p>
<p>受到NLP领域中Transformer成功应用的启发，ViT算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。具体来讲，ViT算法中，会将整幅图像拆分成小图像块，然后把这些小图像块的线性嵌入序列作为Transformer的输入送入网络，然后使用监督学习的方式进行图像分类的训练。</p>
<span id="more"></span>
<p>该算法在中等规模（例如ImageNet）以及大规模（例如ImageNet-21K、JFT-300M）数据集上进行了实验验证，发现：</p>
<ul>
<li>
<p>Transformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果。具体表现为使用中等规模的ImageNet训练的Transformer会比ResNet在精度上低几个百分点。</p>
</li>
<li>
<p>当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平。</p>
</li>
</ul>
<h2 id="模型结构与实现"><a class="markdownIt-Anchor" href="#模型结构与实现"></a> 模型结构与实现</h2>
<p>我们先结合下面的动图来粗略地分析一下ViT的工作流程，如下：<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/18f056b695693c40b440e3ba9c368e07%20(1).gif" alt="" /></p>
<ul>
<li>
<p>将一张图片分成patches</p>
</li>
<li>
<p>将patches铺平</p>
</li>
<li>
<p>将铺平后的patches的线性映射到更低维的空间</p>
</li>
<li>
<p>添加位置embedding编码信息</p>
</li>
<li>
<p>将图像序列数据送入标准Transformer encoder中去</p>
</li>
<li>
<p>在较大的数据集上预训练</p>
</li>
<li>
<p>在下游数据集上微调用于图像分类</p>
</li>
</ul>
<h3 id="图像分块嵌入"><a class="markdownIt-Anchor" href="#图像分块嵌入"></a> 图像分块嵌入</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181841575.png" alt="image.png" /></p>
<p>考虑到在Transformer结构中，输入是一个二维的矩阵，矩阵的形状可以表示为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>是sequence的长度，而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span>是sequence中每个向量的维度。因此，在ViT算法中，首先需要设法将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi><mo stretchy="false">(</mo><mn>224</mn><mo>×</mo><mn>224</mn><mo>×</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H×W×C (224 \times 224 \times 3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span>的三维图像转化为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span>的二维输入。</p>
<p>ViT中的具体实现方式为：将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">H×W×C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>的图像，变为一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mtext>∗</mtext><mi>C</mi><mo stretchy="false">)</mo><mo>=</mo><mn>196</mn><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><msup><mn>6</mn><mn>2</mn></msup><mo>∗</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N×(P^2∗C) = 196 \times (16^2 * 3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span>的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mi>W</mi><mi mathvariant="normal">/</mi><msup><mi>P</mi><mn>2</mn></msup><mo>=</mo><mn>196</mn></mrow><annotation encoding="application/x-tex">N=HW/P^2 = 196</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">9</span><span class="mord">6</span></span></span></span>个图像块，每个图像块的维度则是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>P</mi><mn>2</mn></msup><mtext>∗</mtext><mi>C</mi><mo>=</mo><mn>1</mn><msup><mn>6</mn><mn>2</mn></msup><mo>∗</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">P^2∗C = 16^2 * 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>。其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>是图像块的大小，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>是通道数量。经过如上变换，就可以将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mtext>∗</mtext><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(P^2∗C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>，而我们实际需要的向量维度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span>，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mtext>∗</mtext><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(P^2∗C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord">∗</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>的图像块做一个线性变换，将维度压缩为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">D=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span></span></span></span>即可。</p>
<p>参考了BERT，在头部插入了维度为768的一个表征分类结果的 embedding，因此最后的输入维度为 197 * 768</p>
<h3 id="多头注意力"><a class="markdownIt-Anchor" href="#多头注意力"></a> 多头注意力</h3>
<p>将图像转化为 N×(P2∗C) 的序列后，就可以将其输入到 Transformer 结构中进行特征提取了，如图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181859449.png" alt="image.png" /></p>
<p>Transformer 结构中最重要的结构就是 Multi-head Attention，即多头注意力结构。具有2个head的 Multi-head Attention 结构如图所示。输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">a^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.824664em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span>经过转移矩阵，并切分生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mtext>、</mtext><msup><mi>q</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mtext>、</mtext><msup><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mtext>、</mtext><msup><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mtext>、</mtext><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mtext>、</mtext><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">q^{(i,1)}、q^{(i,2)}、k^{(i,1)}、k^{(i,2)}、v^{(i,1)}、v^{(i,2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>，然后 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">q^{(i,1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">k^{(i,1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>做 attention，得到权重向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>，将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">v^{(i,1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>进行加权求和，得到最终的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">b^{(i,1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>，同理可以得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">b^{(i,2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>。接着将它们拼接起来，通过一个线性层进行处理，得到最终的结果。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181908858.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181919268.png" alt="image.png" /></p>
<h3 id="mlp-头"><a class="markdownIt-Anchor" href="#mlp-头"></a> MLP 头</h3>
<p>上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的，以ViT-B/16为例，输入的是[197, 768]输出的还是[197, 768]。这里我们只是需要分类的信息，所以我们只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。</p>
<h2 id="讨论"><a class="markdownIt-Anchor" href="#讨论"></a> 讨论</h2>
<ul>
<li>为什么小数据时CNN效果要好于VIT，大数据时VIT效果更好一些？</li>
</ul>
<p>传统的CNN卷积神经网络有很强的inductive biases（归纳偏置），是根据图像的本质而设计的一种网络，比如卷积核的平移不变性和局部性。因此在小数据时，VIT学不到这些，自然效果就不好，但是数据多起来后，就不太需要针对图像特意去设计这些小tips了。</p>
<ul>
<li>混合架构</li>
</ul>
<p>作为原始图像图块的替代，可以从CNN的特征图形成输入序列。在该混合模型中，将patch embedding投影应用于从CNN特征图提取的图块。（即，将图像块先经过CNN网络，然后将CNN的网络输出的特征矩阵输进Transformer中，这样也是可以的，论文有尝试过）</p>
<ul>
<li>位置信息</li>
</ul>
<p>Transformer和CNN不同，需要position embedding来编码tokens的位置信息，这主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。但是CNN是滑动的，是本身就具有位置信息的。所以在使用VIT时要手动加上位置信息。论文中试验过使用1-D和2-D的位置信息效果差不多。</p>
<ul>
<li>微调和更高的分辨率</li>
</ul>
<p>通常，我们在大型数据集上对ViT进行预训练，并微调到（较小的）下游任务。为此，我们删除了预训练的预测head，并附加了一个零初始化的D*K的前馈层，其中K是下游类的数量。以比预训练更高的分辨率进行微调通常是有益的。当提供更高分辨率的图像时，我们将图块大小保持不变，这会导致更大的有效序列长度。ViT可以处理任意序列长度（直到内存限制），但是，预训练的位置embedding可能不再有意义。因此，我们根据预先训练的位置嵌入在原始图像中的位置执行2D插值。请注意，只有在分辨率调整和色块提取中，将有关图像2D结构的感应偏差手动注入到Vision Transformer中。</p>
<p>参考链接：</p>
<ul>
<li>
<p><a href="https://blog.csdn.net/lsb2002/article/details/135320751">Visual Transformer (ViT)模型详解-CSDN博客</a></p>
</li>
<li>
<p><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/classification/ViT.html">https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/classification/ViT.html</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>ViLT</title>
    <url>/2025/01/16/ViLT/</url>
    <content><![CDATA[<p><strong>ViLT：最简单的多模态Transformer</strong></p>
<h1 id="综述"><a class="markdownIt-Anchor" href="#综述"></a> 综述</h1>
<p>作者根据下面两点将当前 VLP(Vision-and-Language Pre-training) 模型分成了四大类，并且总结了不同类别方法的特点：</p>
<ul>
<li>
<p>图像和文本的表达力度是不是平衡，如参数量和计算</p>
</li>
<li>
<p>图像和文本两个模态的信息是怎么融合的</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160713958.png" alt="image.png" /></p>
<span id="more"></span>
<p>**VSE、VSE++和SCAN属于(a)类型。**对图像和文本独立使用encoder，图像的更重，文本的更轻，使用简单的<a href="https://zhida.zhihu.com/search?content_id=170292842&amp;content_type=Article&amp;match_order=1&amp;q=%E7%82%B9%E7%A7%AF&amp;zhida_source=entity">点积</a>或者浅层attention层来表示两种模态特征的相似性。</p>
<p>**CLIP属于(b)类型。**每个模态单独使用重的transformer encoder，使用池化后的<a href="https://zhida.zhihu.com/search?content_id=170292842&amp;content_type=Article&amp;match_order=1&amp;q=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&amp;zhida_source=entity">图像特征</a>点积计算特征相似性。</p>
<p><strong>ViLBERT、UNTER和</strong><a href="https://zhida.zhihu.com/search?content_id=170292842&amp;content_type=Article&amp;match_order=1&amp;q=Pixel-BERT&amp;zhida_source=entity"><strong>Pixel-BERT</strong></a>**属于©类型。**这些方法使用深层transformer进行交互作用，但是由于VE仍然使用重的<a href="https://zhida.zhihu.com/search?content_id=170292842&amp;content_type=Article&amp;match_order=1&amp;q=%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;zhida_source=entity">卷积网络</a>进行特征抽取，导致计算量依然很大。</p>
<p>**作者提出的ViLT属于(d)类型。**ViLT是首个将VE设计的如TE一样轻量的方法，该方法的主要计算量都集中在<a href="https://zhida.zhihu.com/search?content_id=170292842&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92&amp;zhida_source=entity">模态交互</a>上。</p>
<p>模态融合很重要，所以要看看当前的模态融合是怎么做的：</p>
<ul>
<li>
<p>**single-stream：**把两个序列 concat 成一个序列，然后让 transformer 自己去学习怎么融合</p>
</li>
<li>
<p>**dual-stream：**先各自对各自的输入做处理，挖掘单独模态中包含的信息，然后再融合</p>
</li>
</ul>
<p>两种融合方法基本效果差不多，dual-stream 有时候会更好一点，但 dual-stream 引入了更多的参数，ViLT 作者还是使用了 single-stream 更轻量的方法</p>
<p>多模态融合之前，特征怎么提取：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160726329.png" alt="image.png" /></p>
<p>**文本端：**使用预训练好的 BERT</p>
<p><strong>视觉端：</strong></p>
<ul>
<li>
<p>之前的方法是先将输入图像通过 backbone 来提取特征，然后使用 RPN 网络来抽取一些 ROI，做一次 NMS 降低区域的数量，然后经过 head 得到 bbox，整个过程非常贵</p>
</li>
<li>
<p>还有一些方法使用 Grid Feature，就是直接用抽出的特征，拉平后直接用，但效果不好</p>
</li>
<li>
<p>所以 ViLT 借鉴 ViT 的 patch embedding layer 来抽取特征，又快又好</p>
</li>
</ul>
<h1 id="vilt-方法"><a class="markdownIt-Anchor" href="#vilt-方法"></a> ViLT 方法</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160811143.png" alt="image.png" /></p>
<p>ViLT 是一个 single-stream 的结构，所以只有一个模型：</p>
<ul>
<li>
<p>文本输入：文本先输入 BERT tokenizer，得到 word embedding，假设文本的长度是 L，H 就是 embedding 维度，一般 base 模型就是 768，就是 Lx H 大小</p>
</li>
<li>
<p>图像输入：把图像打成 patch，每个 patch 进行 patch embedding，得到一系列的 token 编码（紫色），假设图像 token 长度为 N 维度也是 H 那么就 NxH 的特征</p>
</li>
<li>
<p>*：cls token，绿色的是文本的，紫色的是图像的</p>
</li>
<li>
<p>灰色：表示模态，0 表示文本模态，1 表示图像模态，需要 modal-type embedding 的原因在于，对于 single stream 的方法来说，是会把图像和文本的输入拼接在一起的，如果不告诉模型哪些是文本或哪些是图像的话，是不利于学习的，如果告诉了后，模型就可能互相学习文本和图像之间的关系，更有利于找到潜在的关系</p>
</li>
<li>
<p>输入输出序列长度：每个 token embedding 输入 transformer encoder 之前呢，都是由三部分组成的：灰色+绿色+淡绿色的，是加到一起的，而不是 concat，然后所有相加后的特征会被 concat 起来，送入 transformer encoder，所以输入长度就是 1+L+1+N，整个输入的特征就是（N+L+2)xH，输出的序列也是（N+L+2)xH</p>
</li>
</ul>
<p>预训练的两个 Loss：</p>
<ul>
<li>
<p>Image-text matching loss：</p>
<ul>
<li>
<p>该 loss 是用于衡量文本和图像之间的距离，是人为设计的任务，也就是二分类的任务，判断匹配与否，真正有用的位置是 Pooler 位置，在 Pooler 之前呢，这个位置输出的特征是 1xH，经过 Pooler 相当于权重矩阵，得到 HxH，经过 FC 得到 1xH 的输出，做二分类。</p>
</li>
<li>
<p>ViLT 中还使用了 word patch alignment loss，使用最优传输理论来计算分布之间的距离</p>
</li>
</ul>
</li>
<li>
<p>Masked language modeling loss：</p>
<ul>
<li>针对文本的目标函数，也就是 NLP 那边常见的完形填空，也就是在输入文本的时候，随机 MASK 掉一个单词，然后在输出的时候重建出来这个 mask</li>
</ul>
</li>
</ul>
<p>文本模型中使用的小技巧：whole word masking：</p>
<ul>
<li>
<p>作者使用将整个单词都 mask 掉的方法，能进一步的加强文本和图像之间的关系的学习</p>
</li>
<li>
<p>当把单词抹掉之后，句子重建就很困难，那就只能从图像中来拿取信息，所以就能迫使模型来建立图像和文本的关系</p>
</li>
</ul>
<p>图像模型中使用的小技巧：</p>
<ul>
<li>
<p>前面说过数据增强在图像中很有用，但多模态中不太能用好数据增强</p>
</li>
<li>
<p>本文作者做了改动，也就是不用 color 和 cutout，就能尽可能的保证图像和文本保持原有的匹配关系</p>
</li>
</ul>
<h1 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h1>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/369733979">https://zhuanlan.zhihu.com/p/369733979</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/jiaoyangwm/article/details/132247400">【多模态】25、ViLT | 轻量级多模态预训练模型（ICML2021）_vilt预训练模型-CSDN博客</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/12/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2>
<h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>为什么LLM倾向于Decoder-only架构</title>
    <url>/2025/01/24/%E4%B8%BA%E4%BB%80%E4%B9%88LLM%E5%80%BE%E5%90%91%E4%BA%8EDecoder-only%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="什么是-encoder-decoder-prefix-lm-decoder-only-架构"><a class="markdownIt-Anchor" href="#什么是-encoder-decoder-prefix-lm-decoder-only-架构"></a> 什么是 Encoder-Decoder, Prefix-LM, Decoder-Only 架构</h1>
<p>链接：<a href="https://www.zhihu.com/question/588325646/answer/3073802666">https://www.zhihu.com/question/588325646/answer/3073802666</a></p>
<p>目前以Transfromer为基础自回归生成大致可以分为三种架构：</p>
<ul>
<li>
<p>Encoder-Decoder架构，代表开源模型：T5</p>
</li>
<li>
<p>Prefix-LM架构，代表开源模型：ChatGLM</p>
</li>
<li>
<p><a href="https://zhida.zhihu.com/search?content_id=588958526&amp;content_type=Answer&amp;match_order=1&amp;q=Decoder-Only%E6%9E%B6%E6%9E%84&amp;zhida_source=entity">Decoder-Only架构</a>（也叫Causal-LM），代表开源模型：GPT3</p>
</li>
</ul>
<p>先来个结论：Decoder-Only相对于其它二者的优点，**是条件信息和生成信息之间更加对齐，GAP更小，因此更容易训练。**但我还是澄清我的观点，容易训练不代表最终表现会更好，因此，不代表其它架构没有研究的价值，甚至一定程度上还可以说它们潜力更大。</p>
<span id="more"></span>
<p>且听笔者细细道来。</p>
<hr />
<p>首先需要知道一点就是目前的自回归生成文本的应用场景都是给一段条件文本，然后在这个条件文本的基础上开始自回归生成后续文本，生成一句话的建模概率可以表示为如下：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x_1, x_2, ... , x_n, y_1, y_2, ..., y_m)=\prod_{i=1}^{m}p(y_i|Q(x_1, x_2,...,x_n),y_1,y_2,...,y_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.104002em;vertical-align:-0.29971000000000003em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∏</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,...,x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 代表条件信息token， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> 代表信息压缩模型，其实在这里就代表了Encoder， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">y_1,y_2,...,y_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为自回归生成的token。</p>
<p>也就是说，NLG任务都需要先对条件文本做一个信息压缩，然后将压缩信息传给decoder模型，使其生成文本。</p>
<p>我们先看看三者的条件信息与预测信息的处理方式和架构设计。</p>
<h3 id="1encoder-decoder架构"><a class="markdownIt-Anchor" href="#1encoder-decoder架构"></a> 1.Encoder-Decoder架构</h3>
<p>其一般结构都是如下：</p>
<p>Encoder-Decoder架构</p>
<p>Encoder就是一个条件信息压缩模型 QQQ ，Encoder和Decoder之间天然有一个结构上的分离，Encoder的attention矩阵信息是双向的：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173714595.png" alt="image.png" /></p>
<p>但Decoder却是单向自回归生成。</p>
<h3 id="2prefix-lm前缀语言模型架构"><a class="markdownIt-Anchor" href="#2prefix-lm前缀语言模型架构"></a> 2.Prefix-LM前缀语言模型架构</h3>
<p>其一般结构都是如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173722568.png" alt="image.png" /></p>
<p>可以看到Prefix-LM中前半段深黑色的连线为双向语言模型的标准架构，而在后半段通过mask attetnion矩阵使其成为递归生成的单向语言模型，其信息压缩模型 QQQ 也就是Encoder的前半段。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173837755.png" alt="image.png" /></p>
<h3 id="3decoder-only架构"><a class="markdownIt-Anchor" href="#3decoder-only架构"></a> 3.Decoder-Only架构</h3>
<p>也被称为Causal（因果）架构，其一般结构都是如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173736657.png" alt="image.png" /></p>
<p>可以看到该类模型全程都是单向语言信息传输，其做法也是将attention后向信息部分mask掉。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173743020.png" alt="image.png" /></p>
<p>了解完以上三种架构就可以发现为什么Decoder-Only架构条件信息和生成信息之间更加对齐了。Prefix-LM和Encoder-Decoder架构，其信息压缩模型 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> 由于是双向的，建模概率中包含上下文信息，和Decoder本身的建模概率之间是有一定的GAP的，Decoder只有上文信息。</p>
<p>并且Prefix-LM和Encoder-Decoder模型都有专门针对Encoder部分的预训练，目前已知的Encoder模型的预训练任务和Decoder自回归生成任务之间都有GAP，比如MLM任务，NSP任务，SR(Sentence Reordering)任务，SD(Sentence Distance)任务等等。</p>
<p>Decoder-Only架构并不是没有信息压缩模型，其信息压缩模型 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> 就是Decoder自身。因此不论是在预训练任务层面以及条件信息的压缩层面相比其他架构GAP都比较小。</p>
<p>但是，Decoder-Only架构的训练任务并不是完全没有GAP，为了使Transformer能够并行训练，大多数Decoder-Only模型预训练时都采用了Teacher Forcing的模式，即训练时，用label的<a href="https://zhida.zhihu.com/search?content_id=588958526&amp;content_type=Answer&amp;match_order=1&amp;q=token+embedding&amp;zhida_source=entity">token embedding</a>作为计算下一个token的输入，而不是预测的token embedding作为计算下一个token的输入，这也带来了一部分GAP，不过这部分产生的GAP比Encoder的不同建模概率带来的GAP小多了，再加上有很多论文专门提出了解决这个问题的一些方法，因此进一步缩小了GAP，如Scheduled Sampling等等。</p>
<p>条件信息和生成信息的GAP更小，那么Decoder模型识别 QQ Q 输出的压缩信息的成本就更低，自然模型就更加容易训练。</p>
<h2 id="decoder-only一定是最好的吗"><a class="markdownIt-Anchor" href="#decoder-only一定是最好的吗"></a> Decoder-Only一定是最好的吗？</h2>
<p>但是个人觉得Encoder-Decoder架构的潜力应该会更大一些，<strong>尤其是对于中文这种“先组织、再输出”的语言</strong>，个人猜想，如果能设计好Encoder的训练任务和模型结构，将Encoder打造成一个语言组织者，然后将Decoder打造成一个文本输出者，让Encoder组织和指导Decoder输出文本，当然这需要设计好预训练任务和模型结构，现有的Transformer结构和pretrain任务可能无法完成以上猜想。</p>
<h1 id="注意力满秩的问题"><a class="markdownIt-Anchor" href="#注意力满秩的问题"></a> 注意力满秩的问题</h1>
<p>链接：<a href="https://www.zhihu.com/question/588325646/answer/7184008440">https://www.zhihu.com/question/588325646/answer/7184008440</a></p>
<p><strong>简介</strong></p>
<p>最近和同事讨论transformer注意力机制单双向的时候，又温故了一遍<a href="https://zhida.zhihu.com/search?content_id=694552386&amp;content_type=Answer&amp;match_order=1&amp;q=%E8%8B%8F%E7%A5%9E&amp;zhida_source=entity">苏神</a>的这篇文章，“为什么现在主流LLM都是<a href="https://zhida.zhihu.com/search?content_id=694552386&amp;content_type=Answer&amp;match_order=1&amp;q=Decoder-only%E6%9E%B6%E6%9E%84&amp;zhida_source=entity">Decoder-only架构</a>”。链接如下：<a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/9529/comment-page-1">为什么现在的LLM都是Decoder-only的架构？ - 科学空间|Scientific Spaces</a>。</p>
<p>读完发现这里边有很强的背景信息？比如，下面两个问题：</p>
<ul>
<li>
<p>为什么双向就会低秩？</p>
</li>
<li>
<p>为什么低秩表达能力会下降？</p>
</li>
</ul>
<p>所以，本篇回答主要是解释清楚这两个问题。</p>
<h2 id="为什么双向就会低秩"><a class="markdownIt-Anchor" href="#为什么双向就会低秩"></a> 为什么双向就会低秩？</h2>
<h3 id="双向注意力矩阵"><a class="markdownIt-Anchor" href="#双向注意力矩阵"></a> 双向注意力矩阵</h3>
<p>为了方便计算秩，这里使用整型，</p>
<p>如下矩阵A，将第一行的倍数，加到其他行，很容易得到一个初等矩阵，其<strong>秩为4。</strong></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>2</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>3</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>2</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>4</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>7</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>10</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>8</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">A=\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ -1 &amp; -1 &amp; -2 &amp; -3 \\ -2 &amp; -4 &amp; 7 &amp; 10 \\ 2 &amp; 4 &amp; 8 &amp; 9 \end{bmatrix}\\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">4</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">7</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">8</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">3</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span><span class="mspace newline"></span></span></span></p>
<p>同样如下一个矩阵B，第四行完全可以由第三行通过初等变换表示，其<strong>秩为3。</strong></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>8</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>16</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>2</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>6</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>9</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>3</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>4</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>5</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>6</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>8</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>10</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">B=\begin{bmatrix} 5 &amp; 8 &amp; 3 &amp; 16 \\ -2 &amp; -6 &amp; 9 &amp; 1 \\ -3 &amp; 2 &amp; -4 &amp; -5 \\ -6 &amp; 4 &amp; -8 &amp; -10 \end{bmatrix} \\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">3</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">8</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">6</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">4</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">8</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">6</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">5</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span><span class="mspace newline"></span></span></span></p>
<p>以上两个矩阵，都可能是双向注意力矩阵，他们的秩也是有所不同。通过这两个示例，可以得知双向注意力矩阵最大为满秩，也有可能不为满秩。但是，<strong>并不是说双向就一定是低秩的。</strong></p>
<h3 id="单向注意力矩阵"><a class="markdownIt-Anchor" href="#单向注意力矩阵"></a> 单向注意力矩阵</h3>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.23</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>0.45</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.78</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.67</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>0.90</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.12</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>0.34</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.56</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>0.78</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.90</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">C=\begin{bmatrix} 0.23 &amp; 0 &amp; 0 &amp; 0 \\ -0.45 &amp; 0.78 &amp; 0 &amp; 0 \\ 0.67 &amp; -0.90 &amp; 0.12 &amp; 0 \\ -0.34 &amp; 0.56 &amp; -0.78 &amp; 0.90 \end{bmatrix}\\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80303em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">3</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">5</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">7</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mord">8</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">0</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mord">2</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span><span class="mord">8</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">9</span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6529999999999996em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.79999em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.3959900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.4119800000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.653em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span><span class="mspace newline"></span></span></span> 如上矩阵C，本身是一个下三角矩阵，<strong>它一定是满秩</strong>，秩一定是4，所以不存在退化问题。</p>
<h2 id="为什么低秩表达能力会下降"><a class="markdownIt-Anchor" href="#为什么低秩表达能力会下降"></a> 为什么低秩表达能力会下降？</h2>
<h3 id="秩和值域维度关系"><a class="markdownIt-Anchor" href="#秩和值域维度关系"></a> 秩和值域维度关系</h3>
<p>根据秩的几何意义可知，对于相同的自然定义域，如果越小，那么值域的维度就越小。比如：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124173958663.png" alt="image.png" /></p>
<h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3>
<p><strong>为什么秩为2是值域平面？</strong></p>
<p>设x是任意的一个取值为实数的向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">x=\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，A是一个矩阵，设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">A=\begin{bmatrix} 1 &amp; 2 \\ 1 &amp; 3 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，通过矩阵初等变换，可以得知，此矩阵的秩为2。</p>
<p>则 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Ax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span></span></span></span> 是一个两行一列的向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>3</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix} 1x_1+2x_2 \\ 1x_1+3x_2 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">3</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span> ，将矩阵转化为方程式： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mi>x</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>3</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mi>y</mi></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\left\{ \begin{aligned} 1x_1+2x_2=x \\ 1x_1+3x_2=y \end{aligned} \right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.00003em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal">x</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">3</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，通过此方程式，对于x，y取遍一切实数， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>就能铺满整个二维平面。</p>
<p><strong>为什么秩为1是值域直线？</strong></p>
<p>同样设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">A=\begin{bmatrix} 1 &amp; 2 \\ 1 &amp; 2 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，很容易判断此矩阵的秩为1，同理，则 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">Ax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span></span></span></span>是一个两行一列的向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix} 1x_1+2x_2 \\ 1x_1+2x_2 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span>，将矩阵转化为方程式： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mi>x</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mn>1</mn><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mi>y</mi></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\left\{ \begin{aligned} 1x_1+2x_2=x \\ 1x_1+2x_2=y \end{aligned} \right.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.00003em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal">x</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，通过此方程式，可以发现，<strong>x和y始终相等</strong>，对于 ，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mtext>，</mtext><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x，y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord cjk_fallback">，</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 取遍一切实数，他所有表达的一条直线。</p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>通过以上两个示例，可以发现，<strong>秩越大，值域范围越广</strong>，而单向注意力机制则是满秩，所以表达能力是要强于<a href="https://zhida.zhihu.com/search?content_id=694552386&amp;content_type=Answer&amp;match_order=1&amp;q=%E5%8F%8C%E5%90%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;zhida_source=entity">双向注意力机制</a>（可能的低秩）。此时再看苏神的博客就会好理解一些了。</p>
<h1 id="预训练任务难度问题"><a class="markdownIt-Anchor" href="#预训练任务难度问题"></a> 预训练任务难度问题</h1>
<p>链接：<a href="https://www.zhihu.com/question/588325646/answer/3173454912">https://www.zhihu.com/question/588325646/answer/3173454912</a></p>
<p>bert的双向模型可以看到前向和后向，这在预测的时候是天然的优势，但在训练的时候其实反而降低了学习难度，自回归模型的最难任务还是会比双向模型的最难任务要难得多。</p>
<p>换句话说bert的双向提高了下限也拉低了上限。而当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高。</p>
<h1 id="zero-shot能力问题"><a class="markdownIt-Anchor" href="#zero-shot能力问题"></a> Zero-shot能力问题</h1>
<p>链接：<a href="https://www.zhihu.com/question/588325646/answer/2932131238">https://www.zhihu.com/question/588325646/answer/2932131238</a></p>
<h2 id="1-更好的zero-shot性能-更适合于大语料自监督学习"><a class="markdownIt-Anchor" href="#1-更好的zero-shot性能-更适合于大语料自监督学习"></a> 1. 更好的Zero-Shot性能、更适合于大语料自监督学习</h2>
<p>首先，对encoder-decoder与decoder-only的比较早已有之。咱们先把目光放放到模型参数动辄100B之前的时代，看看小一点的模型参数量下、两个架构各有什么优势——Google Brain 和 HuggingFace联合发表的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2204.05832">What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a> 曾经在5B的参数量级下对比了两者性能。</p>
<p>论文最主要的一个结论是decoder-only模型<strong>在没有任何tuning数据的情况下、</strong><a href="https://zhida.zhihu.com/search?content_id=563197831&amp;content_type=Answer&amp;match_order=1&amp;q=zero-shot&amp;zhida_source=entity"><strong>zero-shot</strong></a><strong>表现最好</strong>，而encoder-decoder则需要在一定量的标注数据上做multitask finetuning才能激发最佳性能。</p>
<p>而目前的Large LM的训练范式还是在大规模语料上做自监督学习，很显然，Zero-Shot性能更好的decoder-only架构才能更好地利用这些无标注数据。</p>
<p>此外，Instruct GPT在自监督学习外还引入了RLHF作辅助学习。RLHF本身也不需要人工提供任务特定的标注数据，仅需要在LLM生成的结果上作排序。虽然目前没有太多有关RLHF + encoder-decoder的相关实验，直觉上RLHF带来的提升可能还是不如multitask finetuning，毕竟前者本质只是ranking、引入监督信号没有后者强。</p>
<h2 id="2-大数据训练大参数模型的涌现能力替代了multitask-finetuning"><a class="markdownIt-Anchor" href="#2-大数据训练大参数模型的涌现能力替代了multitask-finetuning"></a> 2. 大数据训练+大参数模型的涌现能力替代了multitask finetuning</h2>
<p>前面说到，5B参数量+170B token数据量时，在做multitask finetuning后encoder-decoder相比decoder-only反而在新任务上会有一定的优势。</p>
<p>那么，在参数量再上一个台阶后，我们知道了LLM大模型表现出了<strong>涌现能力 （emergent abilities）</strong>。关于涌现能力，爱丁堡大学的Yao Fu博士有一篇很好的博客详细阐述：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f">A Closer Look at Large Language Models Emergent Abilities​yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f</a></p>
<p>简言之，在模型参数量足够大时，模型的能力提升不再遵守以往的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2001.08361">log-linear的提升法则</a>，而是突然急速增强性能。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174008344.png" alt="image.png" /></p>
<p>左图是传统的对数-线性性能曲线，而右图的LLM大模型在参数量突破某一量级后会急速提升性能、突破以往的对数-线性曲线。图源自Yao Fu博士的博客。</p>
<p>我们迄今还没有办法解释涌现能力为何出现。涌现能力的一个表现是，参数量达到一定量级后，模型具有了&quot;复杂的推理能力&quot;——譬如从非结构化的文本中自动地提取结构化的知识。</p>
<p>那么，LLM也可以自动地从大数据里面做self multitask finetuning。具体来讲，大数据里面本身天然蕴含了许多任务：比如 双语网页数据-机器翻译、论文(摘要+正文)数据-文本摘要、维基百科数据-命名实体识别等等。</p>
<p>因此，对于常见的NLP任务、LLM可以视为已经self finetuning过了；对于复杂问题，LLM的推理能力可以把这些问题转换成几个基本任务的”和“。譬如<a href="https://link.zhihu.com/?target=https%3A//aclanthology.org/P17-1147/">Closed-book question-answering</a>任务，模型可以将其转换为 Knowledge Graph Completion + Reading Comprehension + Question Answering的组合。</p>
<p>由此，encoder-decoder在multitask finetuning上的优势在大参数量时被LLM的推理能力给拉平了。</p>
<h2 id="3-in-context-learning对llm有few-shot-finetune的作用"><a class="markdownIt-Anchor" href="#3-in-context-learning对llm有few-shot-finetune的作用"></a> 3. In-context learning对LLM有few-shot finetune的作用</h2>
<p>最后，在实际使用LLM时，我们经常会加入<a href="https://zhida.zhihu.com/search?content_id=563197831&amp;content_type=Answer&amp;match_order=1&amp;q=Chain-of-Thought&amp;zhida_source=entity">Chain-of-Thought</a>或者In-Context信息来作为prompt进一步激发模型潜力——例如加入一些例句让<a href="https://zhida.zhihu.com/search?content_id=563197831&amp;content_type=Answer&amp;match_order=1&amp;q=GPT%E7%B1%BB%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity">GPT类模型</a>来模仿、生成更好的结果。</p>
<p>无In-context的生成<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174017337.png" alt="image.png" /></p>
<p>有In-context的生成<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174023994.png" alt="image.png" /></p>
<p>以上两个例子是我使用<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/gpt2%3Ftext%3DCapital%2Bof%2BKorea%2Bis%2BSeoul.%2B%250ACapital%2Bof%2BChina%2Bis%2BBeijing.%2B%250ACapital%2Bof%2BJapan%2Bis">GPT-2的demo</a>生成的，可见GPT类模型In-Context Learning能力很强（虽然后者后面也生成错了一些case，但远强于前者的non-sense）。</p>
<p>近期有论文指出In-Context信息可以视为一种task finetuning</p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2212.10559">Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers​arxiv.org/abs/2212.10559</a></p>
<p>论文的数学推导是定性的，大体上是将prompt信息归为对Transformer Attention层参数的微调。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174051638.png" alt="image.png" /></p>
<p>作者推导的核心步骤</p>
<p>按照这篇论文的思路，decoder-only的架构相比encoder-decoder在In-Context的学习上会更有优势，因为前者的prompt可以更加直接地作用于decoder每一层的参数，微调信号更强。也因此，更适合ChatGPT这类开放域的对话模型作为基础模型。同理，在总数据量少的情况下，In-Context + Decoder-only也更具有few-shot的优势——Google近期的论文在机器翻译上也观察到了类似现象，即中等量的单语数据+大模型+In context就能学习出很好的翻译效果：</p>
<hr />
<h2 id="小结以及encoder-decoder的未来"><a class="markdownIt-Anchor" href="#小结以及encoder-decoder的未来"></a> 小结，以及encoder-decoder的未来</h2>
<p>总言之，decoder-only在参数量不太大时就更具有更强的zero-shot性能、更匹配主流的自监督训练范式；而在大参数量的加持下，具有了涌现能力后、可以匹敌encoder-decoder做finetuning的效果；在In Context的环境下、又能更好地做few-shot任务。</p>
<p>我在这里说些个人看法：decoder-only架构其实也更天然地符合传统的Language Model的模式，前几年encoder-decoder模型的火爆更多是依赖于在特定标注数据上的训练——比如Transformer论文中经典的WMT机器翻译任务。</p>
<p>在未来，Large Language Model+自监督训练应该还是会继续采用decoder-only的架构。encoder-decoder有两个特点可能会使得它在以下两类任务上有优势：</p>
<ul>
<li>encoder的多样性。许多多模态工作(<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2201.12086.pdf">BLIP</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2107.07651.pdf">ALBEF</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2108.10904.pdf">SimVLM</a>)上可以看见encoder-decoder的影子，因为encoder可以用来encode多种模态的信息，而decoder-only的多模态工作相对较少（微软近日的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2303.04671.pdf">visual-ChatGPT</a>更多偏向模型级联pipeline）。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174059388.png" alt="image.png" /></p>
<p>BLIP模型：每个模态可以有一个encoder</p>
<ul>
<li>Deep Encoder+Shallow Decoder的推理优势。Encoder-decoder架构本来在计算效率上（以FLOPs衡量）就是优于其他LM的，且工业界目前常使用Deep Encoder+Shallow Decoder的组合，由于encoder本身并行度高，这类encoder-decoder的infer速度远超大型的decoder-only。在有较多标注数据的任务上，encoder-decoder还是具有成本优势的。</li>
</ul>
<h1 id="训练效率问题-infra角度"><a class="markdownIt-Anchor" href="#训练效率问题-infra角度"></a> 训练效率问题 (Infra角度)</h1>
<p>链接：<a href="https://www.zhihu.com/question/588325646/answer/3422090041">https://www.zhihu.com/question/588325646/answer/3422090041</a></p>
<p>个人觉得 Decoder-Only 的架构最核心的优势是非常方便于 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Scale+Up&amp;zhida_source=entity">Scale Up</a>，基于 Scaling Laws 的<strong>实际训练成本最低。</strong> Scaling Laws 是 LLM 时代最为重要的规律发现，因此有以下的暴论：</p>
<p>在 LLM 时代，如果你提出的新的算法结构可能有 5% 的效果提升，但是引入了额外 50% 的训练成本（计算时间 or 通信量） 的话，那这个新的算法一定是一个负优化。 因为这 50% 的训练成本，基于 Scaling Laws 我可以在原模型上多训练 50% 的 tokens ，或者训练大一半的模型， 带来的最终提升都远大于新算法的 5%。 因此，新的算法研究必然在探索阶段就需要引入 Infra 因素的考量。</p>
<h2 id="流水并行是千卡以上分布式训练中最重要的特性"><a class="markdownIt-Anchor" href="#流水并行是千卡以上分布式训练中最重要的特性"></a> 流水并行是千卡以上分布式训练中最重要的特性</h2>
<p>流水并行 (Pipeline Parallelism ) 是 LLM <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=2&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83&amp;zhida_source=entity">分布式训练</a>扩展到千卡集群以上的一个核心 feature</p>
<p>NVIDIA 在 3076 张 A100 集群上训练的 1T 参数量 LLM 使用的并行方式是：</p>
<ul>
<li>
<p><a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Data+Parallel&amp;zhida_source=entity">Data Parallel</a> Size = 6</p>
</li>
<li>
<p><a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Tensor+Parallel&amp;zhida_source=entity">Tensor Parallel</a> Size = 8</p>
</li>
<li>
<p>Pipeline Parallel Size = 64</p>
</li>
</ul>
<p>可见并行度最高的是 流水并行，超过 DP 和 TP 十倍左右。为什么在三千卡集群上最主要的并行方式是流水并行？</p>
<p>流水并行的核心优势就是用比较少的 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Pipeline+Bubble&amp;zhida_source=entity">Pipeline Bubble</a> 代价 （当 gradient accumulation step 很大时可以忽略不计），较少的 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Tensor+Buffer&amp;zhida_source=entity">Tensor Buffer</a> 显存代价，以及非常低的通信开销，将大模型分割在不同的 Group 中。 大幅减少了单张 GPU 上的 weight tensor 大小（数量） 和 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Activation+tensor&amp;zhida_source=entity">Activation tensor</a> 大小（数量）。</p>
<p>同时，跟 Tensor Parallel 相比， Pipeline Parallel 的通信代价很低且可以被 overlap， Tensor Parallel 虽然也能切分模型大小，但是需要全量的数据（没有减少 Activation tensor 大小），另外极高的通信频率和通信量使得 Tensor Parallel 只能在机器内 8 张卡用 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=NVLink&amp;zhida_source=entity">NVLink</a> 等高速互联来实现，跨机的 TP 会严重拖慢速度。</p>
<p>不仅如此， Pipeline Parallel 还将 Data Parallel 的模型更新限定在一个很小的范围内（比如六台机器）， DP 所需的 AllReduce 通信会随着机器数量增多而变慢。 PP 也让 DP 所需同步的<a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=%E6%A8%A1%E5%9E%8B%E6%A2%AF%E5%BA%A6&amp;zhida_source=entity">模型梯度</a>大小变小了，大大减缓了模型更新对于训练速度的影响。</p>
<p>因此 Pipeline Parallel 是让模型可以达到千亿、集群可以扩充到千卡以上的一个最重要的特性。</p>
<p>然而流水并行有很重要的约束条件： **需要一个 规整对称的、线性顺序的网络结构。**GPT 就是这样一个典型的网络结构： 完全一样的 Transformer Layer 顺序堆叠，没有分叉和不对称情况，当均匀切分 Layer 时，各个 Stage 的前向/反向计算时间均一致。</p>
<p><a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=%E6%B5%81%E6%B0%B4%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83&amp;zhida_source=entity">流水并行训练</a>时的 time line 参考如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174108366.png" alt="image.png" /></p>
<p>流水并行实际的 time line 参考如下 （反向的计算时间是前向的两倍）</p>
<p>整个集群最高效的训练时间段是 step 4、5、6、7 的前向 和 step 0、1、2、3 的反向同时在所有 stage 上并行计算的时候，这个时候集群没有空闲，全部都在并行执行。 当我们增加 acc step （比如从 8 增加到 64）时，中间部分完美并行的时间段占比就会更长， bubble time 的占比就会越来越小。</p>
<p><strong>下面仅讨论两种架构的代表模型： Encoder-Decoder 架构的 T5 和 Decoder-Only 的 GPT。</strong></p>
<p>T5 的网络结构比 GPT 要复杂很多， T5 是 Encoder-Decoder 架构，整个网络分为两大块，且 Encoder 和 Decoder 的 Transformer Layer 参数大小、<a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Attention+%E8%AE%A1%E7%AE%97%E9%87%8F&amp;zhida_source=entity">Attention 计算量</a>、Context Length 等均不一致，导致 Encoder 的理论计算量要比 Decoder 大很多（<strong>整个网络不是均匀对称的</strong>）。 更要命的是， T5 Encoder 的输出要发给每个 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=Decoder+Layer&amp;zhida_source=entity">Decoder Layer</a>，网络结构不是线性而是有大量的分叉，前向反向之间包含了复杂的<a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=%E6%95%B0%E6%8D%AE%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB&amp;zhida_source=entity">数据依赖关系</a>， 会**导致流水并行中，各个 Stage 之间会产生大量的、非对称的、间隔跨多个 Stage 的数据依赖，**更加剧了流水并行的 load balance 问题。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250124174114182.png" alt="image.png" /></p>
<p>T5 的网络结构， Encoder 的输出会发送给多个 Decoder 的输入</p>
<p>所以你如果直接使用 Megatron 跑 T5 的 Pipeline Parallelism，会从 <a href="https://zhida.zhihu.com/search?content_id=652273093&amp;content_type=Answer&amp;match_order=1&amp;q=nsys+prof&amp;zhida_source=entity">nsys prof</a> 时间线上看到大量的缝隙，各个 Stage 之间在互相等待，无法真正流水并行起来。</p>
<blockquote>
<p>Megatron 的 T5 PP 并不是一个最佳实践， 所以理论上可以开发一个针对于 T5 的 PP 版本，通过比较复杂的工程实践优化重叠效果来提升性能。 但即使可以开发出来，也肯定比 GPT 的训练效率低。</p>
</blockquote>
<p>如果我们不用 Pipeline Parallelism 来训练 T5，那么只能借助： DP、TP 和 ZeRO 来进行并行优化了， 这就约束了 T5 的所有 Layer 都必须放在每一个 GPU 上，这种方式在 13B 量级的模型上是 OK 的，但是再往上扩展到 100B、1T 量级就不 work 了。同时由于 TP 只能开到 8 （跨机器也会慢几倍）， 在千卡 GPU 集群以上，大量的 DP 带来的通信变慢的影响也很严重（ZeRO-2/3 会大幅加剧这种通信开销）。 所以我们才说， 虽然 T5 的理论计算量相较于 GPT 没有增加很多，但是在千亿参数、千卡集群以上规模的时候，T5 的实际训练效率比 GPT 慢很多倍。</p>
<p>即使到现在，也没有一个超过 11B 的 T5 模型发布， 而 <strong>11B 恰好是一个不借助 PP，仅通过 ZeRO + TP 就可以训练的模型大小</strong>，避免了 T5 的模型结构非对称性对于 PP 的灾难性影响。</p>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/text-to-text-transfer-transformer%3Ftab%3Dreadme-ov-file%23released-model-checkpoints">https://github.com/google-research/text-to-text-transfer-transformer?tab=readme-ov-file#released-model-checkpoints</a></p>
<h2 id="总结-2"><a class="markdownIt-Anchor" href="#总结-2"></a> 总结</h2>
<p>T5 Scale up 到 100B、500B 的难度很大，训练成本的增加远远高于 GPT。 因此也许 100B 的 T5 训练 10T tokens 的模型能力比 100B 的 GPT 更强，但为此要支付的算力/时间成本远大于 100B GPT 训练 10T tokens，以至于：</p>
<ul>
<li>
<p>没有公司愿意支付这样的代价</p>
</li>
<li>
<p>我还不如支付相同的代价，让 GPT 多训练更多倍的 Tokens；或者训练一个参数量大很多的 GPT</p>
</li>
</ul>
<p>在 Scaling Laws 还没有看到失效的迹象的今天，我们的 LLM 算法演化一定要考虑模型结构的调整对于实际训练效率的影响，否则可能一切花里胡哨的 idea 都只是“玩具”，不值得企业为其付出上千万美金的训练成本。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>参数传递</title>
    <url>/2024/12/16/%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92/</url>
    <content><![CDATA[<h1 id="参数"><a class="markdownIt-Anchor" href="#参数"></a> 参数</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214303103.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="普通参数-可以为nullptr或者可被修改"><a class="markdownIt-Anchor" href="#普通参数-可以为nullptr或者可被修改"></a> 普通参数-可以为nullptr，或者可被修改</h2>
<p>class Widget { … };</p>
<ol>
<li>可以为nullptr,只读参数<br />
void foo (const Widget* const widget);</li>
<li>可以为nullptr,可被修改的参数<br />
void foo(Widget* const widget);</li>
<li>不为nullptr,可被修改的参数<br />
void foo (Widget&amp; widget);<br />
或者void foo(Widget* const widget);如果编程规范有统一要求</li>
</ol>
<h2 id="普通参数-不为-nullptr不能修改不能-move"><a class="markdownIt-Anchor" href="#普通参数-不为-nullptr不能修改不能-move"></a> 普通参数-不为 nullptr，不能修改，不能 move</h2>
<p>class Widget { … };</p>
<ol>
<li>不能修改的只读参数<br />
void foo (const Widget&amp; widget) {<br />
std:cout &lt; widget.x &lt;&lt; std:endl;<br />
}</li>
</ol>
<h2 id="普通参数-不为-nullptr不能修改可以-move"><a class="markdownIt-Anchor" href="#普通参数-不为-nullptr不能修改可以-move"></a> 普通参数-不为 nullptr，不能修改，可以 move</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214314697.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214330669.png" alt="image.png" /></p>
<h1 id="unique_ptrt"><a class="markdownIt-Anchor" href="#unique_ptrt"></a> unique_ptr&lt;T&gt;</h1>
<p>把ownership传递给 foo<br />
<code>void foo (std:unique_ptr\&lt;T&gt; object);</code><br />
<strong>例子：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">auto object = std: make_unique\&lt;std:string&gt;();</span><br><span class="line">foo(std:move(object);</span><br><span class="line">assert(object == nullptr);</span><br></pre></td></tr></table></figure>
<h2 id="unique_ptrt-转换为普通参数"><a class="markdownIt-Anchor" href="#unique_ptrt-转换为普通参数"></a> unique_ptr&lt;T&gt; 转换为普通参数</h2>
<p>如果不需要传递ownership,应该按照普通数据结构来传递，不要<br />
直接传递unique_ptr。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. auto object = std: make_unique&lt;std: string&gt;();</span><br><span class="line">2. foo (object.get(); //foo (const std:string* const str);</span><br><span class="line">3.                    //foo(std:string* const str);</span><br><span class="line">4.foo(*object);       //foo (const std:string&amp; str);</span><br><span class="line">5.                    //foo (std: string&amp; str)</span><br></pre></td></tr></table></figure>
<h2 id="什么时候需要传递stdunique_ptrt"><a class="markdownIt-Anchor" href="#什么时候需要传递stdunique_ptrt"></a> 什么时候需要传递std:unique_ptr&lt;T&gt;&amp;</h2>
<p>用于factory类型的场合，foo返回一个unique_ptr给调用者。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo (std:unqieu_ptr&lt;std:string&gt;&amp; str) &#123;</span><br><span class="line">2. 	str = std::make_unique&lt;std:string&gt;();</span><br><span class="line">3. &#125;</span><br></pre></td></tr></table></figure>
<h1 id="shared_ptrt"><a class="markdownIt-Anchor" href="#shared_ptrt"></a> shared_ptr&lt;T&gt;</h1>
<p>把ownership分享给 foo<br />
<code>void foo (std:shared_ptr&lt;T&gt; object);</code><br />
<strong>例子：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">auto object = std:make_shared&lt;std:string&gt;();</span><br><span class="line">foo (object);</span><br><span class="line">assert(object != nullptr);</span><br></pre></td></tr></table></figure>
<h2 id="shared_ptrt-转换为普通参数"><a class="markdownIt-Anchor" href="#shared_ptrt-转换为普通参数"></a> shared_ptr&lt;T&gt; 转换为普通参数</h2>
<p>如果不需要分享ownership,应该按照普通数据结构来传递，不要直接传递shared_ptr(直接传递shared_ptr涉及引用计数的增减，是很慢的操作)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. auto object = std: make_shared&lt;std:string&gt;();</span><br><span class="line">2. foo(object.get(); //foo (const std:string* const str);</span><br><span class="line">3.                   //foo(std:string* const str);</span><br><span class="line">4.foo(*object);      //foo(const std:string&amp; str);</span><br><span class="line">5.                   //foo(std:string&amp; str)</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214348374.png" alt="image.png" /></p>
<h2 id="什么时候传递-const-stdshared_ptrt"><a class="markdownIt-Anchor" href="#什么时候传递-const-stdshared_ptrt"></a> 什么时候传递 const std:shared_ptr&lt;T&gt;&amp;</h2>
<p>很少见。需要分享ownership,但是又不能 move。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo (const std:shared_ptr&lt;std:string&gt;&amp; str)&#123;</span><br><span class="line">2.   bar(str);</span><br><span class="line">3.   std: cout &lt;&lt;*str &lt;&lt; std:endl;</span><br><span class="line">4. &#125;</span><br><span class="line">5. void bar (std: shared_ptr&lt;std:string&gt; str)&#123;</span><br><span class="line">6.   _str = std: move(str);</span><br><span class="line">7. &#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214357476.png" alt="image.png" /></p>
<h1 id="lambda"><a class="markdownIt-Anchor" href="#lambda"></a> Lambda</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214417945.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-stdfunction"><a class="markdownIt-Anchor" href="#pass-lambda-by-stdfunction"></a> Pass Lambda By std::function</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo() &#123;</span><br><span class="line">2.   int x= 0;</span><br><span class="line">3.   auto func = [=](int i)-&gt;bool &#123; return i&gt; x; &#125;;</span><br><span class="line">4.   bar (func);</span><br><span class="line">5. &#125;</span><br><span class="line">6. void bar (std: function&lt;bool(int number)&gt; func) &#123;</span><br><span class="line">7.   bool result = func(100);</span><br><span class="line">8. &#125;</span><br></pre></td></tr></table></figure>
<p><strong>std::function 是什么？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. class function &#123;</span><br><span class="line">2.   lambda_xyz* lambda_ptr;</span><br><span class="line">3.   ...</span><br><span class="line">4. &#125;;</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214430960.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214438981.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-stdfunction-reference-side-effect"><a class="markdownIt-Anchor" href="#pass-lambda-by-stdfunction-reference-side-effect"></a> Pass Lambda By std::function Reference – Side Effect</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214448012.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-stdfunction-优缺点"><a class="markdownIt-Anchor" href="#pass-lambda-by-stdfunction-优缺点"></a> Pass Lambda By std::function 优缺点</h2>
<p>好处：明确的参数类型和返回值类型（编译器强制检查）<br />
坏处：有一次内存分配的开销（对特别小的lambda可以优化掉）<br />
建议：在性能要求不高的场合使用</p>
<h2 id="pass-lambda-by-template"><a class="markdownIt-Anchor" href="#pass-lambda-by-template"></a> Pass Lambda By Template</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214508109.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-perfect-forwarding"><a class="markdownIt-Anchor" href="#pass-lambda-by-perfect-forwarding"></a> Pass Lambda By Perfect Forwarding</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214533786.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214543503.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-perfect-forwarding-side-effect"><a class="markdownIt-Anchor" href="#pass-lambda-by-perfect-forwarding-side-effect"></a> Pass Lambda By Perfect Forwarding – Side Effect</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214551580.png" alt="image.png" /></p>
<h2 id="pass-lambda-by-perfect-forwarding-优缺点"><a class="markdownIt-Anchor" href="#pass-lambda-by-perfect-forwarding-优缺点"></a> Pass Lambda By Perfect Forwarding  优缺点</h2>
<p>好处：没有内存分配的开销，也没有lambda 拷贝。<br />
坏处：另外失去了强制类型检查，还有副作用。</p>
<h2 id="参考标准库"><a class="markdownIt-Anchor" href="#参考标准库"></a> 参考标准库</h2>
<p>Pass lambda by template<br />
1. template &lt;…, class Function&gt;<br />
2. void for_each(…, Function f)<br />
Pass lambda by perfect forwarding<br />
1. template &lt;., class URBG&gt;<br />
2. void shuffle(…, URBG&amp;&amp; g);</p>
<h1 id="pass-arguments-to-thread"><a class="markdownIt-Anchor" href="#pass-arguments-to-thread"></a> Pass Arguments to Thread</h1>
<h2 id="先拷贝再move"><a class="markdownIt-Anchor" href="#先拷贝再move"></a> 先拷贝再move</h2>
<p>1. void func (int i, const std: string&amp; s);<br />
2. std:thread t (func, 3, “hello”);<br />
编译器产生的代码是这样的：</p>
<ol>
<li>thread的构造函数把参数3和“hello”拷贝到一个安全的地方。这里的<br />
重点是“拷贝”。&quot;hello”被当作const char * 拷贝。</li>
<li>创建线程。</li>
<li>新线程调用func,并把刚才拷贝的参数，move给func。这里的重点是“move”。&quot;hello&quot;以 const char * 的类型 move给 “const std:string&amp; s”, 这是可以的。</li>
</ol>
<h2 id="先拷贝再move的陷阱1"><a class="markdownIt-Anchor" href="#先拷贝再move的陷阱1"></a> 先拷贝再move的陷阱1</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214605516.png" alt="image.png" /></p>
<h2 id="陷阱1的解决方法"><a class="markdownIt-Anchor" href="#陷阱1的解决方法"></a> 陷阱1的解决方法</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214614141.png" alt="image.png" /></p>
<h2 id="先拷贝再move的陷阱2"><a class="markdownIt-Anchor" href="#先拷贝再move的陷阱2"></a> 先拷贝再move的陷阱2</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214623154.png" alt="image.png" /></p>
<h2 id="陷阱2的解决方法"><a class="markdownIt-Anchor" href="#陷阱2的解决方法"></a> 陷阱2的解决方法</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214633346.png" alt="image.png" /></p>
<h2 id="传递类的成员函数给thread"><a class="markdownIt-Anchor" href="#传递类的成员函数给thread"></a> 传递类的成员函数给thread</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. class Widget &#123;</span><br><span class="line">2. public:</span><br><span class="line">3.   void do_work(int number) &#123;. &#125;</span><br><span class="line">4. &#125;;</span><br><span class="line">5. Widget w;</span><br><span class="line">6. std:thread t(&amp;Widget:do_work, &amp;w, 100);</span><br></pre></td></tr></table></figure>
<h1 id="initializer_list"><a class="markdownIt-Anchor" href="#initializer_list"></a> initializer_list</h1>
<p>foo (std: initializer_list&lt;int&gt; numbers);</p>
<p>调用例子：<br />
foo ({1, 2, 3});</p>
<p>initializer_list不需要 const initializer_list&lt;T&gt;&amp;。<br />
不需要引用：initializer_list很小，传引用和传值代价相当。<br />
不需要const：initializer_list默认是const。</p>
<h2 id="什么是initializer_listint"><a class="markdownIt-Anchor" href="#什么是initializer_listint"></a> 什么是initializer_list&lt;int&gt;</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214642741.png" alt="image.png" /></p>
<h1 id="move-only-objects"><a class="markdownIt-Anchor" href="#move-only-objects"></a> move-only objects</h1>
<ul>
<li>std::future</li>
<li>std::thread</li>
<li>std::unique_ptr</li>
</ul>
<h1 id="small-object-that-can-pass-by-value"><a class="markdownIt-Anchor" href="#small-object-that-can-pass-by-value"></a> small object that can pass by value</h1>
<ul>
<li>std::initializer_list</li>
</ul>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>右值引用</title>
    <url>/2024/12/16/%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8/</url>
    <content><![CDATA[<h1 id="基础概念"><a class="markdownIt-Anchor" href="#基础概念"></a> 基础概念</h1>
<h2 id="表达式expression"><a class="markdownIt-Anchor" href="#表达式expression"></a> 表达式（Expression）</h2>
<p>C++表达式是运算符和操作数的组合。运算符包括赋值、数逻比较和函数等，操作数包括变量和literal等。例子：<br />
1. <code>foo(x + y)</code><br />
2. <code>1 + b * foo()</code><br />
3. <code>a = 1 + b * foo()</code><br />
4. <code>a == b &amp;&amp; a &lt; c</code><br />
5. <code>((i &lt; 3) ? i : j)=7</code></p>
<span id="more"></span>
<h2 id="中间结果"><a class="markdownIt-Anchor" href="#中间结果"></a> 中间结果</h2>
<p>表达式可以分解为子表达式。比如：“foo()”是“b * foo()”的子表达式，“b * foo()”是“1+b * foo()”的子表达式。每个表达式和子表达式都产生一个结果。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213404989.png" alt="image.png" /></p>
<h2 id="函数非引用返回值"><a class="markdownIt-Anchor" href="#函数非引用返回值"></a> 函数非引用返回值</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213425072.png" alt="image.png" /></p>
<h2 id="无名的临时变量"><a class="markdownIt-Anchor" href="#无名的临时变量"></a> 无名的临时变量</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213434290.png" alt="image.png" /></p>
<h2 id="copy-vs-move"><a class="markdownIt-Anchor" href="#copy-vs-move"></a> Copy v.s. Move</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213455621.png" alt="image.png" /></p>
<h2 id="利用move实现编译自动优化"><a class="markdownIt-Anchor" href="#利用move实现编译自动优化"></a> 利用move实现编译自动优化</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213506139.png" alt="image.png" /></p>
<h2 id="右值引用rvalue-reference"><a class="markdownIt-Anchor" href="#右值引用rvalue-reference"></a> 右值引用（Rvalue Reference）</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213516271.png" alt="image.png" /></p>
<h2 id="值类别value-category"><a class="markdownIt-Anchor" href="#值类别value-category"></a> 值类别（Value Category）</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213523593.png" alt="image.png" /></p>
<h3 id="lvalue"><a class="markdownIt-Anchor" href="#lvalue"></a> lvalue</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213533694.png" alt="image.png" /></p>
<h3 id="prvalue"><a class="markdownIt-Anchor" href="#prvalue"></a> prvalue</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213540915.png" alt="image.png" /></p>
<h3 id="lvalue-和-prvalue-的例子"><a class="markdownIt-Anchor" href="#lvalue-和-prvalue-的例子"></a> lvalue 和 prvalue 的例子</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213551360.png" alt="image.png" /></p>
<h3 id="xvalue"><a class="markdownIt-Anchor" href="#xvalue"></a> xvalue</h3>
<p>Xvalue是接近生命周期末尾的lvalue,尽管它在内存中，可以被访问，但是程序员主动放弃了对它的访问权。程序员需要显式地进行强制转换std:move(x),告诉编译器x不再访问。std:move(x)表达式的结果是一个xvalue。</p>
<h1 id="stdmove"><a class="markdownIt-Anchor" href="#stdmove"></a> std::move</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213600749.png" alt="image.png" /></p>
<h2 id="实现move操作的完整例子"><a class="markdownIt-Anchor" href="#实现move操作的完整例子"></a> 实现move操作的完整例子</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213609211.png" alt="image.png" /></p>
<p><strong>改进后的例子</strong><br />
初始化列表是在构造函数的冒号后面使用的，用于直接初始化类的成员变量。在构造函数体内赋值则是首先调用默认构造函数或默认初始化后，再进行赋值操作。这种方式避免了额外的赋值操作，特别是对于类类型的成员变量或容器类型时，初始化列表可以提升性能。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213616937.png" alt="image.png" /></p>
<h2 id="move操作常见错误一忘了继续-move"><a class="markdownIt-Anchor" href="#move操作常见错误一忘了继续-move"></a> Move操作常见错误一：忘了继续 move</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213624252.png" alt="image.png" /></p>
<h2 id="move操作常见错误二-move-const-object"><a class="markdownIt-Anchor" href="#move操作常见错误二-move-const-object"></a> Move操作常见错误二： move const object</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213634397.png" alt="image.png" /></p>
<h2 id="move-操作常见错误三-move-from-object-invalid"><a class="markdownIt-Anchor" href="#move-操作常见错误三-move-from-object-invalid"></a> Move 操作常见错误三： move-from object invalid</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213644265.png" alt="image.png" /></p>
<h2 id="move-构造函数的两段式写法"><a class="markdownIt-Anchor" href="#move-构造函数的两段式写法"></a> Move 构造函数的两段式写法</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213651652.png" alt="image.png" /></p>
<h2 id="move-赋值运算符的四段式写法"><a class="markdownIt-Anchor" href="#move-赋值运算符的四段式写法"></a> Move 赋值运算符的四段式写法</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213701677.png" alt="image.png" /></p>
<h2 id="move-操作常见错误四move-同类型的本地返回值"><a class="markdownIt-Anchor" href="#move-操作常见错误四move-同类型的本地返回值"></a> Move 操作常见错误四：move 同类型的本地返回值</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213710567.png" alt="image.png" /></p>
<h2 id="move操作常见错误五忘了-move不同类型的本地返回值"><a class="markdownIt-Anchor" href="#move操作常见错误五忘了-move不同类型的本地返回值"></a> Move操作常见错误五：忘了 move不同类型的本地返回值</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213722724.png" alt="image.png" /></p>
<h2 id="move-操作带来的问题"><a class="markdownIt-Anchor" href="#move-操作带来的问题"></a> Move 操作带来的问题</h2>
<p>假如foo函数想接收一个string类型的参数，它得写两个函数：</p>
<ol>
<li>void foo(const string&amp; str);</li>
<li>void foo(string&amp;&amp; str);<br />
假如foo函数想接收两个string类型的参数，它得写四个函数：</li>
<li>void foo(const string&amp; str1, const string&amp; str2);</li>
<li>void foo(const string&amp; str1, string&amp;&amp; str2);</li>
<li>void foo(string&amp;&amp; str1, const string&amp; str2);</li>
<li>void foo(string&amp;&amp; str1, string&amp;&amp; str2);</li>
</ol>
<h1 id="完美转发perfect-forwarding和转发引用forward-reference"><a class="markdownIt-Anchor" href="#完美转发perfect-forwarding和转发引用forward-reference"></a> 完美转发（Perfect Forwarding）和转发引用（Forward Reference）</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213732468.png" alt="image.png" /></p>
<h2 id="引用折叠reference-collapsing-lvalue"><a class="markdownIt-Anchor" href="#引用折叠reference-collapsing-lvalue"></a> 引用折叠（Reference Collapsing）-- lvalue</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213739335.png" alt="image.png" /></p>
<h2 id="引用折叠reference-collapsing-rvalue"><a class="markdownIt-Anchor" href="#引用折叠reference-collapsing-rvalue"></a> 引用折叠（Reference Collapsing）-- rvalue</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213749826.png" alt="image.png" /></p>
<h2 id="转发引用的关键特征有推理格式对"><a class="markdownIt-Anchor" href="#转发引用的关键特征有推理格式对"></a> 转发引用的关键特征：有推理+格式对</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213800564.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213808575.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213820439.png" alt="image.png" /></p>
<h2 id="完美转发常见问题一把转发引用当作右值引用"><a class="markdownIt-Anchor" href="#完美转发常见问题一把转发引用当作右值引用"></a> 完美转发常见问题一：把转发引用当作右值引用</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213832161.png" alt="image.png" /></p>
<h2 id="完美转发常见问题二把右值引用当作转发引用"><a class="markdownIt-Anchor" href="#完美转发常见问题二把右值引用当作转发引用"></a> 完美转发常见问题二：把右值引用当作转发引用</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213839849.png" alt="image.png" /></p>
<h2 id="完美转发常见问题三同一变量转发多次"><a class="markdownIt-Anchor" href="#完美转发常见问题三同一变量转发多次"></a> 完美转发常见问题三：同一变量转发多次</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213848714.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213855500.png" alt="image.png" /></p>
<h2 id="完美转发常见问题四不要重载完美转发"><a class="markdownIt-Anchor" href="#完美转发常见问题四不要重载完美转发"></a> 完美转发常见问题四：不要重载完美转发</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213903165.png" alt="image.png" /></p>
<h2 id="什么时候用noexcept"><a class="markdownIt-Anchor" href="#什么时候用noexcept"></a> 什么时候用noexcept</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213915030.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>多处理器编程中的一致性问题</title>
    <url>/2024/12/23/%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/reliveIT/article/details/105902477?spm=1001.2014.3001.5501">Why Memory Barriers？中文翻译（上）</a><br />
<a href="https://zhuanlan.zhihu.com/p/48157076">高并发编程–多处理器编程中的一致性问题(上)</a><br />
<a href="https://zhuanlan.zhihu.com/p/48161056">高并发编程–多处理器编程中的一致性问题(下)</a><br />
<a href="https://luyuhuang.tech/2022/06/25/cpp-memory-order.html">谈谈 C++ 中的内存顺序 (Memory Order)</a><br />
<a href="https://www.zhihu.com/column/c_1634220655800573952">C++ 相关知识总结分享</a></p>
<h1 id="memory-barrier"><a class="markdownIt-Anchor" href="#memory-barrier"></a> Memory Barrier</h1>
<h2 id="为什么需要内存屏障"><a class="markdownIt-Anchor" href="#为什么需要内存屏障"></a> 为什么需要内存屏障？</h2>
<p>由于CPU的速度要快于（数量级上的差异）memory以及他们之间的互连器件（interconnect），因此引入了 Store Buffer 和 Invalidate Queues， 可能会导致多线程下 Inconsistency 的情况</p>
<h2 id="cache"><a class="markdownIt-Anchor" href="#cache"></a> Cache</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113405743.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="cache-coherency-protocols-msei"><a class="markdownIt-Anchor" href="#cache-coherency-protocols-msei"></a> Cache Coherency Protocols - MSEI</h2>
<h3 id="为什么需要缓存一致性协议"><a class="markdownIt-Anchor" href="#为什么需要缓存一致性协议"></a> 为什么需要缓存一致性协议？</h3>
<p>cache独占导致的数据不一致：因为每个core都有自己独立的L1 cache，对于一个共享的memory location两个cache可以有自己的copy。那么这就会出现了数据不一致的状况，两个core可能同时访问这个memory location，且如果一个core是对这个memory location进行修改，那么这就需要两边的cache进行同步，防止数据不一致。这个工作就是cache coherence protocol要做的事情。</p>
<h3 id="cacheline-state"><a class="markdownIt-Anchor" href="#cacheline-state"></a> <strong>Cacheline State</strong></h3>
<ul>
<li><strong>Modified(M):</strong><br />
当一个core 的cacheline的状态是M时，说明当前core最近修改了这个cache，那么其他core的cache不能再修改当前cache line对应的memory location，除非该cache将这个修改同步到了memory。这个core对这个memory location可以理解为Owned。</li>
<li><strong>Exclusive(E):</strong><br />
E这个状态与M很像，区别在于当前core并没有修改当前的cacheline，这意味着当前cacheline存储的memory location的值是最新的。当前core可以对该cacheline进行modify且不需要与其他core的cache同步。这个core对这个memory location可以理解为Owned。</li>
<li><strong>Share(S):</strong><br />
S表示当前cacheline在其他core的cache也存在copy，当前core如果需要修改该cacheline则需要与其他core的cache进行提前沟通。</li>
<li><strong>Invalid(I):</strong><br />
I表示当前cacheline是空的。</li>
</ul>
<h3 id="protocol-message"><a class="markdownIt-Anchor" href="#protocol-message"></a> Protocol Message</h3>
<ul>
<li><strong>Read</strong><br />
当一个cache需要读取某个cacheline消息的时候就会发起read消息。</li>
<li><strong>Read Response</strong><br />
read response是read的回应，这response可以来自其他core的cache也可以来自memory。当其他core中对当前cacheline是M状态时，则会发起read response。</li>
<li><strong>Invalidate</strong><br />
Invalidate消息包含对应的memory location，接收到这个消息的cache需要将自己cacheline内容剔除，并响应。</li>
<li><strong>Invalidate Acknowledge</strong><br />
接收到Invalidate后删除cacheline中的数据就向发起者回复invalidate ack。</li>
<li><strong>Read Invalidate</strong><br />
这个消息包含两个操作，read和invalidate，那么它也需要接收read response和多个invalidate ack响应。</li>
<li><strong>Write Back</strong><br />
writeback包含数据和地址，会将这个地址对应的数据刷到内存中。</li>
</ul>
<h3 id="state-machine"><a class="markdownIt-Anchor" href="#state-machine"></a> State Machine</h3>
<ul>
<li><strong>Modified(M) -&gt; Exclusive(E)</strong>：cache可以通过writeback transaction将一个cacheline的数据写回到memory中（或者下一级cache中），这时候，该cacheline的状态从Modified迁移到Exclusive状态。对于cpu而言，cacheline中的数据仍然是最新的，而且是该cpu独占的，因此可以不通知其他cpu cache而直接修改之。</li>
<li><strong>Exclusive(E) -&gt; Modified(M)</strong>：在Exclusive状态下，cpu可以直接将数据写入cacheline，不需要其他操作。相应的，该cacheline状态从Exclusive状态迁移到Modified状态。这个状态迁移过程不涉及bus上的Transaction（即无需MESI Protocol Messages的交互）。</li>
<li><strong>Modified(M) -&gt; Invalid(I)</strong>：CPU 在总线上收到一个read invalidate的请求，同时，该请求是针对一个处于modified状态的cacheline，在这种情况下，CPU必须该cacheline状态设置为无效，并且用read response”和“invalidate acknowledge来回应收到的read invalidate的请求，完成整个bus transaction。一旦完成这个transaction，数据被送往其他cpu cache中，本地的copy已经不存在了。</li>
<li><strong>Invalid(I) -&gt; Modified(M)</strong>：CPU需要执行一个原子的readmodify-write操作，并且其cache中没有缓存数据，这时候，CPU就会在总线上发送一个read invalidate用来请求数据，同时想独自霸占对该数据的所有权。该CPU的cache可以通过read response获取数据并加载cacheline，同时，为了确保其独占的权利，必须收集所有其他cpu发来的invalidate acknowledge之后（其他cpu没有local copy），完成整个bus transaction。</li>
<li><strong>Share(S) -&gt; Modified(M)</strong>：CPU需要执行一个原子的readmodify-write操作，并且其local cache中有read only的缓存数据（cacheline处于shared状态），这时候，CPU就会在总线上发送一个invalidate请求其他cpu清空自己的local copy，以便完成其独自霸占对该数据的所有权的梦想。同样的，该cpu必须收集所有其他cpu发来的invalidate acknowledge之后，才算完成整个bus transaction。</li>
<li><strong>Modified(M) -&gt; Share(S)</strong>：在本cpu独自享受独占数据的时候，其他的cpu发起read请求，希望获取数据，这时候，本cpu必须以其local cacheline的数据回应，并以read response回应之前总线上的read请求。这时候，本cpu失去了独占权，该cacheline状态从Modified状态变成shared状态（有可能也会进行写回的动作）。</li>
<li><strong>Exclusive(E) -&gt; Share(S)</strong>：这个迁移和f类似，只不过开始cacheline的状态是exclusive，cacheline和memory的数据都是最新的，不存在写回的问题。总线上的操作也是在收到read请求之后，以read response回应。</li>
<li><strong>Share(S) -&gt; Exclusive(E)</strong>：如果cpu认为自己很快就会启动对处于shared状态的cacheline进行write操作，因此想提前先霸占上该数据。因此，该cpu会发送invalidate敦促其他cpu清空自己的local copy，当收到全部其他cpu的invalidate acknowledge之后，transaction完成，本cpu上对应的cacheline从shared状态切换exclusive状态。还有另外一种方法也可以完成这个状态切换：当所有其他的cpu对其local copy的cacheline进行写回操作，同时将cacheline中的数据设为无效（主要是为了为新的数据腾些地方），这时候，本cpu坐享其成，直接获得了对该数据的独占权。</li>
<li><strong>Exclusive(E) -&gt; Invalid(I)</strong>：其他的CPU进行一个原子的read-modify-write操作，但是，数据在本cpu的cacheline中，因此，其他的那个CPU会发送read invalidate，请求对该数据以及独占权。本cpu回送read response”和“invalidate acknowledge”，一方面把数据转移到其他cpu的cache中，另外一方面，清空自己的cacheline。</li>
<li><strong>Invalid(I) -&gt; Exclusive(E)</strong>：cpu想要进行write的操作但是数据不在local cache中，因此，该cpu首先发送了read invalidate启动了一次总线transaction。在收到read response回应拿到数据，并且收集所有其他cpu发来的invalidate acknowledge之后（确保其他cpu没有local copy），完成整个bus transaction。当write操作完成之后，该cacheline的状态会从Exclusive状态迁移到Modified状态。</li>
<li><strong>Invalid(I) -&gt; Share(S)</strong>：本CPU执行读操作，发现local cache没有数据，因此通过read发起一次bus transaction，来自其他的cpu local cache或者memory会通过read response回应，从而将该cacheline从Invalid状态迁移到shared状态。</li>
<li><strong>Share(S) -&gt; Invalid(I)</strong>：当cacheline处于shared状态的时候，说明在多个cpu的local cache中存在副本，因此，这些cacheline中的数据都是read only的，一旦其中一个cpu想要执行数据写入的动作，必须先通过invalidate获取该数据的独占权，而其他的CPU会以invalidate acknowledge回应，清空数据并将其cacheline从shared状态修改成invalid状态。</li>
</ul>
<h3 id="mesi-protocol-example"><a class="markdownIt-Anchor" href="#mesi-protocol-example"></a> MESI Protocol Example</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113559619.png" alt="image.png" /></p>
<p>第一列是操作序列号，第二列是执行操作的CPU，第三列是具体执行哪一种操作，第四列描述了各个cpu local cache中的cacheline的状态（用meory address/状态表示），最后一列描述了内存在0地址和8地址的数据内容的状态：V表示是最新的，和cache一致，I表示不是最新的内容，最新的内容保存在cache中。<br />
<strong>sequence 0</strong>，各个cpu cache中的cacheline都是Invalid状态，而Memory中的数据都保存了最新的数据。<br />
<strong>sequence 1</strong>，CPU 0执行了load操作，将address 0的数据加载到寄存器，这个操作使得保存0地址数据的那个cacheline从invalid状态迁移到shared状态。<br />
<strong>sequence 2</strong>，CPU3也对0地址执行了load操作，导致其local cache上对应的cacheline也切换到shared状态。当然，这时候，memory仍然是最新的。<br />
<strong>sequence 3</strong>，CPU 0执行了对地址8的load操作，由于地址0和地址8都是选择同一个cache set，而且，我们之前已经说过，该cache是direct-mapped的（即每个set只有一个cacheline），因此需要首先清空该cacheline中的数据（该操作被称为Invalidation），由于cacheline的状态是shared，因此，不需要通知其他CPU。Invalidation local cache上的cacheline之后，cpu 0的load操作将该cacheline状态修改成Shared状态（保存地址8的数据）。<br />
<strong>sequence 4</strong>，CPU 2也开始执行load操作了，虽然是load操作，但是CPU知道程序随后会修改该值（不是原子操作的read-modify-write，否就是迁移到Modified状态了，也不是单纯的load操作，否则会迁移到shared状态），因此向总线发送了read invalidate命令，一方面获取该数据（自己的local cache中没有地址0的数据），另外，CPU 2想独占该数据（因为随后要write）。这个操作导致CPU 3的cacheline迁移到invalid状态。当然，这时候，memory仍然是最新的有效数据。<br />
<strong>Sequence 5</strong>，CPU 2的store操作很快到来，由于准备工作做的比较充分（Exclusive状态，独占该数据），cpu直接修改cacheline中的数据（对应地址0），从而将其状态迁移到modified状态，同时要注意的是：memory中的数据已经失效，不是最新的数据了，任何其他CPU发起对地址0的load操作都不能从memory中读取，而是通过嗅探（snoop）的方式从CPU 2的local cache中获取。<br />
<strong>sequence 6</strong>，CPU 1对地址0的数据执行原子的加1操作，这时候CPU 1会发出read invalidate命令，将地址0的数据从CPU 2的cacheline中嗅探得到，同时通过invalidate其他CPU local cache的内容而获得独占性的数据访问权。这时候，CPU 2中的cacheline状态变成invalid状态，而CPU 1将从invalid状态迁移到modified状态。<br />
<strong>sequence 7</strong>，CPU 1对地址8进行load操作，由于cacheline被地址0占据，因此需要首先将其驱逐出cache，于是执行write back操作将地址0的数据写回到memory，同时发送read命名，从CPU 0的cache中获得数据加载其cacheline，最后，CPU1的cache变成shared状态（保存地址8的数据）。由于执行了write back操作，memory中地址0的数据又变成最新的有效数据了。</p>
<h2 id="store-buffer"><a class="markdownIt-Anchor" href="#store-buffer"></a> Store Buffer</h2>
<h3 id="为什么要引入-store-buffer"><a class="markdownIt-Anchor" href="#为什么要引入-store-buffer"></a> 为什么要引入 store buffer?</h3>
<p>cpu 0发起一次对某个地址的写操作，但是local cache没有数据，该数据在CPU 1的local cache中，因此，为了完成写操作，CPU 0发出invalidate的命令，invalidate其他CPU的cache数据。只有完成了这些总线上的transaction之后，CPU 0才能正在发起写的操作，这是一个漫长的等待过程。但是，其实没必要等待这么长的时间，毕竟，物理CPU 1中的cacheline保存有什么样子的数据，其实都没有意义，这个值都会被CPU 0新写入的值覆盖的。</p>
<h3 id="什么是-store-buffer"><a class="markdownIt-Anchor" href="#什么是-store-buffer"></a> <strong>什么是 store buffer</strong>?</h3>
<p>有一种可以阻止cpu进入无聊等待状态的方法就是在CPU和cache之间增加store buffer这个HW block，如下图所示：<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113619007.png" alt="image.png" /></p>
<p>一旦增加了store buffer，那么cpu0无需等待其他CPU的相应，只需要将要修改的内容放入store buffer，然后继续执行就OK了。当cacheline完成了bus transaction，并更新了cacheline的状态后，要修改的数据将从store buffer进入cacheline。这些store buffer对于cpu而言是local的</p>
<h3 id="会引入什么新的问题"><a class="markdownIt-Anchor" href="#会引入什么新的问题"></a> <strong>会引入什么新的问题？</strong></h3>
<ul>
<li>
<p><strong>存在多个数据副本（store buffer一份，CPU cache一份）- store forwarding 解决</strong><br />
当CPU执行load操作的时候，不但要看cache，还有看store buffer是否有内容，如果store buffer有该数据，那么就采用store buffer中的值。因此，即便是store操作还没有写入cacheline，store forwarding的效果看起来就好象cpu的store操作被向前传递了一样（后面的load的指令可以感知到这个store操作） 。</p>
</li>
<li>
<p><strong>导致存储系统重排序</strong>- <strong>memory barriers解决</strong><br />
因为现代计算的可见性是通过锁cache+cache coherency protocol缓存一致性协议来解决的，引入了store buffer，program order代码顺序上先写的因为需要bus transaction，所以写到store buffer，从而导致后写的先写入cache可见，这就是存储系统重排序导致的可见性问题。</p>
<p>看这个例子：</p>
  <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>(); <span class="comment">// MODIFIED: memory barrier</span></span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们假设CPU 0执行foo函数，CPU 1执行bar函数。我们再进一步假设a变量在CPU 1的cache中，b在CPU 0 cache中，执行的操作序列如下</p>
<ol>
<li>CPU 0执行a=1的赋值操作，由于a不在local cache中，因此，CPU 0将a值放到store buffer中之后，发送了read invalidate命令到总线上去。</li>
<li>CPU 1执行 while (b == 0) 循环，由于b不在CPU 1的cache中，因此，CPU发送一个read message到总线上，看看是否可以从其他cpu的local cache中或者memory中获取数据</li>
<li>CPU 0继续执行b=1的赋值语句，由于b就在自己的local cache中（cacheline处于modified状态或者exclusive状态），因此CPU0可以直接操作将新的值1写入cache line。</li>
<li>CPU 0收到了read message，将最新的b值”1“回送给CPU 1，同时将b cacheline的状态设定为shared</li>
<li>CPU 1收到了来自CPU 0的read response消息，将b变量的最新值”1“值写入自己的cacheline，状态修改为shared。</li>
<li>由于b值等于1了，因此CPU 1跳出while (b == 0)的循环，继续前行。</li>
<li>CPU 1执行assert(a == 1)，这时候CPU 1的local cache中还是旧的a值，因此assert(a == 1)失败。</li>
<li>CPU 1收到了来自CPU 0的read invalidate消息，以a变量的值进行回应，同时清空自己的cacheline，但是这已经太晚了。</li>
<li>CPU 0收到了read response和invalidate ack的消息之后，将store buffer中的a的最新值”1“数据写入cacheline，然并卵，CPU 1已经assertion fail了。<br />
但是在CPU设计层面是无法判断当前core中执行的变量是否与其他的core中的变量存在关系，因为CPU在执行代码的时候他认为这个当前所执行程序就是一个单线程的，他无法感知多线程的存在。因此这个问题无法在CPU设计层面解决，这个就需要编码人员介入了，编码人员需要告诉CPU现在需要将storebuffer flush到cache里，于是CPU设计者提供了叫memory barrier的工具。<br />
smp_mb()会在执行的时候将storebuffer中的数据全部刷进cache。这样assert就会执行成功了。</li>
</ol>
</li>
</ul>
<h2 id="invalidate-queues"><a class="markdownIt-Anchor" href="#invalidate-queues"></a> Invalidate Queues</h2>
<h3 id="为什么需要-invalidate-queues"><a class="markdownIt-Anchor" href="#为什么需要-invalidate-queues"></a> <strong>为什么需要 Invalidate Queues?</strong></h3>
<p>store buffer的容量是有限的，如果出现大量的bus transaction，把store buffer打满了，也会导致CPU流水线阻塞. 在这种状况下，CPU只能又进入等待状态，直到cache line完成invalidation和ack的交互之后，可以将store buffer的entry写入cacheline，从而为新的store让出空间之后，CPU才可以继续执行。这种状况也可能发生在调用了memory barrier指令之后，因为一旦store buffer中的某个entry被标记了，那么随后的store都必须等待invalidation完成，因此不管是否cache miss，这些store都必须进入store buffer。</p>
<p>引入invalidate queues可以缓解这个状况。store buffer之所以很容易被填充满，主要是其他CPU回应invalidate acknowledge比较慢，如果能够加快这个过程，让store buffer尽快进入cacheline，那么也就不会那么容易填满了。</p>
<p>CPU其实不需要完成invalidate操作就可以回送acknowledgement消息，这样，就不会阻止发生invalidate请求的那个CPU进入无聊的等待状态。CPU可以buffer这些invalidate message（放入Invalidate Queues），然后直接回应acknowledgement，表示自己已经收到请求，随后会慢慢处理。当然，再慢也要有一个度，例如对a变量cacheline的invalidate处理必须在该CPU发送任何关于a变量对应cacheline的操作到bus之前完成。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113629110.png" alt="image.png" /></p>
<h3 id="example-for-invalidate-queues"><a class="markdownIt-Anchor" href="#example-for-invalidate-queues"></a> <strong>Example for Invalidate Queues</strong></h3>
<p>之前的代码引入 Invalidate Queues 会带来新的问题</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>();</span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>(); <span class="comment">// MODIFIED: memory barrier</span></span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>CPU 0执行a=1的赋值操作，由于a在CPU 0 local cache中的cacheline处于shared状态，因此，CPU 0将a的新值“1”放入store buffer，并且发送了invalidate消息去清空CPU 1对应的cacheline。</li>
<li>CPU 1执行while (b == 0)的循环操作，但是b没有在local cache，因此发送read消息试图获取该值。</li>
<li>CPU 1收到了CPU 0的invalidate消息，放入Invalidate Queue，并立刻回送Ack。</li>
<li>CPU 0收到了CPU 1的invalidate ACK之后，即可以越过程序设定内存屏障（第四行代码的smp_mb() ），这样a的新值从store buffer进入cacheline，状态变成Modified。</li>
<li>CPU 0 越过memory barrier后继续执行b=1的赋值操作，由于b值在CPU 0的local cache中，因此store操作完成并进入cache line。</li>
<li>CPU 0收到了read消息后将b的最新值“1”回送给CPU 1，并修正该cacheline为shared状态。</li>
<li>CPU 1收到read response，将b的最新值“1”加载到local cacheline。</li>
<li>对于CPU 1而言，b已经等于1了，因此跳出while (b == 0)的循环，继续执行后续代码</li>
<li>CPU 1执行assert(a == 1)，但是由于这时候CPU 1 cache的a值仍然是旧值0，因此assertion 失败</li>
<li>该来总会来，Invalidate Queue中针对a cacheline的invalidate消息最终会被CPU 1执行，将a设定为无效，但素，大错已经酿成。<br />
很明显，在上文中的场景中，加速Invalidation response导致foo函数中的memory barrier失效了，因此，这时候对Invalidation response已经没有意义了，毕竟程序逻辑都错了。怎么办？其实我们可以让memory barrier指令和Invalidate Queue进行交互来保证确定的memory order。具体做法是这样的：当CPU执行memory barrier指令的时候，对当前Invalidate Queue中的所有的entry进行标注，这些被标注的项次被称为marked entries，而随后CPU执行的任何的load操作都需要等到Invalidate Queue中所有marked entries完成对cacheline的操作之后才能进行。因此，要想保证程序逻辑正确，我们需要给bar函数增加内存屏障的操作<br />
程序修改之后，我们再来看看CPU的执行序列：<br />
1 - 8 相同</li>
<li>CPU 1现在不能继续执行代码，只能等待，直到Invalidate Queue中的message被处理完成</li>
<li>CPU 1处理队列中缓存的Invalidate消息，将a对应的cacheline设置为无效。</li>
<li>由于a变量在local cache中无效，因此CPU 1在执行assert(a == 1)的时候需要发送一个read消息去获取a值。<br />
1. CPU 0用a的新值1回应来自CPU 1的请求。<br />
2. CPU 1获得了a的新值，并放入cacheline，这时候assert(a == 1)不会失败了。</li>
</ol>
<h2 id="read-and-write-memory-barriers"><a class="markdownIt-Anchor" href="#read-and-write-memory-barriers"></a> <strong>Read and Write Memory Barriers</strong></h2>
<p>在我们上面的例子中，memory barrier指令对store buffer和invalidate queue都进行了标注，不过，在实际的代码片段中，foo函数不需要mark invalidate queue，bar函数不需要mark store buffer<br />
因此，许多CPU architecture提供了弱一点的memory barrier指令只mark其中之一。如果只mark invalidate queue，那么这种memory barrier被称为read memory barrier。相应的，write memory barrier只mark store buffer。一个全功能的memory barrier会同时mark store buffer和invalidate queue。<br />
我们一起来看看读写内存屏障的执行效果：对于read memory barrier指令，它只是约束执行CPU上的load操作的顺序，具体的效果就是CPU一定是完成read memory barrier之前的load操作之后，才开始执行read memory barrier之后的load操作。read memory barrier指令象一道栅栏，严格区分了之前和之后的load操作。同样的，write memory barrier指令，它只是约束执行CPU上的store操作的顺序，具体的效果就是CPU一定是完成write memory barrier之前的store操作之后，才开始执行write memory barrier之后的store操作。全功能的memory barrier会同时约束load和store操作，当然只是对执行memory barrier的CPU有效。<br />
现在，我们可以改一个用读写内存屏障的版本了，具体如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_wmb</span>();</span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">smp_rmb</span>();</span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="memory-consistency-memory-model"><a class="markdownIt-Anchor" href="#memory-consistency-memory-model"></a> Memory Consistency (Memory Model)</h1>
<h2 id="reorder"><a class="markdownIt-Anchor" href="#reorder"></a> Reorder</h2>
<p><strong>同一线程中，彼此没有依赖关系的指令会被乱序执行。</strong><br />
我们编码并发布运行需要经过编译器编译后然后在CPU上运行，通常我们认为我们所写的代码是按照顺序执行下去的，就是说上一个语句一定在下一个语句执行之前执行。这是我们的潜意识，然而事实可能并不是这样，因为中间经过了编译器也经过了CPU。编译器和CPU为了充分提高程序运行性能会在内部进行一系列优化，这些优化方法有很多，也很复杂。比较典型的有reorder，Speculative execution等。编译器会对我们写的代码顺序进行reorder，CPU执行的时候也会进行reorder，也就是说在执行时，我们写的代码并不是一定按照我们所看到顺序。但是不用担心，CPU或者编译器在reorder的时候并不会无厘头的reorder，他们至少要保证的是，<strong>在reorder之后，程序所表现出来的行为效果与单线程执行效果是一致的</strong>。这里提到的是单线程，也就是说CPU和编译器并不能感知道你的代码是多线程还是单线程，他只能保证单线程状况时正确的，多线程就不得而知了。<br />
比如下面两条指令的执行顺序完全无法预测：</p>
<ol>
<li>x=1;</li>
<li>y=2;<br />
但是下面两条指令将被串行执行（存在依赖关系）：<br />
1. x = some input value;<br />
2. y=x+z;</li>
</ol>
<h3 id="乱序执行的原因"><a class="markdownIt-Anchor" href="#乱序执行的原因"></a> 乱序执行的原因</h3>
<ol>
<li>
<p><strong>编译器</strong><br />
编译优化假设程序是单线程（Single-Threaded Optimizations）：如果编译器希望对程序指令的执行顺序做出改变，只要这些改变不影响该程序在单线程情况下的运行结果，那么这些改变就是允许的。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113641723.png" alt="image.png" /></p>
</li>
<li>
<p><strong>处理器</strong><br />
现代处理器允许指令乱序执行，<strong>以避免因指令等待资源而导致处理器处于闲置状态。</strong><br />
处理器顺序执行：</p>
 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">y=x; <span class="comment">//cache miss，没有立即得到×的值</span></span><br><span class="line">z=y<span class="number">+1</span>; <span class="comment">//等待直到×被读到</span></span><br><span class="line">w=m+n; <span class="comment">//等待直到z得到结果</span></span><br></pre></td></tr></table></figure>
<p>处理器乱序执行：</p>
 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">y=x; <span class="comment">//cache miss，没有立即得到×的值</span></span><br><span class="line">w=m+n; <span class="comment">//如果m和n已经准备好，这条指令可以先执行</span></span><br><span class="line">z=y<span class="number">+1</span>; <span class="comment">//等待直到被读到</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>存储系统</strong><br />
为了省去写入一致性cache (l2 cache)需要的10个clock cycle和写入memory需要的100个clock cycle，引入了store buffer（写入只需要1个clock cycle）。所以在CPU流水线里一个写指令结束的时候，其实他的数据并没有真正写到memory或者cache里，而是进了store buffer。</p>
</li>
<li>
<p><strong>On-chip Network</strong><br />
CPU核之间的通信</p>
</li>
</ol>
<h2 id="memory-consistency-motivation"><a class="markdownIt-Anchor" href="#memory-consistency-motivation"></a> Memory consistency motivation</h2>
<p>对于C1而言，CPU可以执行S2-&gt;S1,也可以执行S1-&gt;S2, 对于这两种执行方式在C1看来是没有问题的，因为单线程而言这两种执行方式最后达到的效果是一样的。（因为S2和S1是对不同的memory location的操作，所以会reorder，如果是对同一个memory location操作是不允许出现这种reorder的）<br />
如果C1的执行顺序是S2-&gt;L1-&gt;L2-&gt;S1，那么得到的结果r2 = 0，而不是向我们预期的r2 = NEW。对我们而言这种结果是超出预期的，是错的。<br />
那么为了保证不会出现这种超出预期的行为，我们就需要一种规则来约束这种行为不能出现。这个任务就是memory consistency需要保证的。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113649801.png" alt="image.png" /></p>
<h2 id="memory-order"><a class="markdownIt-Anchor" href="#memory-order"></a> Memory order</h2>
<p>代码执行的顺序，这个是全局的，每个CPU core对共享内存的执行都会出现在memory order中如下图所示，每个core的代码都会对应到memory order这条执行线上。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113655885.png" alt="image.png" /></p>
<h2 id="sequential-consistency-sc"><a class="markdownIt-Anchor" href="#sequential-consistency-sc"></a> Sequential Consistency (SC)</h2>
<h3 id="并行程序中的顺序一致性"><a class="markdownIt-Anchor" href="#并行程序中的顺序一致性"></a> 并行程序中的顺序一致性</h3>
<p><strong>如果程序没有竞争（race）,那么程序运行起来就好像是顺序一致性</strong><br />
这句话这么理解：<br />
<strong>竞争</strong>：两个线程访问同一个变量，而且其中有一个操作是写<br />
<strong>没有竞争</strong>：用某种方法把全局变量保护了起来，比如锁，原子指令等等<br />
<strong>顺序一致性</strong>：可以这么理解，在只有一个核的处理器上运行多线程程序，所以这些指令都是交织在一起，按照全局序执行的，这种方式运行的结果我们认为是正确的<br />
<strong>好像顺序一致性</strong>：运行结果跟某种顺序一致性程序运行的结果是一样的</p>
<p>The result of any execution is the same as if the operations of all processors (cores) were executed in some sequential order, and the operations of each individual processor (core) appear in this sequence in the order specified by its program</p>
<p>如上图中，S1 与 S2的program order可以表示为： S1 &lt;p S2； S1与L2的memory order可以表示为 S1 &lt;m L2。 用&lt;p 表示program order的先于顺序，&lt;m表示memory order的先于顺序。</p>
<p><strong>定义</strong></p>
<ol>
<li>All cores insert their loads and stores into the order &lt;m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a=b or a≠b).<br />
所有对共享内存的操作都可以抽象成load(读取)和store(写入)，每一core执行load和store是按照其program order，那么就有S1 &lt;p S2肯定会推出 S1 &lt;m S2，SC的定义也由此引入了load和store的四种关系。在SC的定义中这四种关系是不允许被reorder的，即使是对不同memory location的操作。</li>
<li>Every load gets its value from the last store before it (in global memory order) to the same address</li>
</ol>
<p>只要符合上述两个条件，那么我们就可以说这个memory操作是符合顺序一致性的。</p>
<p>Example:<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113707830.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113714781.png" alt="image.png" /></p>
<p>从图中可以看出在保证program order不变的情况下，memory order的顺序可以随意排列。</p>
<h3 id="竞争和-object-layout-有关"><a class="markdownIt-Anchor" href="#竞争和-object-layout-有关"></a> 竞争和 Object Layout 有关</h3>
<p>第一种情况：全局变量s,类型是struct {char c;char d;}  <mark>没有竞争</mark><br />
第二种情况：全局变量s，类型是struct {int c:9;int d:7;} <mark>有竞争</mark><br />
在上述两种情况下，下面的程序是否有竞争？<br />
线程1：s.c=1;<br />
线程2：s.d=2;<br />
在第二种情况下，声明了一个32 bit int大小的结构体，前9bit的是int c，后面7bit的是int d，然而bit并不是c++保证原子性的单位，因此写c或者d的时候，整个32 bit的int都会被牵连</p>
<h2 id="total-store-order-tso"><a class="markdownIt-Anchor" href="#total-store-order-tso"></a> Total Store Order (TSO)</h2>
<p>TSO的定义与SC的定义有两个变化：<br />
1.  <strong>不保证storeload顺序</strong><br />
举个例子：Core C1中S1和L1， S1先去L1执行，但是S1只是将值送入了write buffer就返回了，紧接着执行L1，L1在memory order中的点执行完之后，S1的write buffer这时候flush到内存，那么S1在memory order这条线上真正执行的点在L1之后了，那么这时候S1与L1就出现了reorder了。<br />
2. <strong>Every load gets its value from the last store before it to the same address:</strong><br />
需要注意的是，无论是TSO还是SC都需要至少保证一点，即使允许reorder也要保证program执行的结果与单线程执行的结果是一致的。比如一对操作：  S1: x = new  L1: y = x.  无论是TSO还是SC都需要保证y读到的是x=new的值（排除其他线程在这两个语句之前对x进行store操作。）<br />
因为TSO引入了write buffer，那么上述x=new会写入buffer，如何确保L1会读到最新的值呢，TSO引入了一种叫“bypass”的概念，就是对于<strong>同一memory location</strong>的读写会保障load会读到store的最新值无论这个store会不会进入write buffer。<br />
如下图所示：  L1读取的是S1的值，即使L1 &lt;m S1 且 S1 &lt;p L1.<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113725472.png" alt="image.png" /></p>
<p>TSO在CPU与memory之间引入了write buffer。CPU写入的时候先写入write buffer然后就返回了，这样就将cpu与memory之间的差距隐藏了，但是这样同样带来了一个问题。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113731619.png" alt="image.png" /></p>
<p>还是上面这个例子，S1将x=NEW放到了core C1的write buffer中，S2将y=NEW放到了C2的write buffer中，那么在执行L1,L2的时候，r1与r2这时候从memory读到是0。这个是违背了SC的，但是这样的设计确实带来了性能的提升。那么在TSO模型下的执行结果如下：前三种与SC一致，第四个执行结果则是TSO独有的，可以看出，TSO中允许执行线交叉。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113738004.png" alt="image.png" /></p>
<p>如果我们想避免这种问题，那么需要在上层代码中添加FENCE，这个fence可以理解为memory barrier，他的作用是将write buffer中的记录flush到内存。<br />
FENCE会强制保证program order。</p>
<ul>
<li>If S(a) &lt;p FENCE ⇒ S(a) &lt;m FENCE /* Store → FENCE */</li>
<li>If FENCE &lt;p L(a) ⇒ FENCE &lt;m L(a) /* FENCE → Load */<br />
如果再S1与L1之间加上FENCE，就保证了S1 &lt;p L1 和 S1 &lt;m L1.</li>
<li><strong>Relaxed memory consistency</strong><br />
SC和TSO严格意义上来说都是一种强一致性模型，因为他们都对程序的执行顺序做了一定的约束，既然存在约束那么就会带来一定的性能损耗。<br />
那么有没有一种没这么多的约束的一致性模型，能够使机器进行深度的优化并发挥极致性能。那么执行顺序的正确性就只能有编码人员来保证了。<br />
relaxed memory consistency实现对于load与store顺序完全放开，除了对同一memory  location的操作保证load看到是最新的store以外其他都不进行约束，编码人员如果想强加order可以通过上述的FENCE。</li>
</ul>
<h1 id="memory-order-2"><a class="markdownIt-Anchor" href="#memory-order-2"></a> Memory Order</h1>
<p>C<ins>11在标准库中引入了memory model，这应该是C</ins>11最重要的特性之一了。C<ins>11引入memory model的意义在于我们可以在high level language层面实现对在多处理器中多线程共享内存交互的控制。我们可以在语言层面忽略compiler，CPU arch的不同对多线程编程的影响了。我们的多线程可以跨平台了。<br />
C</ins> atomic操作数中有一个选项可以指定对应的memory_order，这里的memory order可以理解为上面章节中的memory order。C++11中提供了六种不同memory_order选项，不同的选项会定义不同的memory consistency类型。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> std &#123;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="keyword">enum</span> <span class="title class_">memory_order</span> &#123;</span><br><span class="line">    memory_order_relaxed, memory_order_consume, memory_order_acquire,</span><br><span class="line">      memory_order_release, memory_order_acq_rel, memory_order_seq_cst</span><br><span class="line">&#125; memory_order;</span><br><span class="line"><span class="function">The enumeration memory_order specifies the detailed <span class="title">regular</span> <span class="params">(non-atomic)</span> memory synchronization order as defined in 1.10 <span class="keyword">and</span> may provide <span class="keyword">for</span> operation ordering. [10]      </span></span><br></pre></td></tr></table></figure>
<p>memory order指定了对应的对共享内存的operation order的关系。memory order也是一致性模型的一种反映。</p>
<h2 id="happens-before"><a class="markdownIt-Anchor" href="#happens-before"></a> Happens-before</h2>
<p><strong>Happens-before</strong> 是一个非常重要的概念. 如果操作 a “happens-before” 操作 b, 则操作 a 的结果对于操作 b 可见. happens-before 的关系可以建立在用一个线程的两个操作之间, 也可以建立在不同的线程的两个操作之间.</p>
<ul>
<li>
<p><strong>单线程的情况: sequenced-before</strong><br />
函数的语句按顺序依次执行, 前面的语句先执行, 后面的后执行. 正式地说, 前面的语句总是 <strong>“sequenced-before”</strong> 后面的语句.</p>
</li>
<li>
<p><strong>多线程的情况: synchronizes-with 和 inter-thread happens-before</strong><br />
一般来说多线程都是并发执行的, 如果没有正确的同步操作, 就无法保证两个操作之间有 happens-before 的关系. 如果我们通过一些手段, 让不同线程的两个操作同步, 我们称这两个操作之间有 <strong>synchronizes-with</strong> 的关系<br />
<strong>if thread A stores a value and thread B reads that value, there’s a synchronizes-with relationship between the store in thread A and the load in thread B.</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113748857.png" alt="image.png" /></p>
<p>现在我们来看一个例子. 假设下面的代码中 <code>unlock()</code> 操作 “synchronizes-with” <code>lock()</code> 操作.</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	a += <span class="number">1</span> <span class="comment">// (1)  </span></span><br><span class="line">	<span class="built_in">unlock</span>(); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">	  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="built_in">lock</span>(); <span class="comment">// (3)  </span></span><br><span class="line">	cout &lt;&lt; a &lt;&lt; endl; <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>假设直到 <code>thread1</code> 执行到 (2) 之前, <code>thread2</code> 都会阻塞在 (3) 处的 <code>lock()</code> 中. 那么可以推导出:</p>
<ul>
<li>根据语句顺序, 有 (1) “sequenced-before” (2) 且 (3) “sequenced-before” (4);</li>
<li>因为 (2) “synchronizes-with” (3) 且 (3) “sequenced-before” (4), 所以 (2) “inter-thread happens-before” (4);</li>
<li>因为 (1) “sequenced-before” (2) 且 (2) “inter-thread happens-before” (4), 所以 (1) “inter-thread happens-before” (4); 所以 (1) “happens-before” (4).</li>
</ul>
<p>因此 (4) 可以读到 (1) 对变量 <code>a</code> 的修改.</p>
</li>
</ul>
<h2 id="memory_order_relaxed"><a class="markdownIt-Anchor" href="#memory_order_relaxed"></a> <strong>memory_order_relaxed</strong></h2>
<p>宽松内存模型，这种内存模型对当前原子操作周围的内存访问顺序不做任何保证，也就是允许全部的内存乱序发生。包括 Load-Load、Load-Store、Store-Load 和 Store-Store 乱序。</p>
<h3 id="使用场景"><a class="markdownIt-Anchor" href="#使用场景"></a> 使用场景</h3>
<h4 id="计数器counter"><a class="markdownIt-Anchor" href="#计数器counter"></a> 计数器（Counter）</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113759993.png" alt="image.png" /></p>
<ul>
<li>count是个原子变量，初值为0。</li>
<li>这里可以用relaxed因为join_workers起到了acquire/release的作用（可以假设join_workers内部包含一个 memory barrier）。而在join_workers之前，没有读操作，写操作的顺序也不重要(blindwrite)。</li>
</ul>
<h4 id="简单标志simple-flag-setting"><a class="markdownIt-Anchor" href="#简单标志simple-flag-setting"></a> 简单标志（Simple Flag Setting)</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113805089.png" alt="image.png" /></p>
<p>dirty 和 stop 是原子布尔变量，初始值为 false。dirty可以使用 relaxed 的 原因和计数器相同，dirty其实就是一个最大值为1的计数器。</p>
<h4 id="引用计数reference-counting"><a class="markdownIt-Anchor" href="#引用计数reference-counting"></a> 引用计数（Reference Counting）</h4>
<p><code>std::shared_ptr</code> 增加引用计数时用的就是 <code>memory_order_relaxed</code>, 因为不需要同步; 但是减小应用计数不能用它, 因为需要与析构操作同步.<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223114032860.png" alt="image.png" /></p>
<p>线程1：从0加到1，没有共享，私有变量，不存在memoryorder的问题：在1以上增加，没有区别，refcnt=2或者3对删除来说都是一样的，即：不删除。所以我不关心谁把refcnt从1变成2，谁把它从2变成3。也就是说：加1操作的顺序不重要，只要最后加上去就可以。所以加1操作可以是relaxed。<br />
<strong>对于B操作需要 release 语义</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115041399.png" alt="image.png" /></p>
<p>如果B使用relaxed,那么A和B可以乱序，假设发生在线程2a中。2a的B把refcnt从2减到1，然后线程卡住。线程2b中A和B没有乱序，2b在2a睡眠期间把refcnt从1减到0，并且释放了control_block_ptr。然后线程2a醒过来执行A,就会访问已经释放的对象。为防止A和B乱序，B需要release语义。<br />
<strong>对于B操作需要 acquire_release 语义</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115131954.png" alt="image.png" /><br />
假设B使用release语义，线程2b中B后面的指令C和D就可以提前到B之前。因此线程2a写入的最新的×可能不能被2b看到。更有甚者，D被移到B之前是灾难性的。</p>
<h4 id="单例模式-singleton"><a class="markdownIt-Anchor" href="#单例模式-singleton"></a> 单例模式 (Singleton)</h4>
<h5 id="常见错误"><a class="markdownIt-Anchor" href="#常见错误"></a> 常见错误</h5>
<p>这个实现是否正确？double-check locking<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115437774.png" alt="image.png" /></p>
<p>如果第一个线程执行到B,然后卡住，C尚未执行；第二线程会在A得到一个非空的指针，并返回给用户，但是这个指针指向一个未初始化的对象。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115453757.png" alt="image.png" /></p>
<h5 id="正确写法"><a class="markdownIt-Anchor" href="#正确写法"></a> 正确写法</h5>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115504723.png" alt="image.png" /></p>
<h5 id="最佳写法"><a class="markdownIt-Anchor" href="#最佳写法"></a> 最佳写法</h5>
<p>c++11后 static 默认是线程安全的<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115515680.png" alt="image.png" /></p>
<p>注意: singleton 通常是不释放的</p>
<h4 id="初始化-atomic-array"><a class="markdownIt-Anchor" href="#初始化-atomic-array"></a> 初始化 atomic array</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115524889.png" alt="image.png" /></p>
<p><strong>四种方法</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134818787.png" alt="image.png" /></p>
<ul>
<li>方法1: 普通的 for 循环, 但是不安全, read, write的时候会 out of order</li>
<li>方法2: 单次循环之后, 会有一个mfence, 很消耗时间</li>
<li>方法3: 和方法1一模一样, mfence没了</li>
<li>方法4: mfence在循环体外面, 只执行一次, 性能开销小</li>
</ul>
<h4 id="volatile-关键字"><a class="markdownIt-Anchor" href="#volatile-关键字"></a> “volatile” 关键字</h4>
<ul>
<li>volatile (Java) != volatile(C, C+)</li>
<li>mutex, atomics和memory barriers用于控制程序对内存的访问(原子性和顺序)。</li>
<li>volatile用于控制对I/O的访问，比如I/O寄存器，这些寄存器虽然映射在内存中，但是完全不遵循常规的内存模型。编译器唯一能做的就是不做任何优化，老老实实的去生成汇编指令。volatile不保证原子性。</li>
</ul>
<h3 id="example"><a class="markdownIt-Anchor" href="#example"></a> Example</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_relaxed)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>()); <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>thread1</code> 对不同的变量执行 store 操作. 那么在某些线程看来, 有可能是 <code>x</code> 先变为 <code>true</code>, y 后变为 <code>true</code>; 另一些线程看来, 又有可能是 <code>y</code> 先变为 <code>true</code>, <code>x</code> 后变为 <code>true</code>.<br />
(4) 处的断言就有可能失败. 因为 (2) 与 (3) 之间没有 synchronizes-with 的关系, 所以就不能保证 (1) “happens-before” (4). 因此 (4) 就有可能读到 <code>false</code>.</p>
<h2 id="memory_order_seq_cst"><a class="markdownIt-Anchor" href="#memory_order_seq_cst"></a> memory_order_seq_cst</h2>
<p>顺序一致性，也是默认的选项，这个选项不允许reorder。</p>
<h3 id="acquire-和-release-不保证全局序"><a class="markdownIt-Anchor" href="#acquire-和-release-不保证全局序"></a> Acquire 和 Release 不保证全局序</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134832484.png" alt="image.png" /></p>
<p><strong>On-Chip Network 不保证全局序</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134838406.png" alt="image.png" /></p>
<p>片上网络不保证消息传播的序。比如：处理器3收到x=1,但是处理器4还没有收到x=1(消息还在路上)；同时，处理器4收到y=1,但是处理器3还没有收到y=1(消息还在路上)。这时处理器3和4都打印。</p>
<h3 id="sequentially-consistencysc保证写操作全局序"><a class="markdownIt-Anchor" href="#sequentially-consistencysc保证写操作全局序"></a> Sequentially Consistency(SC)保证写操作全局序</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134844716.png" alt="image.png" /></p>
<p><strong>SC强迫On-Chip Network保证全局序</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134850394.png" alt="image.png" /></p>
<p>片上网络保证消息传播的序，即：先传播×=1到所有处理器，等到所有处理器都收到x=1之后，再传播y=1;或者反过来。总之，写操作串行地使用片上网络，从而在片上网络这一层产生了全局序。</p>
<h3 id="stdatomic语义总结"><a class="markdownIt-Anchor" href="#stdatomic语义总结"></a> std:atomic&lt;…&gt;语义总结</h3>
<ol>
<li>原子性：读写都原子，你不会读到部分结果或者中间结果。</li>
<li>顺序性：所有针对原子变量的读写都会按序执行，实现顺序一致性(SC)。(假定使用std:atomic&lt;…&gt;的默认内存模型)</li>
</ol>
<h3 id="example-2"><a class="markdownIt-Anchor" href="#example-2"></a> Example</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_seq_cst); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_seq_cst); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br><span class="line">std::atomic&lt;<span class="type">int</span>&gt; z&#123;<span class="number">0</span>&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_x_then_y</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!x.<span class="built_in">load</span>(std::memory_order_seq_cst)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="keyword">if</span> (y.<span class="built_in">load</span>(std::memory_order_seq_cst)) ++z; <span class="comment">// (4)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_y_then_x</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_seq_cst)); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="keyword">if</span> (x.<span class="built_in">load</span>(std::memory_order_seq_cst)) ++z; <span class="comment">// (6)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="function">std::thread <span class="title">a</span><span class="params">(thread1)</span>, <span class="title">b</span><span class="params">(thread2)</span>, <span class="title">c</span><span class="params">(read_x_then_y)</span>, <span class="title">d</span><span class="params">(read_y_then_x)</span></span>;  </span><br><span class="line">	a.<span class="built_in">join</span>(), b.<span class="built_in">join</span>(), c.<span class="built_in">join</span>(), d.<span class="built_in">join</span>();  </span><br><span class="line">	<span class="built_in">assert</span>(z.<span class="built_in">load</span>() != <span class="number">0</span>); <span class="comment">// (7)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(7) 处的断言永远不会失败. 因为 <code>x</code> 和 <code>y</code> 的修改顺序是全局一致的, 如果先执行 (1) 后执行 (2), 则 <code>read_y_then_x</code> 中循环 (5) 退出时, 能保证 <code>y</code> 为 <code>true</code>, 此时 <code>x</code> 也必然为 <code>true</code>, 因此 (6) 会被执行; 同理, 如果先执行 (2) 后执行 (1), 则循环 (3) 退出时 <code>y</code> 也必然为 <code>true</code>, 因此 (4) 会被执行. 无论如何, <code>z</code> 最终都不会等于 0.</p>
<p>Sequencial consistent 可以实现 synchronizes-with 的关系. 如果一个 <code>memory_order_seq_cst</code> 的 load 操作在某个原子变量上读到了一个 <code>memory_order_seq_cst</code> 的 store 操作在这个原子变量中写入的值, 则 store 操作 “synchronizes-with” load 操作. 在上面的例子中, 有 (1) “synchronizes-with” (3) 和 (2) “synchronizes-with” (5).</p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>实现 sequencial consistent 模型有一定的开销. 现代 CPU 通常有多核, 每个核心还有自己的缓存. 为了做到全局顺序一致, 每次写入操作都必须同步给其他核心. 为了减少性能开销, 如果不需要全局顺序一致, 我们应该考虑使用更加宽松的顺序模型.</p>
<h2 id="acquire-release"><a class="markdownIt-Anchor" href="#acquire-release"></a> Acquire-Release</h2>
<p><strong>Acquire means “after is after”：之后的所有指令不会早于这条指令开始执行，尤其是读指令不会。即后面访存指令勿重排至此条指令之前</strong><br />
<strong>Release means “before is before”：之前的所有指令都已经执行完，尤其是写指令的结果已经全局可见。即前面访存指令勿重排至此条指令之后，当此条指令的结果对其他线程可见后，之前的所有指令都可见</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134903299.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134917311.png" alt="image.png" /></p>
<p>在 acquire-release 模型中, 会使用 <code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code> 这三种内存顺序. 它们的用法具体是这样的:</p>
<ul>
<li>对原子变量的 load 可以使用 <code>memory_order_acquire</code> 内存顺序. 这称为 <strong>acquire 操作</strong>.</li>
<li>对原子变量的 store 可以使用 <code>memory_order_release</code> 内存顺序. 这称为 <strong>release 操作</strong>.</li>
<li>read-modify-write 操作即读 (load) 又写 (store), 它可以使用 <code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code>:
<ul>
<li>如果使用 <code>memory_order_acquire</code>, 则作为 acquire 操作;</li>
<li>如果使用 <code>memory_order_release</code>, 则作为 release 操作;</li>
<li>如果使用 <code>memory_order_acq_rel</code>, 则同时为两者.<br />
Acquire-release 可以实现 synchronizes-with 的关系. 如果一个 acquire 操作在同一个原子变量上读取到了一个 release 操作写入的值, 则这个 release 操作 “synchronizes-with” 这个 acquire 操作. 我们来看一个例子:</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>(std::memory_order_relaxed)); <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的例子中, 语句 (2) 使用 <code>memory_order_release</code> 在 <code>y</code> 中写入 <code>true</code>, 语句 (3) 中使用 <code>memory_order_acquire</code> 从 <code>y</code> 中读取值. 循环 (3) 退出时, 它已经读取到了 <code>y</code> 的值为 <code>true</code>, 也就是读取到了操作 (2) 中写入的值. 因此有 (2) “synchronizes-with” (3). 我们可以推导出:</p>
<ul>
<li>因为 (2) “synchronizes-with” (3) 且 (3) “sequenced-before” (4), 所以 (2) “inter-thread happens-before” (4);</li>
<li>因为 (1) “sequenced-before” (2) 且 (2) “inter-thread happens-before” (4), 所以 (1) “inter-thread happens-before” (4);<br />
所以 (1) “happens-before” (4). 因此 (4) 能读取到 (1) 中写入的值, 断言永远不会失败. 即使 (1) 和 (4) 用的是 <code>memory_order_relaxed</code>.</li>
</ul>
<p>事实上, 内存顺序为 <code>memory_order_seq_cst</code> 的 load 操作和 store 操作可以分别视为 acquire 操作和 release 操作. 因此对于两个指定了 <code>memory_order_seq_cst</code> 的 store 操作和 load 操作, 如果后者读到了前者写入的值, 则前者 “synchronizes-with” 后者.</p>
<p>为了实现 synchronizes-with 关系, acquire 操作和 release 操作应该成对出现. 如果 <code>memory_order_acquire</code> 的 load 读到了 <code>memory_order_relaxed</code> 的 store 写入的值, 或者 <code>memory_order_relaxed</code> 的 load 读到了 <code>memory_order_release</code> 的 store 写入的值, 都不能实现 synchronizes-with 的关系.</p>
<p>虽然 sequencial consistent 模型能够像 acquire-release 一样实现同步, 但是反过来 acquire-release 模型不能像 sequencial consistent 一样提供全局顺序一致性. 如果将例子中的 <code>memory_order_seq_cst</code> 换成 <code>memory_order_acquire</code> 和 <code>memory_order_release</code></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_x_then_y</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!x.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="keyword">if</span> (y.<span class="built_in">load</span>(std::memory_order_acquire)) ++z; <span class="comment">// (4)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_y_then_x</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="keyword">if</span> (x.<span class="built_in">load</span>(std::memory_order_acquire)) ++z; <span class="comment">// (6)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>则最终不能保证 <code>z</code> 不为 0. 在同一次运行中, <code>read_x_then_y</code> 有可能看到先 (1) 后 (2), 而 <code>read_y_then_x</code> 有可能看到先 (2) 后 (1). 这样有可能 (4) 和 (6) 的 load 的结果都为 <code>false</code>, 导致最后 <code>z</code> 仍然为 0.</p>
<p>Acquire-release 的开销比 sequencial consistent 小. 在 x86 架构下, <code>memory_order_acquire</code> 和 <code>memory_order_release</code> 的操作不会产生任何其他的指令, 只会影响编译器的优化: 任何指令都不能重排到 acquire 操作的前面, 且不能重排到 release 操作的后面; 否则会违反 acquire-release 的语义. 因此很多需要实现 synchronizes-with 关系的场景都会使用 acquire-release.</p>
<h3 id="acquire-和-release-的实现编译器"><a class="markdownIt-Anchor" href="#acquire-和-release-的实现编译器"></a> Acquire 和 Release 的实现（编译器）</h3>
<p>1. Acquire 和 release 操作的内部实现需要利用 memory barrier 指令。<br />
2. Acquire和 release操作内部由汇编语言编写，因此可以排除编译优化的影响，同时通过汇编语言也可以方便地嵌入memory barrier指令。<br />
3. 当编译器看到 memory barrier 指令时，不会把 acquire后面的指令挪到acquire前面，也不会把 release前面的指令挪到 release后面。<br />
4. 编译器不能把一个函数调用后面的指令挪到该函数调用的前面，也不能把一个函数调用前面的指令挪到该函数调用的后面，因为编译器不知道该函数调用内部是否使用了memory barrier指令（如果编译器能够判断该函数内部不涉及 memory barrier指令，另当别论）。</p>
<h3 id="acquire-和-release-的实现处理器"><a class="markdownIt-Anchor" href="#acquire-和-release-的实现处理器"></a> Acquire 和 Release 的实现（处理器）</h3>
<p>PowerPC的 lwsync指令是 memory barrier指令，其工作原理是堵在处理器流水线的入口，不让后续指令进入流水线，直到前面已经进入流水线的指令执行完，并且storebuffer清空。逻辑上看：lwsync保证它前面的指令不会被挪到它后面，它后面的指令不会被挪到它前面，因此是一个双向的 memory barrier。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134932447.png" alt="image.png" /></p>
<h4 id="单独的-memory-barrier-指令代价大"><a class="markdownIt-Anchor" href="#单独的-memory-barrier-指令代价大"></a> 单独的 memory barrier 指令代价大</h4>
<p>Release操作的要求：</p>
<ol>
<li>lwsync前面的指令（比如“writex&quot;）不能挪到lwsync之后；</li>
<li>&quot;Ready=1&quot;这一条写指令不能挪到lwsync之前。lwsync可以满足1，但同时要求lwsync之后所有指令都不能挪到lwsync之前，超过了实际需要，有损编译优化。 <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134941526.png" alt="image.png" /></li>
</ol>
<h4 id="合并的acquire和release"><a class="markdownIt-Anchor" href="#合并的acquire和release"></a> 合并的“Acquire”和“Release&quot;</h4>
<p>Intel IA64处理器把 memory barrier指令和读写指令合并，提供了带 acquire语义的读指令ld.acq（称为acquire load）和带 release语义的写指令 st.rel（称为release store）。因此，线程1的&quot;read/writey’可以挪到“st.rel ready1&quot;之前，线程2的“read/writez&quot;可以挪到“ld.acqr0,ready”之后，增加了编译优化的可能性。<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134950542.png" alt="image.png" /></p>
<h4 id="acquire和-release-自动化"><a class="markdownIt-Anchor" href="#acquire和-release-自动化"></a> Acquire和 release 自动化</h4>
<ol>
<li>不要自己手工地使用 memory barrier指令。</li>
<li>使用锁或者原子变量来自动实现acquire和 release操作。</li>
</ol>
<ul>
<li>Lock acquire/release</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">mutex<span class="number">1.l</span>ock(); <span class="comment">//ld.acq mutex1</span></span><br><span class="line">read/write x;</span><br><span class="line">mutex<span class="number">1.</span><span class="built_in">unlock</span>(); <span class="comment">//st.rel mutex1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>std:atomic&lt;bool&gt; flag = 0;</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (!flag.<span class="built_in">compre_exchange_strong</span>(..); <span class="comment">//ld.acq flag</span></span><br><span class="line">read/write x;</span><br><span class="line">flag = <span class="number">0</span>; <span class="comment">//st.rel flag</span></span><br></pre></td></tr></table></figure>
<h3 id="release-sequences"><a class="markdownIt-Anchor" href="#release-sequences"></a> Release sequences</h3>
<p>到目前为止我们看到的, 无论是 sequencial consistent 还是 acquire-release, 要想实现 synchronizes-with 的关系, acquire 操作必须在同一个原子变量上读到 release 操作的写入的值. 如果 acquire 操作没有读到 release 操作写入的值, 那么它俩之间通常没有 synchronizes-with 的关系. 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x&#123;<span class="number">0</span>&#125;, y&#123;<span class="number">0</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="number">2</span>, std::memory_order_release); <span class="comment">// (3)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread3</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>(std::memory_order_relaxed) == <span class="number">1</span>); <span class="comment">// (5)  </span></span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>上面的例子中, 只要 <code>y</code> 的值非 0 循环 (4) 就会退出. 当它退出时, 有可能读到 (2) 写入的值, 也有可能读到 (3) 写入的值. 如果是后者, 则只能保证 (3) “synchronizes-with” (4), 不能保证与 (2) 与 (4) 之间有同步关系. 因此 (5) 处的断言就有可能失败.</p>
<p>但并不是只有在 acquire 操作读取到 release 操作写入的值时才能构成 synchronizes-with 关系. 为了说这种情况, 我们需要引入 <strong>release sequence</strong> 这个概念.</p>
<p>针对一个原子变量 M 的 release 操作 A 完成后, 接下来 M 上可能还会有一连串的其他操作. 如果这一连串操作是由</p>
<ul>
<li>同一线程上的写操作, 或者</li>
<li>任意线程上的 read-modify-write 操作<br />
这两种构成的, 则称这一连串的操作为<strong>以 release 操作 A 为首的 release sequence</strong>. 这里的写操作和 read-modify-write 操作可以使用任意内存顺序.</li>
</ul>
<p>如果一个 acquire 操作在同一个原子变量上读到了一个 release 操作写入的值, 或者读到了以这个 release 操作为首的 release sequence 写入的值, 那么这个 release 操作 “synchronizes-with” 这个 acquire 操作. 我们来看个例子</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int</span>&gt; data;  </span><br><span class="line">std::atomic&lt;<span class="type">int</span>&gt; flag&#123;<span class="number">0</span>&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	data.<span class="built_in">push_back</span>(<span class="number">42</span>); <span class="comment">// (1)  </span></span><br><span class="line">	flag.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line"><span class="type">int</span> expected = <span class="number">1</span>;  </span><br><span class="line">	<span class="keyword">while</span> (!flag.<span class="built_in">compare_exchange_strong</span>(expected, <span class="number">2</span>, std::memory_order_relaxed)) <span class="comment">// (3)  </span></span><br><span class="line">	expected = <span class="number">1</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread3</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (flag.<span class="built_in">load</span>(std::memory_order_acquire) &lt; <span class="number">2</span>); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(data.<span class="built_in">at</span>(<span class="number">0</span>) == <span class="number">42</span>); <span class="comment">// (5)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的例子中, (3) 处的 <code>compare_exchange_strong</code> 是一种 read-modify-write 操作, 它判断原子变量的值是否与期望的值 (第一个参数) 相等, 如果相等则将原子变量设置成目标值 (第二个参数) 并返回 <code>true</code>, 否则将第一个参数 (引用传递) 设置成原子变量当前值并返回 <code>false</code>. 操作 (3) 会一直循环检查, 当 <code>flag</code> 当值为 1 时, 将其替换成 2. 所以 (3) 属于 (2) 的 release sequence. 而循环 (4) 退出时, 它已经读到了 (3) 写入的值, 也就是 release 操作 (2) 为首的 release sequence 写入的值. 所以有 (2) “synchronizes-with” (4). 因此 (1) “happens-before” (5), (5) 处的断言不会失败.</p>
<p>注意 (3) 处的 <code>compare_exchange_strong</code> 的内存顺序是 <code>memory_order_relaxed</code>, 所以 (2) 与 (3) 并不构成 synchronizes-with 的关系. 也就是说, 当循环 (3) 退出时, 并不能保证 <code>thread2</code> 能读到 <code>data.at(0)</code> 为 42. 但是 (3) 属于 (2) 的 release sequence, 当 (4) 以 <code>memory_order_acquire</code> 的内存顺序读到 (2) 的 release sequence 写入的值时, 可以与 (2) 构成 synchronizes-with 的关系.</p>
<h2 id="memory_order_consume"><a class="markdownIt-Anchor" href="#memory_order_consume"></a> memory_order_consume</h2>
<p><strong>后面依赖此原子变量的访存指令勿重排至此条指令之前</strong><br />
<code>memory_order_consume</code> 其实是 acquire-release 模型的一部分, 但是它比较特殊, 它涉及到数据间相互依赖的关系. 为此我们又要提出两个新概念: <strong>carries dependency</strong> 和 <strong>dependency-ordered before</strong>.<br />
如果操作 a “sequenced-before” b, 且 b 依赖 a 的数据, 则 a “carries a dependency into” b. 一般来说, 如果 a 的值用作 b 的一个操作数, 或者 b 读取到了 a 写入的值, 都可以称为 b 依赖于 a. 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">p++; <span class="comment">// (1)  </span></span><br><span class="line">i++; <span class="comment">// (2)  </span></span><br><span class="line">p[i]; <span class="comment">// (3)</span></span><br></pre></td></tr></table></figure>
<p>有 (1) “sequenced-before” (2) “sequenced-before” (3); (1) 和 (2) 的值作为 (3) 的下标运算符 <code>[]</code> 的操作数, 所以有 (1) “carries a dependency into” (3) 和 (2) “carries a dependency into” (3). 但是 (1) 和 (2) 并没有相互依赖, 它们之间没有 carries dependency 的关系. 类似于 sequenced-before, carries dependency 关系具有传递性.</p>
<p><code>memory_order_consume</code> 可以用于 load 操作. 使用 <code>memory_order_consume</code> 的 load 称为 consume 操作. 如果一个 consume 操作在同一个原子变量上读到了一个 release 操作写入的值, 或以其为首的 release sequence 写入的值, 则这个 release 操作 “dependency-ordered before” 这个 consume 操作.</p>
<p>概念很复杂, 但是基本思路是:</p>
<ul>
<li>release 操作和 acquire 操作构成的 synchronizes-with 可以后接 sequenced-before 构成 inter-thread happens-before 的关系;</li>
<li>release 操作和 consume 操作构成的 dependency-ordered before 则只能后接 carries dependency 构成 inter-thread happens-before 的关系.</li>
<li>无论 inter-thread happens-before 是怎么构成的, 都可以前接 sequenced-before 以延伸其范围.</li>
</ul>
<p>我们来看一个例子:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;std::string*&gt; ptr;  </span><br><span class="line"><span class="type">int</span> data;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	std::string* p = <span class="keyword">new</span> std::<span class="built_in">string</span>(<span class="string">&quot;Hello&quot;</span>); <span class="comment">// (1)  </span></span><br><span class="line">	data = <span class="number">42</span>; <span class="comment">// (2)  </span></span><br><span class="line">	ptr.<span class="built_in">store</span>(p, std::memory_order_release); <span class="comment">// (3)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	std::string* p2;  </span><br><span class="line">	<span class="keyword">while</span> (!(p2 = ptr.<span class="built_in">load</span>(std::memory_order_consume))); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(*p2 == <span class="string">&quot;Hello&quot;</span>); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="built_in">assert</span>(data == <span class="number">42</span>); <span class="comment">// (6)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(4) 处的循环退出时, consume 操作 (4) 读取到 release 操作 (3) 写入的值, 因此 (3) “dependency-ordered before” (4). 由此可以推导出:</p>
<ul>
<li><code>p2</code> 的值作为 (5) 的操作数, 因此 (4) “carries a dependency into” (5);</li>
<li>因为 (3) “dependency-ordered before” (4) 且 (4) “carries a dependency into” (5), 所以 (3) “inter-thread happens-before” (5);</li>
<li>因为 (1) “sequenced-before” (3) 且 (3) “inter-thread happens-before” (5), 所以 (1) “inter-thread happens-before” (5);</li>
</ul>
<p>所以 (1) “happens-before” (5). 因此 (5) 可以读到 (1) 写入的值, 断言 (5) 不会失败. 但是操作 (6) 并不依赖于 (4), 所以 (3) 和 (6) 之间没有 inter-thread happens-before 的关系, 因此断言 (6) 就有可能失败. 回想 2.2 节强调过的, happens-before 没有传递性. 所以不能说因为 (3) “happens-before” (4) 且 (4) “happens-before” (6) 所以 (2) “happens-before” (6).</p>
<p>与 acquire-release 类似, 在 x86 下使用 <code>memory_order_consume</code> 的操作不会产生任何其他的指令, 只会影响编译器优化. 与 consume 操作有依赖关系的指令都不会重排到 consume 操作前面. 它对重排的限制比 acquire 宽松些, acquire 要求所有的指令都不能重排到它的前面, 而 consume 只要求有依赖关系的指令不能重排到它的前面. 因此在某些情况下, consume 的性能可能会高一些.</p>
<h2 id="一些例子"><a class="markdownIt-Anchor" href="#一些例子"></a> 一些例子</h2>
<ul>
<li>
<p><strong>自旋锁</strong><br />
在一些场景下, 如果锁被占用的时间很短, 我们会选择自旋锁, 以减少上下文切换的开销. 锁一般用来保护临界数据的读写, 我们希望同一时间只有一个线程能获取到锁, 且获取到锁后, 被锁保护的数据总是最新的. 前者通过原子操作即可保证, 而后者就需要考虑内存顺序了.</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::deque&lt;<span class="type">int</span>&gt; queue;  </span><br><span class="line">spinlock mu;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="type">int</span> val;  </span><br><span class="line">	<span class="keyword">while</span> ((val = <span class="built_in">read_from_remote</span>())) &#123;  </span><br><span class="line">		mu.<span class="built_in">lock</span>(); <span class="comment">// (1)  </span></span><br><span class="line">		queue.<span class="built_in">push_back</span>(val); <span class="comment">// (2)  </span></span><br><span class="line">		mu.<span class="built_in">unlock</span>(); <span class="comment">// (3)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (<span class="literal">true</span>) &#123;  </span><br><span class="line">		mu.<span class="built_in">lock</span>(); <span class="comment">// (4)  </span></span><br><span class="line">		cout &lt;&lt; queue.<span class="built_in">front</span>() &lt;&lt; endl;  </span><br><span class="line">		queue.<span class="built_in">pop_front</span>(); <span class="comment">// (5)  </span></span><br><span class="line">		mu.<span class="built_in">unlock</span>(); <span class="comment">// (6)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>两个线程并发运行, <code>thread1</code> 往队列里写入数据, <code>thread2</code> 从队列里读出数据. 入队操作 (2) 可能需要复制数据, 移动指针, 甚至 resize 队列, 因此我们要保证获取到锁时, 这些操作的结果完全可见. 出队操作也是同理. 所以自旋锁要保证 unlock 操作 “synchronizes-with” lock 操作, 保证锁保护的数据是完整的.</p>
<p>我们可以用 acquire-release 模型实现自旋锁. 下面是一个简单的实现:</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">spinlock</span> &#123;  </span><br><span class="line">	std::atomic&lt;<span class="type">bool</span>&gt; flag&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line">	<span class="keyword">public</span>:  </span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">		<span class="keyword">while</span> (flag.<span class="built_in">exchange</span>(<span class="literal">true</span>, std::memory_order_acquire)); <span class="comment">// (1)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">		flag.<span class="built_in">store</span>(<span class="literal">false</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>上面的实现中, (1) 处加锁用到的 <code>exchange</code> 是一种 read-modify-write 操作, 它将目标值 (第一个参数) 写入原子变量, 并返回写入前的值. 在这个实现中, 锁被占用时 <code>flag</code> 为 <code>true</code>. 如果锁被占用, (1) 处的 exchange 操作会一直返回 <code>true</code>, 线程阻塞在循环中; 直到锁被释放, <code>flag</code> 为 <code>false</code>, exchange 操作将 <code>flag</code> 重新置为 <code>true</code> 以抢占锁, 并且返回其原来的值 <code>false</code>, 循环退出, 加锁成功. 解锁则很简单, 将 <code>flag</code> 置为 <code>false</code> 即可.</p>
<p>由于解锁操作使用 <code>memory_order_release</code> 且加锁操作使用 <code>memory_order_acquire</code>, 所以能保证加锁成功时与上一次解锁操作构成 “synchronizes-with” 的关系, 也就是 unlock 操作 “synchronizes-with” lock 操作.</p>
<p>加锁时的 exchange 操作是一个 read-modify-write 操作, 它既读又写. 当它使用 <code>memory_order_acquire</code> 时, 只能保证它读的部分是一个 acquire 操作. 如果有两个线程抢占同一个锁</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">spinlock mu;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="comment">// some operations  </span></span><br><span class="line">	mu.<span class="built_in">lock</span>(); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	mu.<span class="built_in">lock</span>(); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(1) 和 (2) 之间没有任何同步关系, 假设先执行操作 (1) 后执行操作 (2), 那么 <code>thread1</code> 中 (1) 之前的操作结果不一定对 <code>thread2</code> 可见. 但能确定的是, 只会有一个线程得到锁, 这是由原子变量的修改顺序 (modification order) 所保证的. 要么 <code>thread1</code> 先将 <code>flag</code> 置为 <code>true</code>, 要么 <code>thread2</code> 先将 <code>flag</code> 置为 <code>true</code>, 这个顺序是全局一致的.</p>
</li>
</ul>
<h2 id="性能评估"><a class="markdownIt-Anchor" href="#性能评估"></a> 性能评估</h2>
<h3 id="atomics-code-gen-for-x86x64"><a class="markdownIt-Anchor" href="#atomics-code-gen-for-x86x64"></a> Atomics Code Gen for x86/x64</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135006264.png" alt="image.png" /></p>
<p>在 x86/x64 处理器上：</p>
<ul>
<li>Reads are not reordered with other reads. <strong>load自带acquire语义</strong></li>
<li>Writes are not reordered with other writes (with some minor exceptions). <strong>write自带release语义</strong></li>
<li>Writes are not reordered with older reads.</li>
<li>Reads may be reordered with older writes to different locations. <strong>可以理解为x86处理器没有out of order. 但是由于芯片里有store buffer, 后面的load可能会超越之前的store (因为存在store buffer里), 看上去好像out of order了. 因此使用强一致性的store时, 会加上一个mfence, 防止后面的load读不到store的值.</strong></li>
</ul>
<h3 id="测试程序"><a class="markdownIt-Anchor" href="#测试程序"></a> 测试程序</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135016920.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135022786.png" alt="image.png" /></p>
<h3 id="性能综述"><a class="markdownIt-Anchor" href="#性能综述"></a> 性能综述</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135027809.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135033175.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135039124.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135046595.png" alt="image.png" /></p>
<h2 id="总结-2"><a class="markdownIt-Anchor" href="#总结-2"></a> 总结</h2>
<ul>
<li><code>memory_order_relaxed</code>: 最宽松的内存顺序, 只保证操作的<strong>原子性</strong>和<strong>修改顺序 (modification order)</strong>.</li>
<li><code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code>: 实现 <strong>acquire 操作</strong>和 <strong>release 操作</strong>, 如果 acquire 操作读到了 release 操作写入的值, 或其 release sequence 写入的值, 则构成 <strong>synchronizes-with 关系</strong>, 进而可以推导出 <strong>happens-before 的关系</strong>.</li>
<li><code>memory_order_consume</code>: 实现 <strong>consume 操作</strong>, 能实现数据依赖相关的同步关系. 如果 consume 操作读到了 release 操作写入的值, 或其 release sequence 写入的值, 则构成 <strong>dependency-ordered before 的关系</strong>, 对于有数据依赖的操作可以进而推导出 <strong>happens-before 的关系</strong>.</li>
<li><code>memory_order_seq_cst</code>: 加强版的 acquire-release 模型, 除了可以实现 <strong>synchronizes-with 关系</strong>, 还保证<strong>全局顺序一致</strong>.</li>
</ul>
<h1 id="memory-order-vs-memory-model-vs-cache-coherence"><a class="markdownIt-Anchor" href="#memory-order-vs-memory-model-vs-cache-coherence"></a> Memory Order v.s. Memory Model v.s. Cache Coherence</h1>
<ul>
<li><strong>Memory order</strong><br />
因为每个 CPU 硬件平台提供的内存一致性模型不一样（比如 X86 是 TSO 模型，而 arm 则是弱内存模型），因此默认情况下，每个 CPU 执行指令期间允许的内存乱序情况是不一样的。编程语言在语言层面上都提供了一些接口，能够忽略compiler，CPU arch的不同对多线程编程的影响。</li>
<li><strong>Memory Model</strong><br />
处理器执行期间的指令重排，注重的是全局的memory order，是保证多处理器编程中的正确性，我们在讨论这个的时候可以把cache当做一个黑盒子来处理，也就是说即使没有cache，我们也同样需要memory consistency来保证正确性。</li>
<li><strong>Cache Coherence</strong><br />
处理器执行期间因为缓存不一致引起的数据不可见，关注于一个memory location。</li>
</ul>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态总结</title>
    <url>/2025/01/16/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161802558.png" alt="image.png" /></p>
<p><strong>VIT</strong>: 多模态学习之前, 都是Oscar或者Uniter这些工作, 他们的缺陷都是因为里面用了一个Object Detection的模型去做视觉特征的抽取, 这种方法就太慢而且太贵了</p>
<span id="more"></span>
<p><strong>Vilt</strong>: 在Vision Transformer出来之后, Vilt的作者就想到在多模态任务中可以把Vision这边用一个Vision Transformer去代替(一个Embedding层就足够了), 这样大大的简化了模型结构</p>
<p><strong>ALBEF</strong>: ViLT和Clip都是ICML21的工作, ALBEF作者发现, Clip比较高效, 适合做Image Text Retrieval, 原始的这些方法(Oscar或者Uniter这些工作)因为Modality Fusion做得好, 所以多模态任务非常强, 而ViLT的结构比较简单, 最后综合三家的长处推出了ALBEF这么一个Fusion Encoder的模式, 取得了不错的结果. 因为ALBEF也release了代码, 而且结果不错, 模型也比较简单, 所以在它之上又延伸出来了很多工作,</p>
<p><strong>Coca</strong>: SimVLM之前是用Encoder Decoder去做多模态的, SimVLM的作者在ALBEF的基础上就用Contrast和Captioning的两个Loss去训练出来了非常强大的模型</p>
<p><strong>VLMO</strong>: 有了ALBEF之后, 微软的研究者推出了VLMO, 用共享参数的方式推出一个统一的做多模态的框架,</p>
<p><strong>Blip</strong>: 基于这种参数共享的思想, ALBEF的作者又推出了Blip模型, 能做非常好的Captioning的功能, 而且它的Caption Filter模型, 也非常的好用, 能够像一个普适的工具一样用到各种各样的情形中去</p>
<p><strong>VIT</strong>: Vision Transformer在文章中也做了用Mask Data Modeling的方式去做Self-Supplied Learning, 但是当时的效果不是很好</p>
<p><strong>BEIT</strong>: 顺着Bert的思想, 微软的研究者就提出BEIT</p>
<p><strong>BEITv2</strong>: 在BEIT的基础上, 很快又推出了BEITv2, 但是主要是做视觉Task的, 并不是做多模态的</p>
<p><strong>Vision Language BEIT:</strong> 因为BEIT可以在视觉上做Mask Modeling, Bert可以在文本上做Mask Modeling, 作者就想视觉和文本是不是可以合在一起, 所以又推出了Vision Language BEIT</p>
<p><strong>BEITv3</strong>: 在一系列的实验经验的积累之下, 作者最后把VLMO, VL-BEIT和BEITv2, 三个工作合起来, 推出了多模态的BEITv3, 大幅超过了之前CoCa, Blip在单模态和多模态上的各种表现</p>
<p><strong>VIT</strong>: 对于Mask的Data Modeling, 可以Mask and Predict不同的东西. 比如<strong>BEIT</strong>就是Predict <strong>Patch, MAE</strong>是Mask Predict <strong>Pixel,</strong> 当然不论是恢复Patch还是恢复Pixel, 其实Vision Transformer这篇paper, 原来都已经做了, 但是效果都不是很好, BEIT和MAE都把效果推到一个非常高的高度</p>
<p><strong>MAE</strong>: Mask Auto Encoder是Mask Predict <strong>Pixel,</strong> MAE有一个非常好的特性就是在视觉那端把大量的Patch全都给Mask掉之后, 只把那些没有Mask掉过的Patch扔给了Vision Transformer去学习, 这样就大大减少了计算量</p>
<p><strong>Flip</strong>: Fast Language Image Prediction把MAE的有用的特性用到Clip的结构里, 模型就是Clip没有任何的改变, 只不过是在视觉这端跟MAE一样都是只用那些没有Mask的Token, 把那些Mask的Token就扔掉了, 这样无形之中就把Sequence Length降低了很多, 所以训练就快了, 也就是他说的Fast Language Image Prediction</p>
<p><strong>一些建议</strong>: 有的代码库是封装的比较好，所以对于好的论文和代码库，看是不够的，必须得上手跑。从main.py里的第一行开始，一行一行往下跑，遇到function就跳过去，再一行一行跑，对照着paper和implementation details去理解。一般一个细分领域，好的代码库跑3<sub>5个就足够了解了，然后好的论文可能5</sub>20篇。再多的论文基本就是看看有什么区别和联系，找点insight就可以了。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>并行无锁数据结构(下) - 跳表, LRU Cache, FIFO队列</title>
    <url>/2024/12/16/%E5%B9%B6%E8%A1%8C%E6%97%A0%E9%94%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%B8%8B-%E8%B7%B3%E8%A1%A8-LRU-Cache-FIFO%E9%98%9F%E5%88%97/</url>
    <content><![CDATA[<h1 id="skip-list"><a class="markdownIt-Anchor" href="#skip-list"></a> Skip List</h1>
<h2 id="基础概念"><a class="markdownIt-Anchor" href="#基础概念"></a> 基础概念</h2>
<p>跳表比起树, 对并行化更友好<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215544008.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="并行跳表的两种实现"><a class="markdownIt-Anchor" href="#并行跳表的两种实现"></a> 并行跳表的两种实现</h2>
<ol>
<li>直面并行性的问题，硬解各种冲突。由于跳表可以看成是多个链表的集合，所以每种并行单链表算法都能衍生出一个并行跳表算法。代表算法是：Fraser Skip List，Fomitchev Skip List和Lazy Skip List。</li>
<li>规避并行性，把塔的升降交给单独线程来管理，代表算法是：No-Hot-Spot Skip List和 Rotating Skip List。</li>
</ol>
<h3 id="fraser-skip-list"><a class="markdownIt-Anchor" href="#fraser-skip-list"></a> Fraser Skip List</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215558305.png" alt="image.png" /></p>
<ul>
<li>每一层链表都是独立的，可以利用Harris List做到无锁。</li>
<li>新节点先插入最底层链表，然后逐级往上长高，每长高一级就是往上一级链表中插入一个新节点。每次插入都是无锁的。长高的过程可以逐级进行，不需要整体上保持原子性，但必须从下往上升。释放节点必须从上往下逐级降低，反其道而行。</li>
<li>Fraser Skip List潜在的问题：塔下降时不能保证上层链表是下层链表的子集。</li>
</ul>
<h3 id="no-hot-spot-skip-list"><a class="markdownIt-Anchor" href="#no-hot-spot-skip-list"></a> No-Hot-Spot Skip List</h3>
<h4 id="insert"><a class="markdownIt-Anchor" href="#insert"></a> Insert</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215604974.png" alt="image.png" /></p>
<p>新节点只插入Base链表。然后由一个辅助线程在适当的时候对跳表进行整理。整理的过程就是把每个节点长高。这个整理是由单线程完成，因此不必考虑写写冲突，而且塔高度的分配是确定的。</p>
<h4 id="remove"><a class="markdownIt-Anchor" href="#remove"></a> Remove</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215614936.png" alt="image.png" /></p>
<h3 id="performance-evaluation"><a class="markdownIt-Anchor" href="#performance-evaluation"></a> Performance Evaluation</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215623612.png" alt="image.png" /></p>
<h1 id="lru-cache"><a class="markdownIt-Anchor" href="#lru-cache"></a> LRU Cache</h1>
<h3 id="key-value-cache"><a class="markdownIt-Anchor" href="#key-value-cache"></a> Key-Value Cache</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215632441.png" alt="image.png" /></p>
<ul>
<li>每个桶都是一个跳表，插入和查找并行，支持标记删除（假设valuesize远大于keysize)。</li>
<li>当一个跳表中被标记的节点太多时，采取如下三步做物理删除：
<ol>
<li>创建一个新的跳表1a,把跳表1中的节点拷贝到1a(指针拷贝)。</li>
<li>将指向bucket1的指针p从1切换到1a,切换需要加锁，且导致少量数据丢失。</li>
<li>释放老的bucket1。</li>
</ol>
</li>
</ul>
<h3 id="cache替换策略-近似lru"><a class="markdownIt-Anchor" href="#cache替换策略-近似lru"></a> Cache替换策略 - 近似LRU</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215638764.png" alt="image.png" /></p>
<ul>
<li>每个节点包含一个count,初值是100。该节点每次被用户读到，它的count加一。</li>
<li>单独的线程在适当的时候遍历所有节点，把每个节点的count减少delta,如果结果小于等于0，就逻辑删除该节点。</li>
<li>delta初始值是1。每次遍历后，线程都将知道Cache中count最小的n个节点，在这些count中挑选一个作为下一次的delta(如果想加快回收，可以把delta调大）。</li>
<li>对count规定一个上限（比如1000），到达1000以后count就不再增加。防止短时间内大量访问导致该节点在变冷后很长时间不能删除。</li>
</ul>
<h3 id="有关-cache-的结论"><a class="markdownIt-Anchor" href="#有关-cache-的结论"></a> 有关 Cache 的结论</h3>
<ul>
<li>只有一个临界区，而且很小（几条赋值指令），不常发生。</li>
<li>Cache的整体性能接近无锁，同时避免了内存管理的复杂性，实现简单，正确性容易推理。</li>
<li>在新跳表替换老跳表的瞬间，可能有数据丢失，但是对于Cache不是问题。（如果愿意，可以在替换后再次遍历老跳表，补回丢失的数据）。</li>
<li>使用count的方法，替换策略近似LRU,但不必维护全局LRU链表，因此并行度高。</li>
</ul>
<h1 id="fifo-queue"><a class="markdownIt-Anchor" href="#fifo-queue"></a> FIFO Queue</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215646802.png" alt="image.png" /></p>
<p>功能（没有查询操作，比单链表简单）：</p>
<ol>
<li>从头删除</li>
<li>从尾插入<br />
需要解决2个问题：</li>
<li>空队列（空队列的插入和删除互相冲突）</li>
<li>ABA问题<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215652387.png" alt="image.png" /></li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215658408.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>并行无锁数据结构(上) - 基本概念, 链表</title>
    <url>/2024/12/16/%E5%B9%B6%E8%A1%8C%E6%97%A0%E9%94%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%B8%8A-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<h1 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h1>
<h2 id="fine-grained-lock-vs-lock-free"><a class="markdownIt-Anchor" href="#fine-grained-lock-vs-lock-free"></a> Fine-grained Lock vs. Lock Free</h2>
<ul>
<li><strong>Fine-grained lock</strong><br />
Slow down all the threads if one thread  is stuck inside the critical section.  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">spin_lock.<span class="built_in">lock</span>();</span><br><span class="line">x= new_value; <span class="comment">// </span></span><br><span class="line">spin_lock.<span class="built_in">unlock</span>();</span><br></pre></td></tr></table></figure>
</li>
<li><strong>Lock Free</strong>
<ul>
<li>no stuck in critical section (critical section is a single atomic operation)</li>
<li>If one thread fails at CAS, there must be another thread succeeds at the CAS (system-wide progress).</li>
</ul>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> expected = x.<span class="built_in">load</span>();</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">	<span class="type">const</span> <span class="type">bool</span> ok = x.<span class="built_in">compare_exchange_strong</span>(expected, new_value);</span><br><span class="line">&#125; <span class="keyword">while</span> (!ok);</span><br><span class="line"><span class="comment">// x.compare_exchange_strong(expected, new_value) 的意思是：</span></span><br><span class="line"><span class="comment">// 如果 x 的当前值等于 expected，就把 new_value 赋值给 x，并且返回 true</span></span><br><span class="line"><span class="comment">// 否则把 x 的当前值传给 expected，返回 false</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<span id="more"></span>
<p>在第一种情况中，如果正好在临界区中触发了一次线程的切换，另一个被切换到的线程得不到这个锁，让跑满了一个时间片，但又做不了任何事情， 10ms后才被强制切换走。在第二种情况中，用了CAS这个原子指令，则不会因为线程切换而产生类似的情况</p>
<h2 id="non-blocking-vs-lock-free-vs-wait-free"><a class="markdownIt-Anchor" href="#non-blocking-vs-lock-free-vs-wait-free"></a> Non-blocking vs. Lock-free vs. Wait-free</h2>
<p>1. Non-blocking: Failure or suspension of any thread cannot cause failure or suspension of another thread.<br />
理解：一个线程在临界区被切换走不会导致另一个线程被卡住<br />
2. Lock-free: A non-blocking algorithm is lock-free if there is a guaranteed system-wide progress.<br />
理解：还能保证整个系统在宏观层面一直在 make progress<br />
3. Wait-free: A non-blocking algorithm is wait-free if there is a guaranteed per-thread progress.<br />
理解：还能保证每个线程一直在 make progress，比如保证能在一定次数之后一定能成功一次，但是实现起来非常复杂。</p>
<h2 id="并行数据结构"><a class="markdownIt-Anchor" href="#并行数据结构"></a> 并行数据结构</h2>
<ol>
<li><strong>Concurrent Search Data Structure(CSDS)</strong>
<ol>
<li>KV容器，三个接口：查找，插入和删除。
<ul>
<li>查找操作的核心是搜索数据结构，比如遍历链表。</li>
<li>插入和删除的过程可以分为两阶段：首先也是对数据结构进行搜索，找到正确的位置，然后完成插入或者删除的动作。</li>
</ul>
</li>
<li>例子：链表（linked list）,跳表（skip list）,哈希表（hash table）,查找树（search tree）和缓存（Cache）<br />
<strong>高性能 CSDS 的关键</strong>
<ol>
<li><strong>搜索快</strong>。搜索过程应该尽量避免（最好完全避免）耗时的操作 (写操作，memory barrier，原子操作），等待和重试（retry）。</li>
<li><strong>细粒度</strong>。修改应该尽可能涉及较小范围。如果CSDS采用锁来实现，就意味着细粒度锁。</li>
</ol>
</li>
</ol>
</li>
<li><strong>非查找数据结构</strong>
<ol>
<li>例子：队列(queue)和栈(stack)</li>
</ol>
</li>
</ol>
<h1 id="singly-linked-list-memory-management"><a class="markdownIt-Anchor" href="#singly-linked-list-memory-management"></a> Singly-Linked List &amp; Memory Management</h1>
<h2 id="回顾单链表"><a class="markdownIt-Anchor" href="#回顾单链表"></a> 回顾单链表</h2>
<ul>
<li>
<p>插入节点<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214932956.png" alt="image.png" /></p>
<ol>
<li>把20的next指针指向30</li>
<li>用cas指令判断10的next指针是否还等于30，如果是的话则改成20，防止在修改的过程中另一个线程也想在10之后插入节点</li>
</ol>
</li>
<li>
<p>删除节点<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214941144.png" alt="image.png" /></p>
<ol>
<li>把head的next指向10后面的节点30，同样用cas保护</li>
</ol>
</li>
</ul>
<h2 id="并行单链表的问题"><a class="markdownIt-Anchor" href="#并行单链表的问题"></a> 并行单链表的问题</h2>
<h3 id="并行单链表的问题1-插入和删除冲突"><a class="markdownIt-Anchor" href="#并行单链表的问题1-插入和删除冲突"></a> 并行单链表的问题1 – 插入和删除冲突</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215112033.png" alt="image.png" /></p>
<p>插入新节点(20)的同时，这个新节点的前驱节点(10)正在被删除。虽然新节点的插入动作完成，但是新节点实际没有插入到链表。</p>
<h3 id="并行单链表的问题2-删除和删除冲突"><a class="markdownIt-Anchor" href="#并行单链表的问题2-删除和删除冲突"></a> 并行单链表的问题2 – 删除和删除冲突</h3>
<ol>
<li>
<p>一个节点(10)被删除的同时，它的后继节点(20)也在被删除。虽然后继节点(20)的删除动作完成，但是后继节点(20)没有真的从链表中删除。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215000078.png" alt="image.png" /></p>
</li>
<li>
<p>一个节点(20)被删除的同时，它的前驱节点(10)也在被删除。等前驱节点的删除动作完成后，节点20又回到链表中。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215143246.png" alt="image.png" /></p>
</li>
</ol>
<h3 id="并行单链表的问题3-内存管理问题"><a class="markdownIt-Anchor" href="#并行单链表的问题3-内存管理问题"></a> 并行单链表的问题3 – 内存管理问题</h3>
<p>线程A把节点10从链表摘除，但是线程B还在访问节点10。线程A不能马上释放节点10的内存，必须等到线程B不再访问节点10，才能释放节点10的内存。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215211226.png" alt="image.png" /></p>
<h3 id="并行单链表的问题4-aba问题"><a class="markdownIt-Anchor" href="#并行单链表的问题4-aba问题"></a> 并行单链表的问题4 – ABA问题</h3>
<p>新建的节点复用了被删除节点的内存<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215223856.png" alt="image.png" /></p>
<h3 id="并行单链表的问题5-线性化-linearizability1"><a class="markdownIt-Anchor" href="#并行单链表的问题5-线性化-linearizability1"></a> 并行单链表的问题5 – 线性化 (Linearizability)（1）</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215230339.png" alt="image.png" /></p>
<ul>
<li>t0：读者停留在Head</li>
<li>t1：读者停留在20</li>
<li>t2：读者停留在20，另一个线程插入10</li>
<li>t3：读者停留在20，另一个线程插入30</li>
<li>t4：读者停留在30<br />
这样，读者看到(20,30)没有出现在历史上。<strong>读者看到的是一个不曾存在的链表。</strong></li>
</ul>
<h3 id="并行单链表的问题5-线性化-linearizability2"><a class="markdownIt-Anchor" href="#并行单链表的问题5-线性化-linearizability2"></a> 并行单链表的问题5 – 线性化 (Linearizability)（2）</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215241469.png" alt="image.png" /></p>
<p>节点10的next指针被设置为nullptr，游标可能认为到了链表尾，会错过节点20和30。<br />
解决方法：</p>
<ol>
<li>删除节点不要把它的 next 指针清空。</li>
<li>游标遍历要找到 Tail 才能认为到了尾。</li>
</ol>
<h3 id="问题总结"><a class="markdownIt-Anchor" href="#问题总结"></a> 问题总结</h3>
<ol>
<li>插入和删除冲突：有解(Harris List)</li>
<li>删除和删除冲突：有解(HarrisList)</li>
<li>内存管理问题：解法不完美(RCU和风险指针：复杂难用；引用计数：性能差)</li>
<li>ABA问题：解法不完美(tagged pointer)</li>
<li>线性化问题：按照CSDS的定义，单链表只提供三个操作：任意位置的插入、任意位置的删除和全链表范围内的查找，并且保证这三个操作是线性化的。<strong>不提供迭代器的功能，无论如何都做不到。</strong></li>
</ol>
<h3 id="观察和思路"><a class="markdownIt-Anchor" href="#观察和思路"></a> 观察和思路</h3>
<p><strong>观察</strong>：都是删除惹的祸。<br />
<strong>思路</strong>：</p>
<ol>
<li>节点不删除（可先标记再集中批量GC,GC加锁）。
<ul>
<li>查找、插入和标记都很快，但是在GC时会短暂阻塞，造成长尾。</li>
</ul>
</li>
<li>节点可删除，但是不要从内存释放（可重用，比如放在内存池）。
<ul>
<li>查找、插入和删除都很快，但是内存不断增长。</li>
</ul>
</li>
<li>节点可删除，节点内存可释放。
<ul>
<li>查找、插入和删除性能降低，或者实现复杂度大（且依赖于sys_membarrier)。</li>
</ul>
</li>
</ol>
<h3 id="回顾并行单链表的问题1和2"><a class="markdownIt-Anchor" href="#回顾并行单链表的问题1和2"></a> 回顾并行单链表的问题1和2</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215248763.png" alt="image.png" /></p>
<p><strong>根本问题</strong>：在决定要删除节点10之后，节点10的next指针发生了变化。如果能够在删除节点10之前把节点10的next指针锁定（不允许其变化），就能避免上述问题。</p>
<h2 id="harris-list"><a class="markdownIt-Anchor" href="#harris-list"></a> Harris List</h2>
<h3 id="remove"><a class="markdownIt-Anchor" href="#remove"></a> Remove</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215254621.png" alt="image.png" /></p>
<p>用”两阶段”的方法删除节点（以删除节点20为例）：</p>
<ol>
<li>标记节点20的next指针。</li>
<li>修改前驱节点（节点10）的next指针，指向后继节点30。1比特标志放在next指针中，读写指针和标志可以做到原子。一旦标记，指针不能修改。Harris List解决了问题1和2。</li>
</ol>
<h4 id="example-high-level"><a class="markdownIt-Anchor" href="#example-high-level"></a> Example - High Level</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215301481.png" alt="image.png" /></p>
<ol>
<li>节点20要被删除，20的next被打上标记</li>
<li>节点10要被删除，10的next被打上标记</li>
<li>节点20删除的过程中，要把节点10的next指向30，但是发现10的next被打上标记，失败</li>
<li>节点10删除的过程中，要把head的next指向20，但是发现20的next已经被打上标记，因此顺水推舟，把head的next指向30</li>
</ol>
<h4 id="example-low-level"><a class="markdownIt-Anchor" href="#example-low-level"></a> Example - Low Level</h4>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215308319.png" alt="image.png" /></p>
<p>解释：</p>
<ul>
<li>Remove step 2：给节点20的next打上标记</li>
<li>Remove step 3：给节点20的prev的next赋值为30</li>
<li>Insert step 4：给节点25的prev的next赋值为25</li>
<li>Remove step 2 和 Insert step 4一定有一个全局序
<ul>
<li>Remove step 2 先执行：Insert step 4失败，因为20的next已经被打上标记，因此remove成功，insert失败</li>
<li>Insert step 4 先执行：Remove step 2失败，因为20的next已经被修改了，因此insert成功，remove失败</li>
</ul>
</li>
</ul>
<h3 id="rcu-like-solution-for-memory-management"><a class="markdownIt-Anchor" href="#rcu-like-solution-for-memory-management"></a> RCU-like Solution for Memory Management</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215314964.png" alt="image.png" /></p>
<ul>
<li>每个线程有一个时间戳，在线程访问链表前，把当前时间记录在时间戳，在访问完成后清除时间戳。</li>
<li>每个线程有一个free list,节点10从链表摘除后，被放到当前线程的free list, 并记下当前时间t0。</li>
<li>当其它所有线程的时间戳都大于t0,可以把节点10从内存释放。<br />
<strong>这个方法好不好？</strong></li>
<li>这个方法很难实现，因为在不同core上读到的时间戳可能会有diff，很难保证时钟是严格递增的</li>
<li>ta, tb是全局的变量，怎么存在数据结构中，如果是个map，那线程id作为key，拿线程id也是额外操作，写map也需要锁，带来开销，因为要保证全局可见</li>
<li>动态库和主程序各自有一套全局变量，可能导致未必能看到这个全局变量</li>
</ul>
<h2 id="michael-list"><a class="markdownIt-Anchor" href="#michael-list"></a> Michael List</h2>
<p>Michael List是 Harris List的改进版，它有两个算法。</p>
<ol>
<li>使用tagged pointer来解决ABA问题。采用 freelist 解决内存管理问题，实际上就是永远不释放内存。如果内存永远不释放，节点从链表摘除后依然可以访问，这时就只需要解决ABA问题。
<ul>
<li>tagged pointer：之所以出现ABA问题，是复用了一块内存，并且next指向的位置也是一样的，解决方法把一个指针64bit中拿出16bit做版本号，这16个bit每次指针做改变时都加一，这样就算两个指针用的内存是一样的，但是版本号是不一样的。<br />
<strong>这个方法好不好？</strong>
<ul>
<li>48bit做内存地址不一定够用，但是给多了版本号又不一定够用了。</li>
</ul>
</li>
</ul>
</li>
<li>采用风险指针来解决ABA和内存管理问题（不需要tagged pointer)。</li>
</ol>
<h3 id="风险指针hazard-pointers解决问题3和4"><a class="markdownIt-Anchor" href="#风险指针hazard-pointers解决问题3和4"></a> 风险指针（Hazard Pointers）解决问题3和4</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215325302.png" alt="image.png" /></p>
<ul>
<li>每个线程把自己正在访问的节点的地址存放在全局可见的地方，比如线程A正在访问两个节点，它们的地址是P1和P2,这些全局可见的指针称为风险指针。线程在访问完这些节点之后，清除自己的风险指针。</li>
<li>每个线程有一个free list,节点10从链表摘除后，被放到当前线程的freelist。然后检查所有其它线程的风险指针，看有没有和节点10的地址相同的，如果没有，说明没有其它人正在访问节点10，节点10可以从内存释放。<br />
<strong>正确性</strong><br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215330139.png" alt="image.png" /><br />
<strong>这个方法好不好？</strong></li>
<li>读者性能很差</li>
</ul>
<h2 id="lazy-list"><a class="markdownIt-Anchor" href="#lazy-list"></a> Lazy List</h2>
<h3 id="remove-2"><a class="markdownIt-Anchor" href="#remove-2"></a> Remove</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215343335.png" alt="image.png" /></p>
<p>用”四阶段”的方法删除节点（以删除节点20为例）：</p>
<ol>
<li>对节点10和20加锁。</li>
<li>检查节点10和20没有被标记，而且节点10依然指向节点20。</li>
<li>标记节点20的next指针。</li>
<li>修改前驱节点（节点10）的next指针，指向后继节点30。</li>
</ol>
<h3 id="insert"><a class="markdownIt-Anchor" href="#insert"></a> Insert</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215349219.png" alt="image.png" /></p>
<p>用”三阶段”的方法插入节点（以插入节点15为例）：</p>
<ol>
<li>对节点10和20加锁。</li>
<li>检查节点10和20没有被标记，而且节点10依然指向节点20。</li>
<li>添加节点15。</li>
</ol>
<h3 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215355274.png" alt="image.png" /></p>
<h2 id="shared_ptr"><a class="markdownIt-Anchor" href="#shared_ptr"></a> Shared_ptr</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215400988.png" alt="image.png" /></p>
<p>使用 atomic shared_ptr来解决内存管理的问题。对 Shared_ptr List的批评：</p>
<ol>
<li>在遍历链表时，每走过一个节点，都需要引用计数加1，离开时需要引用计数减1，而原子操作比较慢，违反了CSDS要求搜索快的原则。</li>
<li>采用递归方式释放整个链表，或者链表中的一段，有可能导致栈溢出。</li>
<li>如果一个线程获得一个节点的引用计数，但是被卡住，等了好久才释放这个引用计数，那么该节点的所有后继节点都会被延迟释放。</li>
</ol>
<h2 id="performance-evaluation"><a class="markdownIt-Anchor" href="#performance-evaluation"></a> Performance Evaluation</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215405537.png" alt="image.png" /></p>
<p>为什么lazy list效果好？因为尽管删除和插入操作加了锁，在遍历/搜索时效率变高了</p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<ol>
<li>在不考虑内存管理的情况下，LazyList是目前最好的并行单链表，Michael List是最好的无锁单链表。</li>
<li>在考虑内存管理的情况下，目前没有工程上比较实用的解法。
<ol>
<li>RCU和风险指针实现复杂，使用也不友好。我们等待它们被标准化到C++标准库，并且采用了sys_membarrier优化，这样才具备在工程上广泛使用的基础。</li>
<li>引用计数虽然使用方便，但是性能不好，违反了搜索快的原则。我们等待引用计数将来能够采用类似&quot;weighted reference counting&quot;之类的优化，解决性能问题。</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>混合专家模型（MoE）详解</title>
    <url>/2025/01/26/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88MoE%EF%BC%89%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://huggingface.co/blog/zh/moe">https://huggingface.co/blog/zh/moe</a></p>
</blockquote>
<p>本文也提供英文版本 <a href="https://huggingface.co/blog/moe">English</a>。</p>
<p>随着 Mixtral 8x7B (<a href="https://mistral.ai/news/mixtral-of-experts/">announcement</a>, <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">model card</a>) 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的 Transformer 模型在开源人工智能社区引起了广泛关注。在本篇博文中，我们将深入探讨 MoEs 的核心组件、训练方法，以及在推理过程中需要考量的各种因素。</p>
<h1 id="简短总结"><a class="markdownIt-Anchor" href="#简短总结"></a> 简短总结</h1>
<p>混合专家模型 (MoEs):</p>
<ul>
<li>
<p>与稠密模型相比， <strong>预训练速度更快</strong></p>
</li>
<li>
<p>与具有相同参数数量的模型相比，具有更快的 <strong>推理速度</strong></p>
</li>
<li>
<p>需要 <strong>大量显存</strong>，因为所有专家系统都需要加载到内存中</p>
</li>
<li>
<p>在 <strong>微调方面存在诸多挑战</strong>，但 <a href="https://arxiv.org/pdf/2305.14705.pdf">近期的研究</a> 表明，对混合专家模型进行 <strong>指令调优具有很大的潜力</strong>。</p>
</li>
</ul>
<span id="more"></span>
<h1 id="什么是混合专家模型"><a class="markdownIt-Anchor" href="#什么是混合专家模型"></a> 什么是混合专家模型？</h1>
<p>模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。</p>
<p>混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p>
<p>那么，究竟什么是一个混合专家模型 (MoE) 呢？作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p>
<ul>
<li>
<p><strong>稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。</p>
</li>
<li>
<p><strong>门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126102106146.png" alt="image.png" /></p>
<p><a href="https://arxiv.org/abs/2101.03961">Switch Transformers paper</a> 论文中的 MoE layer</p>
<p>总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。</p>
<p>尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:</p>
<ul>
<li>
<p><strong>训练挑战</strong>: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。</p>
</li>
<li>
<p><strong>推理挑战</strong>: MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高。以 Mixtral 8x7B 这样的 MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个令牌只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。</p>
</li>
</ul>
<p>了解了 MoE 的基本概念后，让我们进一步探索推动这类模型发展的研究。</p>
<h1 id="混合专家模型简史"><a class="markdownIt-Anchor" href="#混合专家模型简史"></a> 混合专家模型简史</h1>
<p>混合专家模型 (MoE) 的理念起源于 1991 年的论文 <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">Adaptive Mixture of Local Experts</a>。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。</p>
<p>在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:</p>
<ol>
<li>
<p><strong>组件专家</strong>: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，<a href="https://arxiv.org/abs/1312.4314">Eigen、Ranzato 和 Ilya 的研究</a> 探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。</p>
</li>
<li>
<p><strong>条件计算</strong>: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。</p>
</li>
</ol>
<p>这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。特别是在 2017 年，<a href="https://arxiv.org/abs/1701.06538">Shazeer 等人</a> (团队包括 Geoffrey Hinton 和 Jeff Dean，后者有时被戏称为 <a href="https://www.informatika.bg/jeffdean">“谷歌的 Chuck Norris”</a>) 将这一概念应用于 137B 的 LSTM (当时被广泛应用于 NLP 的架构，由 Schmidhuber 提出)。通过引入稀疏性，这项工作在保持极高规模的同时实现了快速的推理速度。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126102117210.png" alt="image.png" /></p>
<p>Outrageously Large Neural Network 论文中的 MoE layer</p>
<p>混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种技术不仅在自然语言处理 (NLP) 领域得到了广泛应用，也开始在计算机视觉领域进行探索。然而，本篇博客文章将主要聚焦于自然语言处理领域的应用和探讨。</p>
<h1 id="什么是稀疏性"><a class="markdownIt-Anchor" href="#什么是稀疏性"></a> 什么是稀疏性?</h1>
<p>稀疏性的概念采用了条件计算的思想。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。</p>
<p>让我们深入分析 Shazeer 对混合专家模型 (MoE) 在翻译应用中的贡献。条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用。</p>
<p>这种稀疏性设置确实带来了一些挑战。例如，在混合专家模型 (MoE) 中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。比如，假设我们的输入批量包含 10 个令牌， <strong>可能会有五个令牌被路由到同一个专家，而剩下的五个令牌分别被路由到不同的专家。这导致了批量大小的不均匀分配和资源利用效率不高的问题</strong>。在接下来的部分中，将会讨论 <a href="https://huggingface.co/blog/zh/moe#%E8%AE%A9moe%E8%B5%B7%E9%A3%9E">让 MoE 高效运行</a> 的其他挑战以及相应的解决方案。</p>
<p>那我们应该如何解决这个问题呢？一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E):</p>
<p>$ y = \sum_{i=1}^{n} G(x)_i E_i(x) $</p>
<p>在这种设置下，虽然所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。但是，如果 G (门控网络的输出) 为 0 会发生什么呢？如果是这种情况，就没有必要计算相应的专家操作，因此我们可以节省计算资源。那么一个典型的门控函数是什么呢？一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。这个网络将学习将输入发送给哪个专家。</p>
<p>$ G_\sigma(x) = \text{Softmax}(x \cdot W_g) $</p>
<p>Shazeer 等人的工作还探索了其他的门控机制，其中包括带噪声的 TopK 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。具体来说:</p>
<ol>
<li>添加一些噪声</li>
</ol>
<p>$ H(x)<em>i = (x \cdot W</em>{\text{g}})<em>i + \text{StandardNormal()} \cdot \text{Softplus}((x \cdot W</em>{\text{noise}})_i) $</p>
<ol start="2">
<li>选择保留前 K 个值</li>
</ol>
<p>$ \text{KeepTopK}(v, k)_i = \begin{cases} v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v, \ -\infty &amp; \text{otherwise.} \end{cases} $</p>
<ol start="3">
<li>应用 Softmax 函数</li>
</ol>
<p>$ G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k)) $</p>
<p>这种稀疏性引入了一些有趣的特性。通过使用较低的 k 值 (例如 1 或 2)，我们可以比激活多个专家时更快地进行训练和推理。为什么不仅选择最顶尖的专家呢？最初的假设是，需要将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。<a href="https://huggingface.co/blog/zh/moe#switch-transformers">Switch Transformers</a> 就这点进行了更多的研究。</p>
<p>我们为什么要添加噪声呢？这是为了专家间的负载均衡！</p>
<h1 id="混合专家模型中令牌的负载均衡"><a class="markdownIt-Anchor" href="#混合专家模型中令牌的负载均衡"></a> 混合专家模型中令牌的负载均衡</h1>
<p>正如之前讨论的，如果所有的令牌都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。为了缓解这个问题，引入了一个 <strong>辅助损失</strong>，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少令牌的阈值。在 <code>transformers</code> 库中，可以通过 <code>aux_loss</code> 参数来控制辅助损失。</p>
<h1 id="moes-and-transformers"><a class="markdownIt-Anchor" href="#moes-and-transformers"></a> MoEs and Transformers</h1>
<p>Transformer 类模型明确表明，增加参数数量可以提高性能，因此谷歌使用 <a href="https://arxiv.org/abs/2006.16668">GShard</a> 尝试将 Transformer 模型的参数量扩展到超过 6000 亿并不令人惊讶。</p>
<p>GShard 将在编码器和解码器中的每个前馈网络 (FFN) 层中的替换为使用 Top-2 门控的混合专家模型 (MoE) 层。下图展示了编码器部分的结构。这种架构对于大规模计算非常有效: 当扩展到多个设备时，MoE 层在不同设备间共享，而其他所有层则在每个设备上复制。我们将在 <a href="https://huggingface.co/blog/zh/moe#%E8%AE%A9moe%E8%B5%B7%E9%A3%9E">“让 MoE 起飞”</a> 部分对这一点进行更详细的讨论。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/63ee982d277f47c08ce3fd1e134caf46" alt="" /></p>
<p>GShard 论文中的 MoE Transformer Encoder</p>
<p>为了保持负载平衡和训练效率，GShard 的作者除了引入了上一节中讨论的类似辅助损失外，还引入了一些关键变化:</p>
<ul>
<li>
<p><strong>随机路由</strong>: 在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。</p>
</li>
<li>
<p><strong>专家容量</strong>: 我们可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。专家容量是 MoE 中最重要的概念之一。为什么需要专家容量呢？因为所有张量的形状在编译时是静态确定的，我们无法提前知道多少令牌会分配给每个专家，因此需要一个固定的容量因子。</p>
</li>
</ul>
<p>GShard 的工作对适用于 MoE 的并行计算模式也做出了重要贡献，但这些内容的讨论超出了这篇博客的范围。</p>
<p><strong>注意</strong>: 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力 (self-attention) 机制，它适用于所有令牌。这就解释了为什么我们可以使用相当于 12B 稠密模型的计算资源来运行一个包含 8 个专家的 47B 模型。如果我们采用 Top-2 门控，模型会使用高达 14B 的参数。但是，由于自注意力操作 (专家间共享) 的存在，实际上模型运行时使用的参数数量是 12B。</p>
<h1 id="switch-transformers"><a class="markdownIt-Anchor" href="#switch-transformers"></a> Switch Transformers</h1>
<p>尽管混合专家模型 (MoE) 显示出了很大的潜力，但它们在训练和微调过程中存在稳定性问题。<a href="https://arxiv.org/abs/2101.03961">Switch Transformers</a> 是一项非常激动人心的工作，它深入研究了这些话题。作者甚至在 Hugging Face 上发布了一个 <a href="https://huggingface.co/google/switch-c-2048">1.6 万亿参数的 MoE</a>，拥有 2048 个专家，你可以使用 <code>transformers</code> 库来运行它。Switch Transformers 实现了与 T5-XXL 相比 4 倍的预训练速度提升。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/c6824adf518e45ffb8d1bb6221730c48" alt="" /></p>
<p>Switch Transformer 论文中的 Switch Transformer Layer</p>
<p>就像在 GShard 中一样，作者用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的令牌) 并拥有四个专家。</p>
<p>与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:</p>
<ul>
<li>
<p>减少门控网络 (路由) 计算负担</p>
</li>
<li>
<p>每个专家的批量大小至少可以减半</p>
</li>
<li>
<p>降低通信成本</p>
</li>
<li>
<p>保持模型质量</p>
</li>
</ul>
<p>Switch Transformers 也对 <strong>专家容量</strong> 这个概念进行了研究。</p>
<p>$ \text{Expert Capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) \times \text{capacity factor} $</p>
<p>上述建议的容量是将批次中的令牌数量均匀分配到各个专家。如果我们使用大于 1 的容量因子，我们为令牌分配不完全平衡时提供了一个缓冲。增加容量因子会导致更高的设备间通信成本，因此这是一个需要考虑的权衡。特别值得注意的是，Switch Transformers 在低容量因子 (例如 1 至 1.25) 下表现出色。</p>
<p>Switch Transformer 的作者还重新审视并简化了前面章节中提到的负载均衡损失。在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。</p>
<p>作者还尝试了混合精度的方法，例如用 <code>bfloat16</code> 精度训练专家，同时对其余计算使用全精度进行。较低的精度可以减少处理器间的通信成本、计算成本以及存储张量的内存。然而，在最初的实验中，当专家和门控网络都使用 <code>bfloat16</code> 精度训练时，出现了不稳定的训练现象。这种不稳定性特别是由路由计算引起的，因为路由涉及指数函数等操作，这些操作对精度要求较高。因此，为了保持计算的稳定性和精确性，保持更高的精度是重要的。为了减轻不稳定性，路由过程也使用了全精度。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/c3f33b46f6324baa86803e7e7d4b305f" alt="" /></p>
<p>使用混合精度不会降低模型质量并可实现更快的训练</p>
<p>这个 <a href="https://colab.research.google.com/drive/1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharing">Jupyter Notebook</a> 展示了如何对 Switch Transformers 进行微调以进行摘要生成的详细指南。然而，在开始微调 Switch Transformers 之前，强烈建议您先阅读关于 <a href="https://huggingface.co/blog/zh/moe#%E5%BE%AE%E8%B0%83%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B">微调混合专家模型</a> 部分的内容。</p>
<p>Switch Transformers 采用了编码器 - 解码器的架构，实现了与 T5 类似的混合专家模型 (MoE) 版本。<a href="https://arxiv.org/abs/2112.06905">GLaM</a> 这篇工作探索了如何使用仅为原来 1/3 的计算资源 (因为 MoE 模型在训练时需要的计算量较少，从而能够显著降低碳足迹) 来训练与 GPT-3 质量相匹配的模型来提高这些模型的规模。作者专注于仅解码器 (decoder-only) 的模型以及少样本和单样本评估，而不是微调。他们使用了 Top-2 路由和更大的容量因子。此外，他们探讨了将容量因子作为一个动态度量，根据训练和评估期间所使用的计算量进行调整。</p>
<h2 id="用-router-z-loss-稳定模型训练"><a class="markdownIt-Anchor" href="#用-router-z-loss-稳定模型训练"></a> 用 Router z-loss 稳定模型训练</h2>
<p>之前讨论的平衡损失可能会导致稳定性问题。我们可以使用许多方法来稳定稀疏模型的训练，但这可能会牺牲模型质量。例如，引入 dropout 可以提高稳定性，但会导致模型质量下降。另一方面，增加更多的乘法分量可以提高质量，但会降低模型稳定性。</p>
<p><a href="https://arxiv.org/abs/2202.08906">ST-MoE</a> 引入的 <code>Router z-loss</code> 在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 <code>logits</code> 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要。为了深入了解这一机制，建议参考原始论文以获得更全面的细节。</p>
<h1 id="专家如何学习"><a class="markdownIt-Anchor" href="#专家如何学习"></a> 专家如何学习？</h1>
<p>ST-MoE 的研究者们发现，编码器中不同的专家倾向于专注于特定类型的令牌或浅层概念。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。此外，研究者们还对这一模型进行了多语言训练。尽管人们可能会预期每个专家处理一种特定语言，但实际上并非如此。由于令牌路由和负载均衡的机制，没有任何专家被特定配置以专门处理某一特定语言。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/dfee2ddd744b4c03bafbc2f9a65bac59" alt="" /></p>
<p>ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格</p>
<h1 id="专家的数量对预训练有何影响"><a class="markdownIt-Anchor" href="#专家的数量对预训练有何影响"></a> 专家的数量对预训练有何影响？</h1>
<p>增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的增加而递减 (尤其是当专家数量达到 256 或 512 之后更为明显)。同时，这也意味着在推理过程中，需要更多的显存来加载整个模型。值得注意的是，Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。</p>
<h1 id="微调混合专家模型"><a class="markdownIt-Anchor" href="#微调混合专家模型"></a> 微调混合专家模型</h1>
<blockquote>
<p><code>4.36.0</code> 版本的 <code>transformers</code> 库支持 Mixtral 模型。你可以用以下命令进行安装: <code>pip install &quot;transformers==4.36.0 --upgrade</code></p>
</blockquote>
<p>稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p>
<p>在微调过程中是否使用辅助损失是一个需要决策的问题。ST-MoE 的作者尝试关闭辅助损失，发现即使高达 11% 的令牌被丢弃，模型的质量也没有显著受到影响。令牌丢弃可能是一种正则化形式，有助于防止过拟合。</p>
<p>Switch Transformers 的作者观察到，在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。另一方面，对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/7a9141706aff4d0db3501b79ac3ff473" alt="" /></p>
<p>在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。该图来自 ST-MoE 论文</p>
<p>一种可行的微调策略是尝试冻结所有非专家层的权重。实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。我们可以尝试相反的方法: 仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/bc095a327b9a4b78926e5b93b5424d42" alt="" /></p>
<p>通过仅冻结 MoE 层，我们可以在保持质量的同时加快训练速度。该图来自 ST-MoE 论文</p>
<p>在微调稀疏混合专家模型 (MoE) 时需要考虑的最后一个问题是，它们有特别的微调超参数设置——例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/dee66e102a884150bc4d4bb1589eabfd" alt="" /></p>
<p>提高学习率和调小批量可以提升稀疏模型微调质量。该图来自 ST-MoE 论文</p>
<p>此时，您可能会对人们微调 MoE 中遇到的这些挑战而感到沮丧，但最近的一篇论文 <a href="https://arxiv.org/pdf/2305.14705.pdf">《MoEs Meets Instruction Tuning》</a> (2023 年 7 月) 带来了令人兴奋的发现。这篇论文进行了以下实验:</p>
<ul>
<li>
<p>单任务微调</p>
</li>
<li>
<p>多任务指令微调</p>
</li>
<li>
<p>多任务指令微调后接单任务微调</p>
</li>
</ul>
<p>当研究者们对 MoE 和对应性能相当的 T5 模型进行微调时，他们发现 T5 的对应模型表现更为出色。然而，当研究者们对 Flan T5 (一种 T5 的指令优化版本) 的 MoE 版本进行微调时，MoE 的性能显著提升。更值得注意的是，Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升，这意味着 MoE 模型可能从指令式微调中获益更多，甚至超过了稠密模型。此外，MoE 在多任务学习中表现更佳。与之前关闭 <strong>辅助损失</strong> 函数的做法相反，实际上这种损失函数可以帮助防止过拟合。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/2e1ca919702a495d8e223ff82cb0e473" alt="" /></p>
<p>与稠密模型相比，稀疏模型从指令微调中受益更多。该图来自 MoEs Meets instructions Tuning 论文</p>
<h1 id="稀疏-vs-稠密如何选择"><a class="markdownIt-Anchor" href="#稀疏-vs-稠密如何选择"></a> 稀疏 VS 稠密，如何选择?</h1>
<p>稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。</p>
<p><strong>注意</strong>: 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。</p>
<h1 id="让-moe-起飞"><a class="markdownIt-Anchor" href="#让-moe-起飞"></a> 让 MoE 起飞</h1>
<p>最初的混合专家模型 (MoE) 设计采用了分支结构，这导致了计算效率低下。这种低效主要是因为 GPU 并不是为处理这种结构而设计的，而且由于设备间需要传递数据，网络带宽常常成为性能瓶颈。在接下来的讨论中，我们会讨论一些现有的研究成果，旨在使这些模型在预训练和推理阶段更加高效和实用。我们来看看如何优化 MoE 模型，让 MoE 起飞。</p>
<h2 id="并行计算"><a class="markdownIt-Anchor" href="#并行计算"></a> 并行计算</h2>
<p>让我们简要回顾一下并行计算的几种形式:</p>
<ul>
<li>
<p><strong>数据并行</strong>: 相同的权重在所有节点上复制，数据在节点之间分割。</p>
</li>
<li>
<p><strong>模型并行</strong>: 模型在节点之间分割，相同的数据在所有节点上复制。</p>
</li>
<li>
<p><strong>模型和数据并行</strong>: 我们可以在节点之间同时分割模型和数据。注意，不同的节点处理不同批次的数据。</p>
</li>
<li>
<p><strong>专家并行</strong>: 专家被放置在不同的节点上。如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。</p>
</li>
</ul>
<p>在专家并行中，专家被放置在不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的令牌被发送到拥有所需专家的节点。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/5a81ee9bb8c34d93b9634ee1228d5144" alt="" /></p>
<p>Switch Transformers 论文中展示如何使用不同的并行技术在节点上分割数据和模型的插图</p>
<h2 id="容量因子和通信开销"><a class="markdownIt-Anchor" href="#容量因子和通信开销"></a> 容量因子和通信开销</h2>
<p>提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子，同时每个节点配置一个专家。在评估性能时，应根据需要调整容量因子，以在设备间的通信成本和计算成本之间找到一个平衡点。</p>
<h2 id="部署技术"><a class="markdownIt-Anchor" href="#部署技术"></a> 部署技术</h2>
<blockquote>
<p>您可以在 <code>Inference Endpoints</code> 部署 <a href="https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&amp;vendor=aws&amp;region=us-east-1&amp;accelerator=gpu&amp;instance_size=2xlarge&amp;task=text-generation&amp;no_suggested_compute=true&amp;tgi=true&amp;tgi_max_batch_total_tokens=1024000&amp;tgi_max_total_tokens=32000">mistralai/Mixtral-8x7B-Instruct-v0.1</a>。</p>
</blockquote>
<p>部署混合专家模型 (MoE) 的一个关键挑战是其庞大的参数规模。对于本地使用情况，我们可能希望使用更小的模型。为了使模型更适合部署，下面是几种有用的技术:</p>
<ul>
<li>
<p>预先蒸馏实验: Switch Transformers 的研究者们进行了预先蒸馏的实验。他们通过将 MoE 模型蒸馏回其对应的稠密模型，成功保留了 30-40%的由稀疏性带来的性能提升。预先蒸馏不仅加快了预训练速度，还使得在推理中使用更小型的模型成为可能。</p>
</li>
<li>
<p>任务级别路由: 最新的方法中，路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。</p>
</li>
<li>
<p>专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。这样可以在不显著牺牲性能的情况下降低模型的复杂度。</p>
</li>
</ul>
<h2 id="高效训练"><a class="markdownIt-Anchor" href="#高效训练"></a> 高效训练</h2>
<p>FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p>
<p>Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。其核心优势在于，它不会丢弃任何令牌，并能高效地适应现代硬件架构 (支持块稀疏矩阵乘)，从而达到显著的加速效果。Megablocks 的创新之处在于，它不像传统 MoE 那样使用批量矩阵乘法 (这通常假设所有专家形状相同且处理相同数量的令牌)，而是将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配。</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/fe82f7741d2146b1bc249a69fc030b05" alt="" /></p>
<p>针对不同规模的专家和令牌数量的块稀疏矩阵乘法。该图来自 <a href="https://arxiv.org/abs/2211.15841">MegaBlocks</a> 论文</p>
<h1 id="开源混合专家模型"><a class="markdownIt-Anchor" href="#开源混合专家模型"></a> 开源混合专家模型</h1>
<p>目前，下面这些开源项目可以用于训练混合专家模型 (MoE):</p>
<ul>
<li>
<p>Megablocks: <a href="https://github.com/stanford-futuredata/megablocks">https://github.com/stanford-futuredata/megablocks</a></p>
</li>
<li>
<p>Fairseq: <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm">https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm</a></p>
</li>
<li>
<p>OpenMoE: <a href="https://github.com/XueFuzhao/OpenMoE">https://github.com/XueFuzhao/OpenMoE</a></p>
</li>
</ul>
<p>对于开源的混合专家模型 (MoE)，你可以关注下面这些:</p>
<ul>
<li>
<p><a href="https://huggingface.co/collections/google/switch-transformers-release-6548c35c6507968374b56d1f">Switch Transformers (Google)</a>: 基于 T5 的 MoE 集合，专家数量从 8 名到 2048 名。最大的模型有 1.6 万亿个参数。</p>
</li>
<li>
<p><a href="https://huggingface.co/facebook/nllb-moe-54b">NLLB MoE (Meta)</a>: NLLB 翻译模型的一个 MoE 变体。</p>
</li>
<li>
<p><a href="https://huggingface.co/fuzhao">OpenMoE</a>: 社区对基于 Llama 的模型的 MoE 尝试。</p>
</li>
<li>
<p><a href="https://huggingface.co/mistralai">Mixtral 8x7B (Mistral)</a>: 一个性能超越了 Llama 2 70B 的高质量混合专家模型，并且具有更快的推理速度。此外，还发布了一个经过指令微调的模型。有关更多信息，可以在 Mistral 的 <a href="https://mistral.ai/news/mixtral-of-experts/">公告博客文章</a> 中了解。</p>
</li>
</ul>
<h1 id="一些有趣的研究方向"><a class="markdownIt-Anchor" href="#一些有趣的研究方向"></a> 一些有趣的研究方向</h1>
<p>首先是尝试将稀疏混合专家模型 (SMoE) <strong>蒸馏</strong> 回到具有更少实际参数但相似等价参数量的稠密模型。</p>
<p>MoE 的 <strong>量化</strong> 也是一个有趣的研究领域。例如，<a href="https://arxiv.org/abs/2310.16795">QMoE</a> (2023 年 10 月) 通过将 MoE 量化到每个参数不到 1 位，将 1.6 万亿参数的 Switch Transformer 所需的存储从 3.2TB 压缩到仅 160GB。</p>
<p>简而言之，一些值得探索的有趣领域包括:</p>
<ul>
<li>
<p>将 Mixtral 蒸馏成一个稠密模型。</p>
</li>
<li>
<p>探索合并专家模型的技术及其对推理时间的影响。</p>
</li>
<li>
<p>尝试对 Mixtral 进行极端量化的实验。</p>
</li>
</ul>
<h1 id="相关资源"><a class="markdownIt-Anchor" href="#相关资源"></a> 相关资源</h1>
<ul>
<li>
<p><a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">Adaptive Mixture of Local Experts (1991)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1312.4314">Learning Factored Representations in a Deep Mixture of Experts (2013)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2006.16668">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun 2020)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2112.06905">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2202.08906">ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)</a></p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/3503221.3508418">FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models(April 2022)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2211.15841">MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022)</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2305.14705">Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May 2023)</a></p>
</li>
<li>
<p><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">Mixtral-8x7B-v0.1</a>, <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral-8x7B-Instruct-v0.1</a>.</p>
</li>
</ul>
<h1 id="citation"><a class="markdownIt-Anchor" href="#citation"></a> Citation</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@misc &#123;sanseviero2023moe,</span><br><span class="line">    author       = &#123; Omar Sanseviero and</span><br><span class="line">                     Lewis Tunstall and</span><br><span class="line">                     Philipp Schmid and</span><br><span class="line">                     Sourab Mangrulkar and</span><br><span class="line">                     Younes Belkada and</span><br><span class="line">                     Pedro Cuenca</span><br><span class="line">                   &#125;,</span><br><span class="line">    title        = &#123; Mixture of Experts Explained &#125;,</span><br><span class="line">    year         = 2023,</span><br><span class="line">    url          = &#123; https://huggingface.co/blog/moe &#125;,</span><br><span class="line">    publisher    = &#123; Hugging Face Blog &#125;</span><br><span class="line">&#125;</span><br><span class="line">Sanseviero, et al., &quot;Mixture of Experts Explained&quot;, Hugging Face Blog, 2023.</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大模型</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>Llama 3.1-405B 训练</title>
    <url>/2025/01/26/Llama-3-1-405B-%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p><a href="https://scontent-nrt1-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=t6egZJ8QdI4Q7kNvgFMhbmG&amp;_nc_ht=scontent-nrt1-1.xx&amp;oh=00_AYBspddKakesakPWwavuhJerPmZXDEBsnrcTmoxP6-MQSg&amp;oe=66A7944D">技术报告链接</a></p>
<p>计算资源：Llama 3.1-405B 在16,000 个 H100 GPU 上训练，每个 GPU 的运行功率为 700W TDP，配备 80GB HBM3。每台服务器配备8个 GPU 和2个 CPU。在服务器内部，8个 GPU 通过 NVLink 连接。预训练总共用了54天。</p>
<p>Llama 3.1-405B 采用了 <strong>4D 混合并行：TP (张量并行）+ CP (序列并行）+ PP (流水线并行）+ DP (数据并行）,</strong> 配置如下表</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134936971.png" alt="image.png" /></p>
<span id="more"></span>
<h2 id="4d-并行的由来"><a class="markdownIt-Anchor" href="#4d-并行的由来"></a> 4D 并行的由来</h2>
<h3 id="模型并行"><a class="markdownIt-Anchor" href="#模型并行"></a> <strong>模型并行</strong></h3>
<p>模型并行（MP，Model Parallelism）可分为流水线并行（PP，Pipeline Parallelism）和张量并行（TP，Tensor Parallesim），都是解决当GPU放不下一个模型时，而将模型拆分到不同的GPU上的方法。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134945872.png" alt="image.png" /></p>
<h3 id="tp-张量并行"><a class="markdownIt-Anchor" href="#tp-张量并行"></a> <strong>TP (张量并行）</strong></h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134953133.png" alt="image.png" /></p>
<p>优点：可以横向切分矩阵，同时也切分了激活（这样就从参数、优化器和激活3个层面都减少了显存）</p>
<p>缺点：TP的缺点是前向和后向都有2次 AllReduce，而 Allreduce 跨机器通信比较慢，机器间带宽不如nvlink高，而且 Allreduce 不能利用通信和计算重叠，因此 TP <strong>不能跨机器用</strong></p>
<ul>
<li><strong>All Reduce</strong>：在所有的节点上都应用同样的 Reduce 操作。All Reduce 操作可通过单节点上Reduce + Broadcast 操作完成。如下图所示，All Reduce Sum 操作将所有计算设备上的数据汇聚到各个计算设备中，并执行求和操作。改进：<a href="https://zhuanlan.zhihu.com/p/69797852">Ring All-Reduce 算法</a></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135005929.png" alt="image.png" /></p>
<h3 id="pp-流水线并行"><a class="markdownIt-Anchor" href="#pp-流水线并行"></a> <strong>PP (流水线并行）</strong></h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135013648.png" alt="image.png" /></p>
<p>PP即模型可以按照层间切分</p>
<p>优点：只需要层之间通信，通信量相对较小，故PP可以扩展到机器间</p>
<p>缺点：存在 bubble，可能会影响并行效率</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135021185.png" alt="image.png" /></p>
<h3 id="dp-数据并行"><a class="markdownIt-Anchor" href="#dp-数据并行"></a> <strong>DP (数据并行）</strong></h3>
<p>数据并行的核心思想是对输入数据按 batch 维度进行划分，将<strong>数据分配给不同GPU进行计算</strong>。在数据并行里，<strong>每个GPU上存储的模型、优化器状态是完全相同的</strong>。当每块GPU上的前后向传播完成后，需要将每块GPU上计算出的模型梯度汇总求平均，以得到整个batch的模型梯度。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135028031.png" alt="image.png" /></p>
<p>数据并行是在小模型时代（BERT）最常用的并行方法，在模型不是非常大的情况下，优先使用数据并行。</p>
<p><strong>Zero是数据并行的升级版</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135035221.png" alt="image.png" /></p>
<p>ZeRO 的核心思想是在进行 all-reduce 操作的时候，不需要把完整的结果 reduce 到所有 gpu 上，每个 gpu 只需要维护自己的那一块，在需要完整数据的时候，再进行通信拉取完整的数据。尽管 <strong>ZeRO</strong> <strong>分片存储模型参数、梯度还有优化器状态</strong>，但是这并不是一种模型并行。<strong>ZeRO</strong> <strong>本质上就是一套数据并行的训练框架</strong>，只是加入了一套优化存储冗余的策略。</p>
<p><strong>数据并行对于通信的额外开销低，比模型并行更适合扩展到机器间</strong></p>
<h3 id="cp-context-并行link"><a class="markdownIt-Anchor" href="#cp-context-并行link"></a> <strong>CP (Context 并行）[</strong><a href="https://arxiv.org/pdf/2105.13120"><strong>link</strong></a><strong>]</strong></h3>
<h5 id="sequence-parallelism"><a class="markdownIt-Anchor" href="#sequence-parallelism"></a> <strong>Sequence Parallelism</strong></h5>
<p>Megatron 在他们的 Tensor Parallelism 的基础上，将 Transformer 核的 LayerNorm 以及 Dropout 层的输入<strong>按 Sequence Length 维度进行了切分，</strong> 在 self-attention 计算前聚合（all-gather）了sequence的内容</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135046659.png" alt="image.png" /></p>
<h5 id="context-parallelism"><a class="markdownIt-Anchor" href="#context-parallelism"></a> <strong>Context Parallelism</strong></h5>
<p>相当于 Sequence Parallelism 的升级版，<strong>解决SP中未完成的self-attention序列并行问题</strong>。Self-attention的计算里面为什么需要完整的序列？ Attention中QKV的计算需要用到一个完整sequence信息，计算上的耦合使得该模块不能先运算后进行简单拼接。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5864385em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622307142857143em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222307142857144em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17776928571428574em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></p>
<p>基于 ring attention 的方法</p>
<ul>
<li><strong>Step 1 -</strong> <strong>数据切分</strong>：Q/K/V拆分成数据 [Q0,Q1,Q2]/[K0,K1,K2]/[V0,V1,V2] ，CP设备组分为rank0、rank1、rank2，每个rank拿到固定[b, sq/3, np, hd]大小数据Q；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135059955.png" alt="image.png" /></p>
<ul>
<li><strong>Step 2 - Attention计算</strong>：第一次计算时，rank拿一份K、V数据，比如rank0拿到K0，V0。计算通过FA(FlashAttention)2模块完成获得第一个输出O_00。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135109436.png" alt="image.png" /></p>
<ul>
<li><strong>Step 3 - KV数据交换</strong>：计算的同时，可以进行数据交换。每个rank与相邻的rank进行环形P2P通信，传出自己的KV值，同时拿到下一次需要运算的数据，一共需要完成CP−1次通信。如rank0，算完K0,V0后下次需要运算的数据为K2, V2，从rank2获取；同时，rank0将K0V0数据传递给rank1。这个步骤可以和 Step 2 重叠，提高通讯效率。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135116119.png" alt="image.png" /></p>
<ul>
<li><strong>Step 4 - 计算最终输出:</strong> 每个rank的Q与KV匹配计算完后获得三个输出值，然后进行结果修正得到[O_X0, O_X1, O_X2]，X值为rank序号。最后每个rank将自己的分块结果进行聚合（加法）运算得到结果O_X。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135122474.png" alt="image.png" /></p>
<p><strong>通信量</strong>：增加了p2p通信，通信总量为 2∗b∗sq∗np∗hd∗（cp−1）/ cp个单位</p>
<p><strong>内存</strong>：假设每个rank只有一个buffer大小，QKV输入的显存变为 b∗sq∗np∗hd∗5/cp 个单位</p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> <strong>总结</strong></h3>
<ul>
<li>
<p><strong>数据并行解决 batch size 大，存储冗余的问题，模型并行解决模型大的问题，Context 并行解决 Sequence 大的问题</strong></p>
</li>
<li>
<p><strong>对于通讯需求/带宽要求的排序：张量并行 &gt; Context 并行 &gt; 流水线并行 &gt; 数据并行</strong></p>
</li>
</ul>
<h2 id="llama-训练细节"><a class="markdownIt-Anchor" href="#llama-训练细节"></a> Llama 训练细节</h2>
<h3 id="tp-张量并行-2"><a class="markdownIt-Anchor" href="#tp-张量并行-2"></a> <strong>TP (张量并行）</strong></h3>
<p>无变化</p>
<h3 id="pp-流水线并行-2"><a class="markdownIt-Anchor" href="#pp-流水线并行-2"></a> <strong>PP (流水线并行）</strong></h3>
<ul>
<li><strong>减少 bubble： 使用interleaved 1F1B scheduling</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135130127.png" alt="image.png" /></p>
<p><strong>当一个 microbatch 的前向传播完成后，立即进入反向传播阶段，反向传播完成后就可以丢弃掉对应 microbatch 缓存的激活，减少显存的需求。同时每个设备处理的是多个层的子集，这些子集被称为模型块，每个设备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势，在一个mini batch的所有micro batch都反向传播完成后，进行 flushing 更新参数。</strong> 比如将模型分为2个模型块，设备 1 处理层 1、2、9、10，设备 2 处理层 3、4、11、12，设备 3 处理层 5、6、13、14，设备 2 处理层 7、8、15、16。一个 mini batch 前向传播的流程会变为 Device 1 -&gt; Device 2 -&gt; Device 3 -&gt; Device 4 -&gt; Device 1 -&gt; Device 2 -&gt; Device 3 -&gt; Device 4</p>
<ul>
<li>
<p><strong>内存和计算不均衡的问题：第一个 stage 需要做 embedding -&gt; 额外 latency 和 memory, 最后一个 stage 需要计算输出和损失 -&gt; 需要额外 latency</strong></p>
<ul>
<li>Llama的做法：<strong>对第一个和最后一个stage 减少分配一个 transformer layer 的计算</strong></li>
</ul>
</li>
</ul>
<h3 id="dp-数据并行-2"><a class="markdownIt-Anchor" href="#dp-数据并行-2"></a> DP (数据并行）</h3>
<p>使用了 <strong>FSDP</strong> (ZeRO stage 3 的一种实现，即对于优化器状态，梯度和参数都进行分片），</p>
<p>回顾一下Zero stage 3会带来1.5x的额外通讯开销：</p>
<p><strong>Stage 2 中，总通讯量为</strong> <strong>φ (对梯度</strong> <strong>Reduce-Scatter)</strong> <strong>+ φ (对更新的参数</strong> <strong>All-Gather) = 2****φ</strong></p>
<p><strong>Stage 3 中，总通讯量为</strong> <strong>φ (前向传播时对参数</strong> <strong>All-Gather)</strong> <strong>+ φ (反向传播时对参数</strong> <strong>All-Gather) +</strong> <strong>φ (对梯度</strong> <strong>Reduce-Scatter) = 3****φ</strong></p>
<p>LLama做的修改是仅在正向传播时对模型参数分片，<strong>反向传播时不对模型参数分片</strong>，免去了额外的 all-gather 通讯开销</p>
<h3 id="cp-context-并行"><a class="markdownIt-Anchor" href="#cp-context-并行"></a> <strong>CP (Context 并行）</strong></h3>
<p>没有用之前提到的 ring-attenion 方法，而<strong>使用了 All-gather 方法</strong>：</p>
<ul>
<li><strong>All Gather</strong>：将所有节点上收集其他所有节点上的数据，All Gather 相当于一个 Gather 操作之后跟着一个 Broadcast 操作。如下图所示，All Gather 操作将所有计算设备上的数据收集到每个计算设备中。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135153448.png" alt="image.png" /></p>
<ul>
<li>
<p>在计算attention得时候query是按照序列并行拆分的，但是key和value都是全量的数据，要想key和value是全量的数据，那么必须获取所有卡的key和value。ring-attention 借助了 ring 的想法重叠了通讯和计算，提高通讯效率，减少显存开销</p>
</li>
<li>
<p><strong>Llama 选择了直接在计算前对 KV 做 all-gather 操作，这样会更加占用显存，并且通讯和计算时间也不能重叠。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135211167.png" alt="image.png" /></p>
<p>Meta给的解释是：</p>
<ul>
<li>
<p>更容易、更灵活地支持不同类型的注意力掩码</p>
<ul>
<li>Casual mask: 在Transformer模型中，自注意力（self-attention）是在整个 token 序列上计算的，包括当前 token 之后的 token。在训练期间，我们不希望模型在预测当前 token 时看到未来的 token 而“作弊”。 为了防止这种情况，我们使用了因果掩码（causal mask），将所有未来的 token 设置为零，有效地从注意力机制中屏蔽了它们。这使得<strong>模型在进行预测时只能关注过去和当前的 token</strong> ，并确保它仅基于每个时间步骤可用的信息进行预测。具体实现中，这种掩码可以通过原始输入和一个合适的上三角矩阵相乘（或者逻辑与）来得到。(<a href="https://blog.csdn.net/qq_35169059/article/details/101678207">Transformer的矩阵维度分析和Mask详解_transformer mask矩阵-CSDN博客</a>、<a href="http://www.360doc.com/content/23/0607/09/7673502_1083788043.shtml">图解GPT2模型Self Attention注意力机制</a>)<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135222385.png" alt="image.png" /></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135230125.png" alt="image.png" /></p>
<ul>
<li>Document mask: 防止同一序列内不同 document 之间做 self-attention</li>
</ul>
</li>
<li>
<p>由于使用了GQA，通信的K和V张量比Q张量小得多，因此all-gather延迟很小。注意力计算的时间复杂度比all-gather大一个数量级（O(S^2)与O(S)，其中S表示序列长度），使all-gather的开销可以忽略不计。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135236325.png" alt="image.png" /></p>
</li>
<li>
<p>猜测Ring的话会把Attention计算切成更小更碎的算子，在高带宽的机器上 all-gather 可能会更加适合</p>
</li>
</ul>
</li>
<li>
<p>Load-balancing 优化</p>
<ul>
<li>对于不同的的sequence 分片，由于有causal mask，存在计算不均衡的问题<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135243392.png" alt="image.png" /></li>
</ul>
<p><strong>LLama 训练时将输入序列划分为 2 × CP 块，每个CP rank 接收两个块。第 i 个 CP rank 同时接收第 i 个块和第 (2 × CP − 1 − i) 个块。</strong></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>训练技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>LoRA (Low-Rank Adaptation) 低秩微调</title>
    <url>/2025/01/26/LoRA-Low-Rank-Adaptation-%E4%BD%8E%E7%A7%A9%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>随着最近大规模语言模型（Large Language Model，LLM）的出现，数十亿乃至千亿的参数量级成为了LLM的标配。如此参数量级的模型意味着传统的模型微调或者线性探测无法同时在训练效率和效果上同时满足开发者的要求。在这里介绍一个近期训练LLM普遍使用的PEFT（Parameter-Efficient Fine-Tuning）算法：<strong>LoRA</strong>（Low Rank Adaptation）[1]，顾名思义，LoRA的核心思想是基于低秩的适配器进行优化。</p>
<h2 id="1-背景知识"><a class="markdownIt-Anchor" href="#1-背景知识"></a> 1. 背景知识</h2>
<h3 id="11-什么是秩"><a class="markdownIt-Anchor" href="#11-什么是秩"></a> 1.1 什么是秩？</h3>
<p>那么什么是秩呢？矩阵的秩（rank）分为行秩和列秩，行秩指的是矩阵的线性无关的行的个数，列秩同理。因为一个矩阵的行秩和列秩总是相等的，因此它们统一被叫做矩阵的<strong>秩</strong>。在机器学习中，我们通常使用一个矩阵来表示一个全连接层，但是这个全连接层往往是过参数化的，这意味着我们可以通过计算这个矩阵的秩来确定哪些特征是重要和相关的。例如在主成分分析（PCA）和奇异值分解（SVD）中，我们通过一个较低维度的表示来近似表示一个高维矩阵或数据集（图1）。换句话说，我们试图找到原始特征空间（或矩阵）中少数维度的（线性）组合，能够捕捉数据集中大部分的信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141522733.png" alt="image.png" /></p>
<span id="more"></span>
<p>图1：PCA将三维特征压缩为二维图</p>
<h3 id="12-过参数化"><a class="markdownIt-Anchor" href="#12-过参数化"></a> 1.2 过参数化</h3>
<p>现在深度学习的参数动不动就有几百万，LLM的参数更是数十亿起步。许多工作[2]已经表明，深度学习的矩阵往往是过参数化的（over-parametrized）。特征的内在维度（intrinsic dimension）指的是在深度学习中的真实或潜在的低维结构或信息的维度。它表示特征中存在的有效信息的维度，与特征的实际维度可能不同。事实上许多问题的内在维度比人们认为的要小的多，而对于某个数据集，内在维度在不同参数量级的模型上差距并不大。这个内在维度指的是我们解决这个问题实际上需要的参数空间的维度，我们对模型的微调通常调整的也是这些低秩的内在维度。这个结论说明了两个现象：</p>
<ol>
<li>
<p>一旦我们找到了足够解决问题的参数空间，再增加这个参数空间的大小并不会显著提升模型的性能。</p>
</li>
<li>
<p>一个过参数的模型的参数空间是有压缩的空间的，这也就是LoRA的提出动机。</p>
</li>
</ol>
<h2 id="2-lora"><a class="markdownIt-Anchor" href="#2-lora"></a> 2. LoRA</h2>
<h3 id="21-计算原理"><a class="markdownIt-Anchor" href="#21-计算原理"></a> 2.1 计算原理</h3>
<p>和其它串行的适配器算法不同，LoRA的做法是在LLM的某些矩阵（ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol W \in \mathbb R^{d \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72521em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>）旁插入一个和它并行的新的权值矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi mathvariant="bold-italic">W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\Delta \boldsymbol W\in \mathbb R^{d \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72521em;vertical-align:-0.0391em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>，但是因为模型的低秩性的存在，我们可以将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi mathvariant="bold-italic">W</mi></mrow><annotation encoding="application/x-tex">\Delta \boldsymbol W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span></span></span></span> 拆分成降维矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol A \in \mathbb R^{d \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72521em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span></span></span></span> 和升维矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol B \in \mathbb R^{r \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72521em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span> （图2），其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≪</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>d</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r \ll \min(d,k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span>，从而实现了以极小的参数数量训练LLM。在训练时，我们将LLM的参数固定，只训练矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">B</mi></mrow><annotation encoding="application/x-tex">\boldsymbol B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span></span></span></span> 。根据式(1)，在模型训练完成之后，我们可以直接将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">B</mi></mrow><annotation encoding="application/x-tex">\boldsymbol B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span></span></span></span> 加到原参数上，从而在推理时不会产生额外的推理时延。</p>
<p><span class='katex-error' title='ParseError: KaTeX parse error: \tag works only in display equations'>\boldsymbol h = \boldsymbol W_0 \boldsymbol x + \Delta \boldsymbol W \boldsymbol x = (\boldsymbol W_0 + \Delta \boldsymbol W ) \boldsymbol x = \boldsymbol W \boldsymbol x + \boldsymbol A \boldsymbol B \boldsymbol x\tag1</span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141531228.png" alt="image.png" /></p>
<p>图2：在训练时，LoRA在预训练权值旁插入了一组和它并行的低秩矩阵</p>
<p>在初始化时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span> 使用高斯初始化，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">B</mi></mrow><annotation encoding="application/x-tex">\boldsymbol B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span></span></span></span>使用的零矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">0</mi></mrow><annotation encoding="application/x-tex">\boldsymbol 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">0</span></span></span></span></span> 进行的初始化。因为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span>通常是一个非常小的值（实验证明1，2，4，8的效果就非常好），所以LoRA在训练时引入的参数量是非常小的，因此它的训练也是非常高效的，也不会带来显著的显存增加。</p>
<p>LoRA要求 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span></span></span></span> 或者 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">B</mi></mrow><annotation encoding="application/x-tex">\boldsymbol B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span></span></span></span> 其中之一必须使用零矩阵进行初始化，这样当数据第一次通过网络时，它和预训练的结果是一致的，这样便保证了模型在初始阶段便有一个不错的效果。苏剑林老师指出，这种一个全零，一个非全零的方式带来了不对称的问题[3]，其实我们也可以使用两个非全零矩阵进行初始化，但是需要事先将预训练权重减去初始化的值，即： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">W</mi><mo>=</mo><msub><mi mathvariant="bold-italic">W</mi><mn>0</mn></msub><mo>−</mo><msub><mi mathvariant="bold-italic">A</mi><mn>0</mn></msub><msub><mi mathvariant="bold-italic">B</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant="bold-italic">A</mi><mi mathvariant="bold-italic">B</mi></mrow><annotation encoding="application/x-tex">\boldsymbol W = \boldsymbol W_0 - \boldsymbol A_0 \boldsymbol B_0 + \boldsymbol A \boldsymbol B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83611em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol">A</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord boldsymbol">A</span></span><span class="mord"><span class="mord boldsymbol" style="margin-right:0.04835em;">B</span></span></span></span></span>。</p>
<p>LoRA实现起来非常简单，注意在下面代码的第17行有一个参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> ，它是一个缩放参数，通常是一个常数。通过设置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 有助于在变化 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span>时减少重新调整超参数的需求。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_dim = <span class="number">768</span> <span class="comment"># 例如，预训练模型的隐藏大小</span></span><br><span class="line">output_dim = <span class="number">768</span> <span class="comment"># 例如，层的输出大小</span></span><br><span class="line">rank = <span class="number">8</span> <span class="comment"># 低秩适应的等级&#x27;r&#x27;</span></span><br><span class="line">W = ... <span class="comment"># 来自预训练网络的权重，形状为 input_dim x output_dim</span></span><br><span class="line">W_A = nn.Parameter(torch.empty(input_dim, rank)) <span class="comment"># LoRA权重A</span></span><br><span class="line">W_B = nn.Parameter(torch.empty(rank, output_dim)) <span class="comment"># LoRA权重B</span></span><br><span class="line"><span class="comment"># 初始化LoRA权重</span></span><br><span class="line">nn.init.kaiming_uniform_(W_A, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">nn.init.zeros_(W_B)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">regular_forward_matmul</span>(<span class="params">x, W</span>):</span><br><span class="line">  h = x @ W</span><br><span class="line">  <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lora_forward_matmul</span>(<span class="params">x, W, W_A, W_B</span>):</span><br><span class="line">  h = x @ W <span class="comment"># 常规矩阵乘法</span></span><br><span class="line">  h += x @ (W_A @ W_B) * alpha <span class="comment"># 使用缩放的LoRA权重</span></span><br><span class="line">  <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<h3 id="22-transformer中的lora"><a class="markdownIt-Anchor" href="#22-transformer中的lora"></a> 2.2 Transformer中的LoRA</h3>
<p>理论上LoRA的思想可以应用到任何权值矩阵上，例如在Transformer的自注意机制中我们就有四个权值矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">W</mi><mi>v</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">W</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\boldsymbol W_q, \boldsymbol W_k, \boldsymbol W_v, \boldsymbol W_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.972218em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，另外在Transformer的全连接中也有两个权值矩阵。关于LoRA在Transformer的作用位置，作者在自注意力层做了一组对照实验，实验结果如表1。从表1的实验结果中我们可以看出，如果只将LoRA作用到某个单一矩阵上， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol W_q, \boldsymbol W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.972218em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的效果并不理想。而如果考虑两个矩阵， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold-italic">W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol W_q, \boldsymbol W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.972218em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.15972em;">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的组合是一个不错的选择。而最好的方式是在所有的权值矩阵都加上LoRA，因为这样有利于模型捕捉到所有矩阵的关键信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141618328.png" alt="image.png" /></p>
<p>表1：LoRA在不同的自注意力权值矩阵上的对照实验</p>
<h3 id="23-秩的大小"><a class="markdownIt-Anchor" href="#23-秩的大小"></a> 2.3 秩的大小</h3>
<p>关于秩的大小，作者也做了一组对照实验。从表2的实验结果可以看出，秩的值并不是越大越好，一般在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">r=8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span></span></span></span>的时候便达到了最优解。而且1或者2这种很小的秩的表现也不差。证明了之前的权值矩阵可能拥有很小的内在秩的假设。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141624856.png" alt="image.png" /></p>
<p>表2：LoRA在不同秩大小上的对照实验</p>
<p>通过对比 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">r=8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">r=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 计算得到的矩阵经过SVD得到的两个右奇异矩阵的格拉斯曼距离相似性，作者得出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">r=8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span></span></span></span>的特征包含在了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">r=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> 的其中的8个特征中，设置更大的奇异值反而引入了额外的噪声。</p>
<h2 id="3-总结"><a class="markdownIt-Anchor" href="#3-总结"></a> 3. 总结</h2>
<p>LoRA是PEFT非常重要的一个算法，也是训练LLM模型时实践出的一个效果非常好的算法。LoRA并不复杂但是设计的非常巧妙：首先LoRA并不会带来任何的推理时间的增加。其次LoRA并不会更改原始模型，而是只训练一个新增的额外参数，而且这个参数仅用来适配当前任务。但是这也意味着LoRA在训练多任务时需要多个不同的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">\Delta W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> ，多任务的学习对于LoRA来说比较困难，除非把它们当成同一个任务。</p>
<h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/663557294">https://zhuanlan.zhihu.com/p/663557294</a></li>
</ol>
]]></content>
      <categories>
        <category>大模型</category>
        <category>训练技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式训练技术 (数据并行, 模型并行, DeepSpeed Zero, Megatron-LM)</title>
    <url>/2025/01/26/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h2 id="分布式训练的意义"><a class="markdownIt-Anchor" href="#分布式训练的意义"></a> 分布式训练的意义</h2>
<ul>
<li>
<p><strong>节省训练时间</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135905592.png" alt="image.png" /></p>
<p><strong>计算速率：</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126135914854.png" alt="image.png" /></p>
</li>
</ul>
<span id="more"></span>
<ul>
<li>
<p><strong>突破显存限制</strong></p>
<p><strong>模型训练的总内存</strong></p>
<ul>
<li>
<p>大模型混合精度训练过程中，使用 FP16 进行前向传递，FP32 反向传递梯度信息；优化器更新模型参数时，使用 FP32 优化器状态、FP32 的梯度来更新模型参数。</p>
</li>
<li>
<p>在一次训练迭代中，每个可训练参数都对应1个梯度，2个优化器状态（Adam)。设模型参数量为φ (FP16)，那么梯度的参数量为2φ (FP32)，Adam优化器的参数量为4φ (FP32)</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140153578.png" alt="image.png" /></p>
</li>
<li>
<p>一般大模型训练为了节省内存，使用激活重计算来减少中间激活值。</p>
<ul>
<li>训练总内存 = 模型内存(φ) + 梯度内存(2φ) + 优化器内存(2φ) + 激活内存(?φ) + 其他内存(1.xφ)</li>
</ul>
</li>
<li>
<p>因此在LLMs大模型训练过程中最少需要8X倍于模型权重的大小内存，eg.LLAMA-65B:</p>
<ul>
<li>
<p>65B模型权重参数I30GB,训练时候总消耗内存最小为130GBx8≈1TB。</p>
</li>
<li>
<p>1TG/64GB≈17，除去激活值需要的内存，仅仅放下一个大模型就需要32张卡。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="并行方式"><a class="markdownIt-Anchor" href="#并行方式"></a> 并行方式</h2>
<p>主要可以分为三种并行方式：数据并行（Data Parallel）、张量并行（Tensor Parallel）和流水线并行（Pipeline Parallel）；其中</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140139092.png" alt="image.png" /></p>
<h3 id="数据并行"><a class="markdownIt-Anchor" href="#数据并行"></a> 数据并行</h3>
<p>数据并行是目前最为常见和基础的并行方式。这种并行方式的核心思想是对输入数据按 batch 维度进行划分，将<strong>数据分配给不同GPU进行计算</strong>。在数据并行里，<strong>每个GPU上存储的模型、优化器状态是完全相同的</strong>。当每块GPU上的前后向传播完成后，需要将每块GPU上计算出的模型梯度汇总求平均，以得到整个batch的模型梯度。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140204847.png" alt="image.png" /></p>
<h3 id="模型并行"><a class="markdownIt-Anchor" href="#模型并行"></a> 模型并行</h3>
<p>模型并行（MP，Model Parallelism）可分为流水线并行（PP，Pipeline Parallelism）和张量并行（TP，Tensor Parallesim），都是解决当GPU放不下一个模型时，而将模型拆分到不同的GPU上的方法。</p>
<p>在<strong>数据并行</strong>训练中，一个明显的特点是<strong>每个 GPU 持有整个模型权重的副本</strong>。当参数规模为千亿时，存储模型参数就需要数百GB的显存空间，超出单个GPU卡的显存容量。显然，仅靠数据并行无法满足超大规模模型训练对于显存的需求。为了解决这个问题，可以采用模型并行技术。与数据并行在不同设备都有完整的计算图不同，模型并行是不同设备负责单个计算图不同部分的计算。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140211230.png" alt="image.png" /></p>
<p>模型并行从计算图的切分角度：</p>
<p>1、按模型的layer层切分到不同设备，即_<strong>层间并行</strong>_，我们称之为<strong>流水线并行</strong>。</p>
<p>2、将计算图中的层内的参数切分到不同设备，即_<strong>层内并行</strong>_，我们称之为<strong>张量并行</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140217206.png" alt="image.png" /></p>
<p>由于Pipeline并行和Tensor并行是正交的，所以可以同时使用，如下图pipeline并行是竖切，tensor并行是横切，每个色块代表一个device，一个模型运行在4个device上</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140223559.png" alt="image.png" /></p>
<h4 id="张量并行"><a class="markdownIt-Anchor" href="#张量并行"></a> 张量并行</h4>
<p>模型并行是不同设备负责单个计算图不同部分的计算。而将计算图中的层内的参数（张量）切分到不同设备（即层内并行），每个设备只拥有模型的一部分，以减少内存负荷，称之为张量模型并行。</p>
<p>张量并行从数学原理上来看就是对于<code>linear</code>层就是把矩阵分块进行计算，然后把结果合并；对于非<code>linear</code>层，则不做额外设计；张量并行的核心就是将矩阵乘法进行拆分，从而降低模型对单卡的显存需求</p>
<p>对于模型中某一个线性变换<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">Y=XA</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span></span></span></span>（X是输入，是权重，Y是输出）可以有两种变换方式：</p>
<p>1、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mo stretchy="false">[</mo><mi>X</mi><mn>1</mn><mtext>​</mtext><mi>X</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>A</mi><mn>1</mn><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mi>X</mi><mn>1</mn><mi>A</mi><mn>1</mn><mo>+</mo><mi>X</mi><mn>2</mn><mi>A</mi><mn>2</mn><mo>=</mo><mi>Y</mi><mn>1</mn><mo>+</mo><mi>Y</mi><mn>2</mn><mo>=</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">XA=[ X1 ​ X2 ​ ][ A1 A2 ​ ]=X1A1+X2A2=Y1+Y2=Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">1</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">2</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p>2、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mi>X</mi><mtext>​</mtext><mo stretchy="false">[</mo><mi>A</mi><mn>1</mn><mtext>​</mtext><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mo stretchy="false">[</mo><mi>X</mi><mi>A</mi><mn>1</mn><mtext>​</mtext><mi>X</mi><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mo stretchy="false">[</mo><mi>Y</mi><mn>1</mn><mtext>​</mtext><mi>Y</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">XA= X ​ [ A1 ​ A2 ​ ]=[ XA1 ​ XA2 ​ ]=[ Y1 ​ Y2 ​ ]=Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">​</span><span class="mopen">[</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p>分别对应两种并行方式：</p>
<h5 id="行并行row-parallelism"><a class="markdownIt-Anchor" href="#行并行row-parallelism"></a> 行并行（Row Parallelism）</h5>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mo stretchy="false">[</mo><mi>X</mi><mn>1</mn><mtext>​</mtext><mi>X</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>A</mi><mn>1</mn><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mi>X</mi><mn>1</mn><mi>A</mi><mn>1</mn><mo>+</mo><mi>X</mi><mn>2</mn><mi>A</mi><mn>2</mn><mo>=</mo><mi>Y</mi><mn>1</mn><mo>+</mo><mi>Y</mi><mn>2</mn><mo>=</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">XA=[ X1 ​ X2 ​ ][ A1 A2 ​ ]=X1A1+X2A2=Y1+Y2=Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">1</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">2</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p>X1 和 A1 就可以放到 GPU0 之上计算得出 Y1，X2 和 A2 可以放到第二个 GPU1 之上计算得出 Y2，然后，把Y1和Y2结果相加，得到最终的输出Y。$</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140258737.png" alt="image.png" /></p>
<h5 id="列并行column-parallelism"><a class="markdownIt-Anchor" href="#列并行column-parallelism"></a> 列并行（Column Parallelism）</h5>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>A</mi><mo>=</mo><mi>X</mi><mtext>​</mtext><mo stretchy="false">[</mo><mi>A</mi><mn>1</mn><mtext>​</mtext><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mo stretchy="false">[</mo><mi>X</mi><mi>A</mi><mn>1</mn><mtext>​</mtext><mi>X</mi><mi>A</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mo stretchy="false">[</mo><mi>Y</mi><mn>1</mn><mtext>​</mtext><mi>Y</mi><mn>2</mn><mtext>​</mtext><mo stretchy="false">]</mo><mo>=</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">XA= X ​ [ A1 ​ A2 ​ ]=[ XA1 ​ XA2 ​ ]=[ Y1 ​ Y2 ​ ]=Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">​</span><span class="mopen">[</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">A</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">1</span><span class="mord">​</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">2</span><span class="mord">​</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p>将 X 分别放置在GPU0 和GPU1，将 A1 放置在 GPU0，将 A2 放置在 GPU1，然后分别进行矩阵运行，最终将2个GPU上面的矩阵拼接在一起，得到最终的输出Y。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140309913.png" alt="image.png" /></p>
<h4 id="流水线并行"><a class="markdownIt-Anchor" href="#流水线并行"></a> 流水线并行</h4>
<p>**流水线并行（Pipeline Parallelism，PP）**是一种并行计算策略，将模型的各个层分段处理，并将每个段分布在不同的计算设备上，使得前后阶段能够流水式、分批进行工作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140316557.png" alt="image.png" /></p>
<h5 id="naive-pipelining"><a class="markdownIt-Anchor" href="#naive-pipelining"></a> Naive Pipelining</h5>
<p>当一个模型大到单个GPU无法训练时，最直接的想法是对模型层进行划分，然后将划分后的部分放置在不同的GPU上。下面以一个4层的序列模型为例，介绍朴素层并行：</p>
<p>假设一个大模型有4层，采用PP的方式，可以把其中的0层放在GPU序号为0的GPU的显存中，其他的1–3层以此类推。</p>
<p>整个朴素层并行前向传播和后向传播的过程如下图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140323222.png" alt="image.png" /></p>
<p><strong>朴素层并行的缺点</strong>：</p>
<ul>
<li>
<p><strong>低GPU利用率</strong>。 在任意时刻，有且仅有一个GPU在工作，其他GPU都是空闲的。</p>
</li>
<li>
<p><strong>计算和通信没有重叠</strong>。在发送前向传播的中间结果(FWD)或者反向传播的中间结果(BWD)时，GPU也是空闲的。</p>
</li>
<li>
<p><strong>高显存占用</strong>。GPU1需要保存整个minibatch的所有激活，直至最后完成参数更新。如果batch size很大，这将对显存带来巨大的挑战。</p>
</li>
</ul>
<h5 id="gpipe"><a class="markdownIt-Anchor" href="#gpipe"></a> <strong>GPipe</strong></h5>
<p>GPipe通过将minibatch划分为更小且相等尺寸的microbatch来提高效率。具体来说，让每个microbatch独立的计算前后向传播，然后将每个mircobatch的梯度相加，就能得到整个batch的梯度。由于每个层仅在一个GPU上，对mircobatch的梯度求和仅需要在本地进行即可，不需要通信。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140330219.png" alt="image.png" /></p>
<p>在GPipe的调度中，每个timestep上花费的时间要比朴素层并行更短，因为每个GPU仅需要处理microbatch</p>
<h6 id="gpipe的bubbles问题"><a class="markdownIt-Anchor" href="#gpipe的bubbles问题"></a> <strong>GPipe的Bubbles问题</strong></h6>
<p>bubbles指的是流水线中没有进行任何有效工作的点。这是由于操作之间的依赖导致的。例如，在GPU3执行完F1之前，GPU4只能等待。整个流水线过程中的bubbles如下图所示</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140337185.png" alt="image.png" /></p>
<p>增大microbatch的数量，可以降低bubbles的比例</p>
<h6 id="gpipe的显存占用问题"><a class="markdownIt-Anchor" href="#gpipe的显存占用问题"></a> <strong>GPipe的显存占用问题</strong></h6>
<p>在GPipe中，GPU需要在前向传播至反向传播这段时间内缓存激活(activations)。</p>
<h5 id="pipedream-link"><a class="markdownIt-Anchor" href="#pipedream-link"></a> <strong>PipeDream [</strong><a href="https://dl.acm.org/doi/pdf/10.1145/3341301.3359646"><strong>link</strong></a><strong>]</strong></h5>
<p>GPipe需要等所有的microbatch前向传播完成后，才会开始反向传播。PipeDream提出了 <strong>1F1B</strong> 流水线策略，即一个前向通道和一个后向通道。1F1B 流水线策略引入了任务调度机制，使得下游设备能够在等待上游计算的同时执行其他可并行的任务，从而提高设备的利用率。<strong>当一个microbatch的前向传播完成后，立即进入反向传播阶段</strong>。理论上，反向传播完成后就可以丢弃掉对应microbatch缓存的激活。由于PipeDream的反向传播完成的要比GPipe早，因此也会减少显存的需求。</p>
<p>下图是PipeDream的调度图，4个GPU和8个microbatchs。蓝色的方块表示前向传播，绿色表示反向传播，数字则是microbatch的id。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140343047.png" alt="image.png" /></p>
<p><strong>如何避免正向和反向传播中权重的冲突？</strong></p>
<p><strong>引入多版本控制</strong></p>
<ul>
<li>
<p><strong>Weight stashing</strong></p>
<p><strong>各个阶段****都用最新版本的权重进行前向计算</strong>，处理输入的Micro-batch。计算前向传播之后，会将这份参数保存下来用于同一个Micro-batch的后向计算。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140349614.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Vertical Sync</strong></p>
<p>每个Micro-batch进入pipeline时都使用输入stage最新版本的参数，并且参数的版本号会伴随该Micro-batch数据整个生命周期，在<strong>各个阶段都是用同一个版本的****权重</strong>（而不是Weight stashing那样都使用最新版本的参数），从而实现了stage间的参数一致性。</p>
</li>
</ul>
<h2 id="分布式训练框架"><a class="markdownIt-Anchor" href="#分布式训练框架"></a> 分布式训练框架</h2>
<h3 id="megatron"><a class="markdownIt-Anchor" href="#megatron"></a> Megatron</h3>
<p>NVIDIA Megatron 是一个基于 PyTorch 的分布式训练框架，用来训练超大Transformer语言模型</p>
<h4 id="megatron-v1-link"><a class="markdownIt-Anchor" href="#megatron-v1-link"></a> Megatron-v1 [<a href="https://arxiv.org/pdf/1909.08053">link</a>]</h4>
<p>Megatron-v1 专门对 Tranformer 模型提出了<strong>张量并行</strong>的方案。</p>
<p>Tranformer 中的 MLP 结构均包含两层全连接（FC）层，即存在两个矩阵乘，对第一个 FC 层的参数矩阵按列切块，对第二个 FC层参数矩阵按行切块。这样第一个 FC 层的输出恰好满足第二个 FC 层数据输入要求（按列切分），因此可以省去第一个 FC 层后的汇总通信操作。</p>
<p>多头自注意力机制的张量并行与 MLP 类似，因为具有多个独立的头，因此相较于 MLP 更容易实现并行</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140422483.png" alt="image.png" /></p>
<p><strong>通信成本</strong>：每个 MLP 层或者 Self-Attention 层都是需要1次 All-reduce 操作，前向 + 反向则是2次 All-reduce 操作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140430912.png" alt="image.png" /></p>
<h5 id="tensor-并行的问题"><a class="markdownIt-Anchor" href="#tensor-并行的问题"></a> <strong>Tensor 并行的问题</strong></h5>
<p><strong>activation占用显存上</strong>：切分了大部分的activation，<strong>layernorm、dropout前后的显存没有切分</strong>。v3的序列并行做的主要事情就是把activation切分彻底。</p>
<p><strong>通信量大</strong>：不同卡上的结果需要合并，此时会有较大的通信量，通常比数据并行、pipeline并行的通信量要大很多。受限于通信量大，因此tensor并行通常只用于机器内的并行，例如nvidia实验中，多机、每个机器上8张卡，tensor并行最多设置为8。</p>
<h4 id="megatron-v2-link"><a class="markdownIt-Anchor" href="#megatron-v2-link"></a> Megatron-v2 [<a href="https://arxiv.org/pdf/2104.04473">link</a>]</h4>
<p>v2这篇大部分是在讨论<strong>3D并行</strong>（combine pipeline, tensor, and data parallelism), 对于pipeline并行也做了一些优化</p>
<ul>
<li>
<p><strong>Takeaway 1</strong>: Model parallelism 中，pipeline parallelism 的特点是点对点通信成本较低。而 tensor parallelism 则使用 all-reduce 通信。当考虑不同形式的模型并行时，tensor 并行的数量需要小于等于机器的卡数，然后使用流水线并行来跨服务器扩展到更大的模型。</p>
</li>
<li>
<p><strong>Takeaway 2</strong>: Data parallelism 比 Model parallelism 通信成本和对中间结果的缓存要求更低，更利于扩展到更多gpu中。</p>
</li>
</ul>
<h5 id="pipeline并行优化"><a class="markdownIt-Anchor" href="#pipeline并行优化"></a> <strong>Pipeline并行优化</strong></h5>
<p><strong>朴素的 pipelining —&gt; GPipe 减少气泡 —&gt; 1F1B pipeline 减少 activation 显存占用 —&gt;</strong> <strong>interleaved 1F1B 进一步减少气泡</strong></p>
<p>Megatron 给出了非交错式和交错式 (interleaved) 两种方式调度方式，如图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140438042.png" alt="image.png" /></p>
<p><strong>1F1B 非交错式调度模式</strong>可分为三个阶段。首先是热身阶段，在该阶段中，计算设备中进行不同数量的前向计算。接下来的阶段是前向-后向阶段，计算设备按顺序执行一次前向计算，然后进行一次后向计算。最后一个阶段是后向阶段，计算设备在完成最后一次后向计算。相比于 GPipe 策略，非交错式调度模式在节省内存方面表现更好，如图示的device4，在完成 micro-batch 1 的前向传输之后马上开始进行 micro-batch 1 的反向 Backward Pass，减少了 micro-batch 1的 Activations 在其他 Device 中的缓存时间。然而，<strong>它需要与 GPipe 策略一样的时间来完成一轮计算</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140445702.png" alt="image.png" /></p>
<p><strong>1F1B 交错式调度模式</strong>要求 micro-batch 的数量是流水线阶段的整数倍。每个设备不再仅负责连续多个层的计算，而是可以处理多个层的子集，这些子集被称为模型块。具体而言，在之前的模式中，设备 1 可能负责层 1-4，设备 2 负责层 5-8，以此类推。然而，在新的模式下，<strong>设备 1 可以处理层 1、2、9、10</strong>，<strong>设备 2 处理层 3、4、11、12</strong>，以此类推。这种模式下，每个设备在流水线中被分配到多个阶段。例如，设备 1 可能参与热身阶段、前向计算阶段和后向计算阶段的某些子集任务。每个设备可以并行执行不同阶段的计算任务，从而更好地利用流水线并行的优势。这种模式不仅在内存消耗方面表现出色，还能够提高计算效率，使得大型模型的并行系统能够更高效地完成计算任务。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140452271.png" alt="image.png" /></p>
<h5 id="bubble-time-fraction-比较"><a class="markdownIt-Anchor" href="#bubble-time-fraction-比较"></a> <strong>Bubble Time Fraction 比较</strong></h5>
<p><strong>Gpipe</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>p</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{p - 1}{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.242216em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.897216em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, microbatches in a batch as<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">m</span></span></span></span>, the number of pipeline stages (number of devices used for pipeline parallelism) as<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span></p>
<p><strong>Default 1F1B</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>p</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{p - 1}{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.242216em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.897216em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p><strong>Interleaved 1F1B</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>v</mi></mfrac><mo>⋅</mo><mfrac><mrow><mi>p</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{v} \cdot \frac{p - 1}{m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.242216em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.897216em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>, stages (or model chunks) each device has as<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></p>
<h5 id="interleaved-1f1b-的代价"><a class="markdownIt-Anchor" href="#interleaved-1f1b-的代价"></a> <strong>Interleaved 1F1B 的代价</strong></h5>
<p><strong>代价一个是额外引入了成倍的通讯</strong>，本来 1F1B 只有 p 个 stage 间需要按顺序通信传递 bsh 大小的中间结果，传递 p-1 次。现在 interleaved 变成了 pv 个 stage 间按顺序传递，传递 pv-1 次。<strong>增加了 p(v-1) 次的通信量</strong>。</p>
<h4 id="megatron-v3-link"><a class="markdownIt-Anchor" href="#megatron-v3-link"></a> Megatron-v3 [<a href="https://arxiv.org/pdf/2205.05198">link</a>]</h4>
<p>MegatronLM的第三篇论文【Reducing Activation Recomputation in Large Transformer Models】是2022年出的，<strong>这篇论文做的主要事情是 activation 的优化</strong>。在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过 recomputation 重计算的方式降低显存占用，但会带来额外的计算代价。这篇论文提出了两种方法，分别是 sequece parallel 和 selective activation recomputation，这两种方法和Tensor并行是可以相结合的，可以有效减少不必要的计算量。</p>
<ul>
<li>
<p>序列并行（sequece parallel）将 tensor 并行时的 activation 分得更彻底，减少了计算量，且没有增加通信的数据量，相当于减少显存也减少时间；</p>
</li>
<li>
<p>selective activation recomputation 比无 activation recomputation 更加节省显存且增加的计算量不大，比 full activation recomputation 增加了显存但速度更快。</p>
</li>
</ul>
<h5 id="sequence-parallel"><a class="markdownIt-Anchor" href="#sequence-parallel"></a> <strong>Sequence Parallel</strong></h5>
<p>如下图，前面v1的时候我们也提到tensor并行时只对下面两个框内的 model state 和 activation 做切分，layernorm 和 dropout 的都没分。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140500473.png" alt="image.png" /></p>
<p>Megatron 在他们的 Tensor Parallelism 的基础上，将 Transformer 核的 LayerNorm 以及 Dropout 层的输入<strong>按 Sequence Length 维度进行了切分</strong>，使得各个设备上面只需要做一部分的 Dropout 和 LayerNorm 即可。</p>
<p>这里把 v1 中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> copy 操作和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo stretchy="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">\overline{f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.08888em;vertical-align:-0.19444em;"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.89444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-3.81444em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span> all-reduce 操作换成了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span> all-gather 操作和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>g</mi><mo stretchy="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">\overline{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.825em;vertical-align:-0.19444em;"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span> reduce-scatter运算。Tensor并行在一次前向和后向总共有4次的 all-reduce 操作，在Sequence并行一次前向和后向总共有4次all-gather和4次reduce-scatter操作。而 ring all-reduce 执行过程可以分解为一个 reduce-scatter 然后跟着一个 all-gather ，因此Sequence并行相比<strong>没有引入更多的通信代价</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140516538.png" alt="image.png" /></p>
<h5 id="selective-activation-recomputation"><a class="markdownIt-Anchor" href="#selective-activation-recomputation"></a> <strong>Selective Activation Recomputation</strong></h5>
<p>Full activation recomputation 是最常见的方法，就是对每一层只保存输入，做后向前，再做一遍这一层的前向计算。这样需要的activation可以降低到只存输入，2sbh，2是因为fp16占两个字节。<strong>Full activation recomputation的主要问题是多走了一遍前向。</strong></p>
<p>selective 的动机是把模型层分成两种，一种计算量大但是 activation 小，另一种计算量小但是相对activation 大，那他这里就<strong>只保存计算量大但是 activation 小的层的激活</strong>。这样只需要花相对少的activation 便可以减少较多的计算量。</p>
<p>在 gpt3 中，论文发现 softmax 这里需要的 activation 占三分之二，但是计算量比较小，那这里每次重计算，其他的三分之一的 activation 就存起来。通过实验发现能够<strong>节省 70%激活所需要的显存，同时只增加 2.7% 重计算的额外开销</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140541800.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140549135.png" alt="image.png" /></p>
<h3 id="deepspeed"><a class="markdownIt-Anchor" href="#deepspeed"></a> DeepSpeed</h3>
<p>微软的大模型训练框架,核心技术是 Zero 相关的一系列paper。</p>
<p>在过去两年 DeepSpeed 团队发表了三篇ZeRO相关的论文，提出了去除冗余参数、引入CPU和内存、引入NVMe等方法，从始至终都围绕着一个目标：将显存优化进行到底。</p>
<h4 id="zero-link"><a class="markdownIt-Anchor" href="#zero-link"></a> ZeRO [<a href="https://arxiv.org/pdf/1910.02054">link</a>]</h4>
<p><strong>ZeRO = ZeRO-DP</strong> (优化 model states) <strong>+ ZeRO-R</strong> (优化 activations, temporary buffers, and memory fragments)</p>
<h5 id="zero-dp-data-parallel"><a class="markdownIt-Anchor" href="#zero-dp-data-parallel"></a> ZeRO-DP (Data Parallel)</h5>
<p>在<strong>混合精度训练</strong>中，前向传播和反向传播都使用 fp16 的权重和激活函数。然而，为了有效地计算并应用反向传播结束时的更新，混合精度优化器保留了一份 fp32 的参数副本以及所有其他优化器状态的 fp32 副本。以 <strong>Adam</strong> 为具体例子，使用 Adam 对一个有 φ 参数的模型进行混合精度训练，需要足够的内存来存储<strong>参数和梯度的 fp16 副本</strong>，分别需要 <strong>2****φ</strong> <strong>和</strong> <strong>2****φ</strong> 字节的内存。此外，它还需要存储<strong>优化器状态：参数、动量和方差的 fp32 副本</strong>，分别需要 <strong>4<strong><strong>φ</strong></strong>, 4****φ</strong> <strong>和 4****φ</strong> 字节的内存。我们用 K 来表示优化器状态的内存倍增器，即存储它们需要额外的 Kφ 字节内存。混合精度 Adam 的 K 值为 12。总共这就导致了 2φ + 2φ + Kφ = <strong>16****φ</strong> 字节的内存需求。</p>
<p>ZeRO-DP 有三个主要的优化阶段，这些阶段分别对应于优化器状态、梯度和参数的分区</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140558267.png" alt="image.png" /></p>
<ul>
<li>
<p><strong>ZeRO-DP Stage 1: 优化器状态</strong> <strong>Partitioning</strong> - 内存减少4倍，通信量与数据并行相同。</p>
<p>在这一阶段，优化器的状态（如动量和方差）被分区存储在各个GPU中。这意味着每个GPU只保存其负责部分的状态，从而大幅减少了单个GPU所需的内存。</p>
<ul>
<li>
<p><strong>Step 1:</strong> 执行1步 Step forward &amp; backward 计算后，各得一份梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">G_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140640118.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Step 2:</strong> 对梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">G_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>执行 AIl-Reduce , 得到完整梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span></span></span></span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140646906.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Step 3</strong>: 得到完整梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span></span></span></span>,对对应的权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">W_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>更新，而权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">W_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>更新由对应的优化器状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">O_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和梯度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span></span></span></span>共同决定</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140658755.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Step 4</strong>: 再对权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>执行 all-gather 使得每 GPU 都有更新后完整<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140708548.png" alt="image.png" /></p>
</li>
</ul>
</li>
<li>
<p><strong>ZeRO-DP Stage 2: 增加梯度</strong> <strong>Partitioning</strong> - 内存减少8倍，通信量与数据并行相同。</p>
<p>此阶段在优化器状态分区的基础上，进一步将梯度也进行分区。这样，每个GPU不仅其优化器状态是局部的，其梯度也是局部的，进一步减少了内存需求。</p>
<p><strong>做法</strong>: 把 stage 1 step 2 的 all-reduce 操作替换成 **Reduce-Scatter, 而 All-Reduce 操作本身就包括 Reduce-Scatter，**没有增加通信开销，同时减少了显存开销</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140809738.png" alt="image.png" /></p>
<p><strong>Naive DP 的做法</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140823075.png" alt="image.png" /></p>
<p><strong>Naive DP</strong> 在汇总梯度和分发更新后参数时使用了 <strong>All-Reduce</strong> 操作，<strong>ZeRO-DP</strong> <strong>Stage 2</strong> 现在改成了对梯度 <strong>Reduce-Scatter</strong>，对参数更新 <strong>All-Gather</strong>，相当于把两个阶段拆开了，没有增加通讯，白嫖很多显存空间。</p>
</li>
<li>
<p><strong>ZeRO-DP Stage 3: 增加参数</strong> <strong>Partitioning</strong> - 内存减少与数据并行程度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">N_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>线性相关。例如，跨64个GPU（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">N_d = 64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span>）分区将实现64倍的内存减少。这会导致<strong>通信量增加50%</strong>。</p>
<p>在此阶段，模型的参数也被分区存储。这意味着每个GPU只存储和更新一部分模型参数，大大减少了单个GPU上的内存需求。虽然这增加了一些通信开销（因为在前向和后向传播中需要更频繁地交换参数信息），但这种增加是有限的。</p>
<p><strong>Stage 2 中，总通讯量为</strong> <strong>φ (对梯度</strong> <strong>Reduce-Scatter)</strong> <strong>+ φ (对更新的参数</strong> <strong>All-Gather) = 2****φ</strong></p>
<p><strong>Stage 3 中，总通讯量为</strong> <strong>φ (前向传播时对参数</strong> <strong>All-Gather)</strong> <strong>+ φ (反向传播时对参数</strong> <strong>All-Gather) +</strong> <strong>φ (对梯度</strong> <strong>Reduce-Scatter) = 3****φ</strong></p>
</li>
</ul>
<p><strong>ZeRO-Animation</strong><br />
﻿<a href="https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">视频出处</a></p>
<p>ZeRO-DP 的核心思想是在进行 all-reduce 操作的时候，不需要把完整的结果 reduce 到所有 gpu 上，每个 gpu 只需要维护自己的那一块，在需要完整数据的时候，再进行通信拉取完整的数据。尽管 ZeRO 分片存储模型参数、梯度还有优化器状态，但是这并不是一种模型并行。<strong>ZeRO</strong> <strong>本质上就是一套数据并行的训练框架</strong>，只是加入了一套优化存储冗余的策略。</p>
<h5 id="zero-r"><a class="markdownIt-Anchor" href="#zero-r"></a> <strong>ZeRO-R</strong></h5>
<p>在 ZeRO-DP 提高了 model states 的 memory efficiency 之后，由<strong>激活数据、临时缓冲区和无法使用的内存碎片</strong>所消耗的剩余内存可能成为第二个内存瓶颈。ZeRO-R 针对这方面做了优化。</p>
<ul>
<li>
<p>**Activation: 把 activation checkpoints 也做分片，需要用时进行 All-gather 操作，**对于非常大的模型，ZeRO 甚至可以选择将 activation partition 移动到 CPU 内存中。</p>
</li>
<li>
<p><strong>Temporary buffers:</strong> 模型训练过程中经常会创建一些大小不等的临时缓冲区，比如对梯度进行 All-Reduce 啥的，解决办法就是**预先创建一个固定的缓冲区，**训练过程中不再动态创建，如果要传输的数据较小，则多组数据 bucket 后再一次性传输，提高效率。</p>
</li>
<li>
<p><strong>Fragmented Memory</strong>: 内存碎片化是由于短生命周期和长生命周期内存对象之间的交错造成的。在前向传播过程中，激活检查点是长生命周期的，但是重新计算的激活是短生命周期的。类似地，在反向计算中，激活梯度是短生命周期的，而参数梯度是长生命周期的。解决方法是<strong>预先分配一块连续的显存</strong>，将常驻显存的模型状态和checkpointed activation存在里面，剩余显存用于动态创建和销毁discarded activation，复用了操作系统对内存的优化，<strong>不断内存整理</strong>。</p>
</li>
</ul>
<h4 id="zero-offload-link"><a class="markdownIt-Anchor" href="#zero-offload-link"></a> ZeRO-Offload [<a href="https://arxiv.org/pdf/2101.06840">link</a>]</h4>
<p><strong>ZeRO-Offload 把一部分计算量小的活放到CPU上去做</strong></p>
<ul>
<li>
<p>将计算得到的梯度 offload 到 CPU</p>
</li>
<li>
<p>在CPU上维护优化器状态，如动量和方差</p>
</li>
<li>
<p>在CPU上执行参数更新计算</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140919964.png" alt="image.png" /></p>
<p><strong>CPU与GPU通信成本很大</strong>，为尽可能节省开销，GPU在计算梯度的同时也在把计算好的梯度传递给CPU，当计算梯度完成时，CPU也同时获得了所有的梯度，CPU更新参数的过程也是如此，<strong>动态同步更新</strong>参数。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140926960.png" alt="image.png" /></p>
<h5 id="扩展性"><a class="markdownIt-Anchor" href="#扩展性"></a> 扩展性</h5>
<p>在多卡场景，ZeRO-Offload 利用了 ZeRO-2，回忆下 ZeRO-2 是将 Adam 状态和梯度进行了分区，每张卡只保存<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，而 ZeRO-Offload 做的同样是将这<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>的 Adam 状态和梯度都 offload 到内存，在 CPU 上进行参数更新。</p>
<p><code>注意：在多卡场景，利用CPU多核并行计算，每张卡至少对应一个CPU进程，由这个进程负责进行局部参数更新。</code></p>
<p>并且CPU和GPU的通信量和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>无关，因为传输的是 fp16 gradient 和 fp16 parameter，总的传输量是固定的，由于利用多核并行计算，每个CPU进程只负责 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>的计算，反而随着卡数增加节省了CPU计算时间。</p>
<h4 id="zero-infinity-link"><a class="markdownIt-Anchor" href="#zero-infinity-link"></a> ZeRO-Infinity [<a href="https://arxiv.org/pdf/2104.07857">link</a>]</h4>
<p><strong>ZeRO-Infinity</strong> 是一种新颖的深度学习 (DL) 训练技术，用于将模型训练从单个 GPU 扩展到具有数千个 GPU 的大型超级计算机。它利用系统的全部内存容量，同时<strong>利用所有异构内存</strong>（GPU、CPU 和非易失性内存，简称 NVMe），为前所未有的模型规模提供支持。</p>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/04/1400x788_deepspeed_nologo-1.mp4">视频链接</a></p>
<p><strong>ZeRO-Offload v.s. ZeRO-Infinity</strong></p>
<p>DeepSpeed 在 ZeRO-2 中首次引入了与 ZeRO-Offload。ZeRO-Infinity 是用在 ZeRO-3 中的下一代 offload 功能。ZeRO-Infinity 能够卸载比 ZeRO-Offload 更多的数据，并且具有更有效地利用带宽以及计算和通信的重叠。</p>
<h3 id="pytorch"><a class="markdownIt-Anchor" href="#pytorch"></a> Pytorch</h3>
<h4 id="dpdata-parallelism"><a class="markdownIt-Anchor" href="#dpdata-parallelism"></a> <strong>DP（Data Parallelism）</strong></h4>
<p>数据并行主要介绍<code>torch.nn.DataParallel</code>，这是Pytorch最早提供的一种数据并行方式，它基于单进程多线程进行实现的，它使用<strong>一个进程来计算模型权重</strong>，在每个批处理期间将数据分发到每个GPU。</p>
<p><strong>处理过程</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140947155.png" alt="image.png" /></p>
<h4 id="ddpdistributed-data-parallelism"><a class="markdownIt-Anchor" href="#ddpdistributed-data-parallelism"></a> DDP（Distributed Data Parallelism）</h4>
<p>分布式数据并行(<code>torch.nn.DistributedDataParallel</code>)，基于多进程进行实现的，每个进程都有独立的优化器，执行自己的更新过程。每个进程都执行相同的任务，并且每个进程都与所有其他进程通信。进程（GPU）之间只传递梯度，这样网络通信就不再是瓶颈。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126140954823.png" alt="image.png" /></p>
<p>DP 和 DDP 的主要差异有以下几点：</p>
<ul>
<li>
<p>DP 是基于单进程多线程的实现，只用于单机情况，而 <strong>DDP 是多进程实现</strong>的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现分布式训练，并且因为每个进程都是独立的 Python 解释器，DDP 避免了 GIL 带来的性能开销。</p>
</li>
<li>
<p>参数更新的方式不同。DDP在各进程完成前向传播后，<strong>梯度在各个 GPUs 间进行 Ring All-Reduce</strong>，每个 GPU 都收到其他 GPU 的梯度，从而可以独自进行反向传播和参数更新。（而 DP是梯度汇总到 GPU0，反向传播更新参数，再广播参数给其他剩余的 GPU）</p>
</li>
<li>
<p>通信效率。<strong>DataParallel</strong> 是将梯度 reduce 到主卡，在主卡上更新参数，再将参数 broadcast 给其他 GPU，这样<strong>无论是主卡的负载还是通信开销都比 DDP 大很多</strong></p>
</li>
</ul>
<h4 id="fsdpfully-sharded-data-parallel"><a class="markdownIt-Anchor" href="#fsdpfully-sharded-data-parallel"></a> FSDP（Fully Sharded Data Parallel）</h4>
<p><strong>ZeRO stage 3 的实现</strong></p>
<h2 id="4-实战"><a class="markdownIt-Anchor" href="#4-实战"></a> 4. <strong>实战</strong></h2>
<p>使用的是 seq_len=1024，hidden_size=1024，attention-head_size=16，layer_size=24，模型参数大小为3.45亿的 Transformer 模型。batch_size=8，实验结果如下</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gpu 数</td>
<td>ZeRO Stage</td>
<td>张量并行度</td>
<td>流水线并行度</td>
<td>micro batch 大小</td>
<td>每个 Iteration 训练时间 (s)</td>
<td>单卡显存占用 (MB)</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>0.63</td>
<td>8856</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>0.40</td>
<td>8180</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>0.48</td>
<td>5579</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>0.46</td>
<td>4702</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>0.65</td>
<td>4501</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>4</td>
<td>0.54</td>
<td>4618</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>0.40</td>
<td>4903</td>
</tr>
</tbody>
</table>
<h2 id="附录"><a class="markdownIt-Anchor" href="#附录"></a> 附录</h2>
<h3 id="集合通信原语"><a class="markdownIt-Anchor" href="#集合通信原语"></a> <strong>集合通信原语</strong></h3>
<ul>
<li>
<p><strong>Broadcast</strong>：主节点把自身的数据发送到集群中的其他节点。分布式训练系统中常用于网络参数的初始化。如图4.21所示，计算设备 1 将大小为 1 × N 的张量进行广播，最终每张卡输出均为 [1 × N] 的矩阵。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141016191.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Scatter</strong>：主节点将数据进行划分并散布至其他指定的节点。Scatter 与 Broadcast 非常相似，但不同的是，Scatter 是将数据的不同部分，按需发送给所有的进程。如下图所示，计算设备1 将大小为 1 × N 的张量分为 4 份后发送到不同节点。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141023264.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Reduce</strong>：是一系列简单运算操作的统称，是将不同节点上的计算结果进行聚合（Aggregation），可以细分为：SUM、MIN、MAX、PROD、LOR 等类型的规约操作。如下图所示，ReduceSum 操作将所有其它计算设备上的数据汇聚到计算设备 1，并执行求和操作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141029266.png" alt="image.png" /></p>
</li>
<li>
<p><strong>All Reduce</strong>：在所有的节点上都应用同样的 Reduce 操作。All Reduce 操作可通过单节点上Reduce + Broadcast 操作完成。如下图所示，All Reduce Sum 操作将所有计算设备上的数据汇聚到各个计算设备中，并执行求和操作。<a href="https://zhuanlan.zhihu.com/p/69797852">Ring All-Reduce 算法</a></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141045545.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Gather</strong>：将多个节点上的数据收集到单个节点上，Gather 可以理解为反向的 Scatter。如下图所示，Gather 操作将所有计算设备上的数据收集到计算设备 1 中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141052716.png" alt="image.png" /></p>
</li>
<li>
<p><strong>All Gather</strong>：将所有节点上收集其他所有节点上的数据，All Gather 相当于一个 Gather 操作之后跟着一个 Broadcast 操作。如下图所示，All Gather 操作将所有计算设备上的数据收集到每个计算设备中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141058165.png" alt="image.png" /></p>
</li>
<li>
<p><strong>Reduce Scatter</strong>：将每个节点中的张量切分为多个块，每个块分配给不同的节点。接收到的块会在每个节点上进行特定的操作，例如求和、取平均值等。如下图所示，每个计算设备都将其中的张量切分为 4 块，并分发到 4 个不同的计算设备中，每个计算设备分别对接收到的分块进行特定操作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141105174.png" alt="image.png" /></p>
</li>
<li>
<p><strong>All to All</strong>：将每个节点的张量切分为多个块，每个块分别发送给不同的节点。如下图所示，每个计算设备都将其中的张量切分为 4 块，并分发到 4 个不同的计算设备中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126141111968.png" alt="image.png" /></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>训练技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>序列并行</title>
    <url>/2025/01/26/%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[<h2 id="sequence-parallelism"><a class="markdownIt-Anchor" href="#sequence-parallelism"></a> <strong>Sequence Parallelism</strong></h2>
<p>Megatron 在他们的 Tensor Parallelism 的基础上，将 Transformer 核的 LayerNorm 以及 Dropout 层的输入**按 Sequence Length 维度进行了切分，**在 self-attention 计算前聚合（all-gather）了sequence的内容</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134624926.png" alt="image.png" /></p>
<h2 id="context-parallelism"><a class="markdownIt-Anchor" href="#context-parallelism"></a> <strong>Context Parallelism</strong></h2>
<p>相当于 Sequence Parallelism 的升级版，<strong>解决SP中未完成的self-attention序列并行问题</strong>。</p>
<span id="more"></span>
<p>Self-attention的计算里面为什么需要完整的序列？ Attention中QKV的计算需要用到一个完整sequence信息，计算上的耦合使得该模块不能先运算后进行简单拼接。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5864385em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622307142857143em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222307142857144em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17776928571428574em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></p>
<h3 id="ring-attention"><a class="markdownIt-Anchor" href="#ring-attention"></a> Ring Attention</h3>
<ul>
<li><strong>Step 1 -</strong> <strong>数据切分</strong>：Q/K/V拆分成数据 [Q0,Q1,Q2]/[K0,K1,K2]/[V0,V1,V2] ，CP设备组分为rank0、rank1、rank2，每个rank拿到固定[b, sq/3, np, hd]大小数据Q；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134736786.png" alt="image.png" /></p>
<ul>
<li><strong>Step 2 - Attention计算</strong>：第一次计算时，rank拿一份K、V数据，比如rank0拿到K0，V0。计算通过FA(FlashAttention)2模块完成获得第一个输出O_00。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134745070.png" alt="image.png" /></p>
<ul>
<li><strong>Step 3 - KV数据交换</strong>：计算的同时，可以进行数据交换。每个rank与相邻的rank进行环形P2P通信，传出自己的KV值，同时拿到下一次需要运算的数据，一共需要完成CP−1次通信。如rank0，算完K0,V0后下次需要运算的数据为K2, V2，从rank2获取；同时，rank0将K0V0数据传递给rank1。这个步骤可以和 Step 2 重叠，提高通讯效率。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134754970.png" alt="image.png" /></p>
<ul>
<li><strong>Step 4 - 计算最终输出:</strong> 每个rank的Q与KV匹配计算完后获得三个输出值，然后进行结果修正得到[O_X0, O_X1, O_X2]，X值为rank序号。最后每个rank将自己的分块结果进行聚合（加法）运算得到结果O_X。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134802866.png" alt="image.png" /></p>
<p><strong>通信量</strong>：假设我们有一个长度为 N 的输入序列，每个位置都用一个d维向量表示，增加了p2p通信，通信量为 2∗N∗d∗(cp−1)/ cp个单位</p>
<p><strong>内存</strong>：假设每个rank只有一个buffer大小，QKV输入的显存变为 5∗N∗d/cp 个单位</p>
<h3 id="deepspeed-ulysses"><a class="markdownIt-Anchor" href="#deepspeed-ulysses"></a> Deepspeed Ulysses</h3>
<p><a href="https://arxiv.org/pdf/2309.14509">论文链接</a></p>
<p><strong>DeepSpeed-Ulysses的核心设计</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134810851.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134818290.png" alt="image.png" /></p>
<p>DeepSpeed-Ulysses 的核心在于切分 Q、K、V 后进行的 All-to-All 通信方式，这个通信方式同 Allreduce 一样，是分布式训练中的 Collective functions，All-to-All 在每个进程向每个其他进程发消息的一部分，最后处理器拥有各个进程消息的一部分。他的作用相当于分布式转置Transpose操作</p>
<ul>
<li><strong>All to All</strong>：将每个节点的张量切分为多个块，每个块分别发送给不同的节点。如下图所示，每个计算设备都将其中的张量切分为 4 块，并分发到 4 个不同的计算设备中。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134825254.png" alt="image.png" /></p>
<p>整个过程分成两步：</p>
<ol>
<li>对 K、Q、V，分别按照序列长度进行分割，得到 Local-Q、Local-K、Local-V，这里的 P 表示分割的数量，也就是对每一个 GPU 放入的序列长度是 N/P，通过 All-to-All 通信，获取序列长度为 N 但进行注意力头切割的</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134831348.png" alt="image.png" /></p>
<ol>
<li>计算O_h，再通过 All-to-All 通信，获取 Local-O，在每个进程内，使用 Local O，进行 后续操作。这样去做的意义就在于对于每一个设备送入处理的序列长度只是 N/P，这样大大减少了设备的运算量，但同时，因为 All-To-All 通信方式的存在，使得即使每个设备被送入的序列长度只有 N/P，但他们做 Attention 时还是考虑了整体的长度，可谓巧妙。</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126134839410.png" alt="image.png" /></p>
<p>缺点：Ulysses也有明显缺点，就是转置后切分维度d/P，我们希望d/P=hc/P * head_size，即对head_cnt所在维度切分，这样Attention的计算都在一张卡上完成，从而可以使用FlashAttention等单卡优化。但是如果遇到GQA或者MQA情况，K、V的head_cnt很小，导致GPU数目P也不能变得很大。</p>
<h3 id="通讯开销"><a class="markdownIt-Anchor" href="#通讯开销"></a> 通讯开销</h3>
<p>DeepSpeed-Ulysses与其他现有的长序列方法的区别在于其更小的累积通信量以及随着序列并行度增加而更好的可扩展性，如下所示：</p>
<p>在具有节点内NVSwitch互连和节点间胖树IB拓扑的现代集群上，针对一个聚合消息大小为_M_的全对全传输，传输到_P_个GPU上的每个链接的通信量为_M/P_。 对于隐藏层大小为h、序列长度为N且并行度为P的Transformer模型，DeepSpeed序列并行会在注意计算之前对QKV投影执行聚合消息大小为_3Nh_的全对全操作，并在注意计算之后对输出上下文投影执行大小为_Nh_的另一个全对全操作。因此，DeepSpeed序列并行每个链接的聚合通信量为_<strong>4Nh/P（或O(N/P)复杂度）</strong>_。值得注意的是，当N和P成比例增加时，这个通信量是恒定的。</p>
<p>相比之下，现有的方法，如Megatron-LM，在N线性增长的情况下会导致通信量线性增加，而与P无关，从而导致_<strong>O(N)的通信复杂度</strong><em>。例如，Megatron-LM对每个Transformer模型层都执行两个大小为_Nh_的_all-gather_操作，以及两个大小为_Nh_的_reduce-scatter_操作。然而，当_P &gt;&gt; 1_时，大小为M的每个all-gather和reduce-scatter的成本仍然是M，而不是_M/P</em>。因此，Megatron-LM序列并行会导致每个链接的通信量为_<strong>4Nh</strong>_，这比DeepSpeed序列并行大P倍。这使得DeepSpeed序列并行可以在实现显著更高的训练效率的同时支持极长序列训练。我们的实验评估结果与此理论分析相符。</p>
<h2 id="ringattention-对比-deepspeed-ulysses"><a class="markdownIt-Anchor" href="#ringattention-对比-deepspeed-ulysses"></a> RingAttention 对比 DeepSpeed-Ulysses</h2>
<p><strong>DeepSpeed-Ulysses的问题：</strong></p>
<ul>
<li>
<p>需要在head_num维度切分，对并行度的约束较大，且在切分上和 tp 会互相限制</p>
</li>
<li>
<p>通信和计算无法重叠</p>
</li>
</ul>
<p><strong>RingAttention的问题：</strong></p>
<ul>
<li>
<p>ring 的计算方式导致完整的 fa kernel 被拆分成多个小 kernel，且计算完成后，还需要调用一些算子来更新 output 和 lse。gpu调度和计算效率会下降，也会增加 kernek launch 的开销</p>
</li>
<li>
<p>使用p2p通信而非集合通信，通信带宽较小，且有死锁的风险</p>
</li>
<li>
<p>若通信和计算无法重叠，性能较差</p>
</li>
</ul>
<p><strong>DeepSpeed-Ulysses的优点：</strong></p>
<ul>
<li>
<p>使用节点内 NVSwitch 和节点间 fat tree 的通信网络时，all2all 的通信量较小</p>
</li>
<li>
<p>gqa 可以降低通信量</p>
</li>
</ul>
<p><strong>RingAttention的优点：</strong></p>
<ul>
<li>
<p>并行度自由，可以无限切分</p>
</li>
<li>
<p>gqa 可以降低通信量</p>
</li>
<li>
<p>序列长度或模型规模变大时，通信可以被计算覆盖</p>
</li>
</ul>
<h2 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h2>
<p><a href="https://blog.csdn.net/m0_51341794/article/details/137789342">https://blog.csdn.net/m0_51341794/article/details/137789342</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>训练技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>Chunked Prefill</title>
    <url>/2025/01/26/Chunked-Prefill/</url>
    <content><![CDATA[<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> <strong>Introduction</strong></h2>
<ol>
<li>
<p><strong>Prefill 阶段会并行处理输入 prompt 的所有 token，因此很小的 batch size 就会打满GPU utilization</strong> 比如说，13B 的 LLaMA 输入一条 512 tokens 的 prompt 做 prefill 就会打满单卡 A6000。<strong>反过来，（开启KV Cache）的 decode 阶段每个自回归阶段仅仅会生成一个 token，因此 GPU utilization 很低</strong></p>
</li>
<li>
<p>Decode 时，单个 token 的开销显著大于 prefill 阶段；增大 batch size，prefill 的单个 token 开销几乎是不变的，而 decode 的开销显著下降。可见，prefill 在 batch size 很小时就已经占满了 GPU 效率，而 decode 阶段在 batch size 很大时才会占满。实际上后文会说明，prefill 阶段的输入 L 很长，因此计算开销大；而 decode 阶段输入的 L 一直是 1，但是需要反复读读 KV Cache，故而 IO 开销很大。</p>
</li>
<li>
<p>需要把 decode 阶段的 batch size 开的非常大才有可能占满 GPU utilization，但是开这么大的 batch size 会因为 KV Cache 读写开销太大而变得不现实，所以 docode 阶段的 GPU utilization 是很难占满的。因此，在实际情况下，decode 仍旧是 memory bounded 而非 compute bounded 的。</p>
</li>
<li>
<p>一条 prompt 会被 prefill 一次，但是会 decode 多次，直到 decode 满足终结条件，譬如 end token 或者长度限制。</p>
</li>
<li>
<p>Tensor Parallize 卡间通讯需求大，而 Pipeline Paralize 需要不断优化 pipeline bubble。</p>
</li>
<li>
<p>Chunked Prefill 做出了两步优化。首先将长短不一的 prompts 拆分为长短一致的 chunks 进行 prefill；其次这些 chunks 间的气泡可以插入/捎带（piggyback）其他完成了 prefill 的 prompts 的 decode 需求。</p>
</li>
</ol>
<span id="more"></span>
<h2 id="transformer-architecture"><a class="markdownIt-Anchor" href="#transformer-architecture"></a> Transformer Architecture</h2>
<ol>
<li>
<p>Transformer decoder block 在计算上可以看做六个操作的总和：pre-proj，attn，post-proj，ffn_ln1，ffn_ln2，others（比如说 layer normalization，activation functions，residual connection…）</p>
</li>
<li>
<p>Transformer 的输出可以视为一个 tensor X of shape [B, L, H]。其中 B 是 batch size，L 是 input tokens length，H 是模型的 embedding size。</p>
</li>
<li>
<p>Prefill 的第一步做 pre-proj。从数学上，就是简单的线性运算，分别用三个大小为 [H, H] 的矩阵 W<sup>Q,W</sup>K,W^V 和 X 做乘积。从计算上，就是输入 X 和一个大小为 [H, 3H] 的矩阵相乘。</p>
</li>
<li>
<p>attn 操作在计算上的输入是 Q, K, V，而输出 Y 仍旧是大小为 [B, L, H] 的 tensor。post-proj 采用大小为 [H, H] 的 W_0 矩阵和 Y 相乘，输出结果 Z 的大小仍旧为 [B, L, H]。</p>
</li>
<li>
<p>ffn_ln1 和 ffn_ln2 在计算上的输入是 Z。ffn_ln1 中，Z 和大小为 [H, H’] 的矩阵相乘，得到大小为 [B, L, H’] 的 tensor，接着和大小为 [H’, H] 的矩阵相乘，再投影回去得到一个大小仍旧为 [B, L, H] 的 tensor。上式中，H’ 为模型的 second hidden dimension。</p>
</li>
<li>
<p>Decode 阶段和 Prefill 阶段的操作完全一致，不过每次只会生成一个 token 并且输入给下个阶段。采用 KV Cache 后，实质上的输入是上次生成的那一个 token，输入的 tensor 大小是 [B, 1, H]（input tokens 的长度为 1）。</p>
</li>
<li>
<p>每个 token 的 KV Cache 大小均为 [1, H]。</p>
</li>
</ol>
<h2 id="多-gpu-推理"><a class="markdownIt-Anchor" href="#多-gpu-推理"></a> <strong>多 GPU 推理</strong></h2>
<ol>
<li>
<p>tensor parallelism 在单机多卡且卡间通讯强时较优；而 pipeline parallism 主要在多机情况下用于 serve 大到没法在单机上运行的模型。</p>
</li>
<li>
<p>pipeline parallism 是卡间通讯不好时唯一可行的 model parallelism 方法。</p>
</li>
<li>
<p>PP 将模型按照 layer 分开，每个 GPU 负责一部分 layer；而 TP shards 【TODO】每个 layer 到所有 GPU 上。PP 比起 TP 具有更好的 compute-communication ratio，因而不要求昂贵的卡间高速通讯。</p>
</li>
</ol>
<h2 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> <strong>Motivation</strong></h2>
<ol>
<li>
<p>当前的推理框架存在两个低效原因，首先是 decode 阶段的 memory boundary，第二是 pipeline parallelism 带来的 pipeline bubble。</p>
</li>
<li>
<p>在 transformer block 中，除开五个主要部分之外的 others 占据的开销不超过 5%。</p>
</li>
<li>
<p>inference 只会做 forward passes 而没有 training 中的 backward。</p>
</li>
<li>
<p>基于如上的分析可见 prefill 和 decode 具有很不一样的优化目标，因此 chunked prefill 进行了两点优化：对 prefill 进行数学上等价的切分；在 prefill 切成的这些 chunks 的气泡处捎带其他 request 的 decode pass。</p>
</li>
</ol>
<h2 id="具体实现"><a class="markdownIt-Anchor" href="#具体实现"></a> <strong>具体实现</strong></h2>
<ol>
<li>
<p>随着模型 hidden size 增大，更小的 chunk size 就会打满 gpu。</p>
</li>
<li>
<p>实际上模型部署时有着非常长的 system prompts，因此 chunk 是可行的。</p>
</li>
<li>
<p>如果 chunk size 开的非常小，那么 prefill 的效率会因为 GPU 利用率变低而降低。</p>
</li>
<li>
<p>Chunked prefill 需要特殊处理﻿attention mask。</p>
</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155845842.png" alt="image.png" /></p>
<ol>
<li>
<p>做了 chunked prefill 后，prefill 的开销会略微增大。因为计算后续 chunk 的 KV 时需要不断地从 GPU memory 中里读出当前 chunk 的 KV 到 kernal 里面；而不做 chunked prefill 时，最开端的那些 KV Cache 可以不用反复从 GPU memory 中反复读取进入 kernal，毕竟他们一直在 kernal 里面。</p>
</li>
<li>
<p>即便如此，我们仍旧要做 chunked prefill，因为做了 chunk 之后，可以在 chunk 的 bubble 处捎带 decode 请求。这么做是有利于 decode 的，因为 decode 的 memory 开销除了要从 GPU memory 中 fetch KV Cache 之外，还有一部分开销是要 fetch 模型参数。采用 piggyback 的方式捎带 decode 到 chunk 的 bubble 后（称为 decode-maximal batching），可以直接 reuse prefill 阶段 fetch 的 模型参数。如此操作，几乎可以让 decode 从一个 memory bound 操作转换为一个 compute bound 操作。经过测试，通过捎带的 decode 的耗时会显著降低到原本的 10%。</p>
</li>
<li>
<p>由此可见【TODO】，更小的 chunk size 可以捎带更多的 decode 请求，但是降低了 prefill 效率；chunk size 也是一个 trade off 罢。</p>
</li>
<li>
<p>sequence length 整除 GPU tile size 时，GPU 的效率最高；反过来，仅仅增加 1 个 token length 都可能显著降低 GPU 效率。</p>
</li>
<li>
<p>可见，一句话总结：chunked prefill 一大意义在于利用 model parameters resue 来降低 decode 的开销。此外，也减小了 pipeline bubble 的影响。</p>
</li>
</ol>
<h1 id="vllm-中的chunked-prefill"><a class="markdownIt-Anchor" href="#vllm-中的chunked-prefill"></a> VLLM 中的Chunked Prefill</h1>
<p>vLLM 支持一个实验性功能——分块预填充（chunked prefill）。分块预填充可以将大规模的预填充任务分解为较小的块，并将其与解码请求一起批量处理。</p>
<p>你可以通过以下方式启用该功能：</p>
<ul>
<li>
<p>在命令行中指定 <code>--enable-chunked-prefill</code> 参数。</p>
</li>
<li>
<p>或者在 LLM 构造函数中设置 <code>enable_chunked_prefill=True</code>。</p>
</li>
</ul>
<p>示例代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">llm = LLM(model=&quot;meta-llama/Llama-2-7b-hf&quot;, enable_chunked_prefill=True)</span><br><span class="line"># 设置 max_num_batched_tokens 参数以优化性能。</span><br><span class="line"># 注意：默认情况下，chunked prefill 的 max_num_batched_tokens 是 512。</span><br><span class="line"># llm = LLM(model=&quot;meta-llama/Llama-2-7b-hf&quot;, enable_chunked_prefill=True, max_num_batched_tokens=512)</span><br></pre></td></tr></table></figure>
<h2 id="默认调度策略"><a class="markdownIt-Anchor" href="#默认调度策略"></a> 默认调度策略</h2>
<p>默认情况下，vLLM 调度器优先处理预填充任务（prefill），并且不会将预填充和解码任务批处理到同一个批次中。这种策略可以优化 <strong>TTFT</strong>（首次生成 token 的时间），但会导致 <strong>ITL</strong>（生成 token 之间的延迟）较高，以及 GPU 利用率效率较低。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155853788.png" alt="image.png" /></p>
<h2 id="启用分块预填充后的策略"><a class="markdownIt-Anchor" href="#启用分块预填充后的策略"></a> 启用分块预填充后的策略</h2>
<p>启用分块预填充后，调度策略会改变为优先处理解码请求：</p>
<ol>
<li>
<p>首先将所有待处理的解码请求打包到一个批次中。</p>
</li>
<li>
<p>当有剩余的 token 预算（<code>max_num_batched_tokens</code>）时，调度器会处理待处理的预填充请求。</p>
</li>
<li>
<p>如果最后一个预填充请求无法装入 <code>max_num_batched_tokens</code> 中，它会被分块处理。</p>
</li>
</ol>
<h2 id="优势"><a class="markdownIt-Anchor" href="#优势"></a> 优势</h2>
<p>这种策略有以下两个优势：</p>
<ol>
<li>
<p><strong>提升 ITL 和解码性能：</strong> 因为解码请求被优先处理。</p>
</li>
<li>
<p><strong>提高 GPU 利用率：</strong> 通过将计算密集型（预填充）和内存密集型（解码）任务安排到同一个批次中。</p>
</li>
</ol>
<h2 id="性能优化"><a class="markdownIt-Anchor" href="#性能优化"></a> 性能优化</h2>
<p>你可以通过调整 <code>max_num_batched_tokens</code> 参数来优化性能：</p>
<ul>
<li>
<p>默认值为 2048，这是在初步基准测试（Llama 70B 和 mixtral 8x22B 模型）中针对 A100 GPU 获得的最佳 ITL 设置。</p>
<ul>
<li>
<p><strong>较小的</strong> <code>max_num_batched_tokens</code><strong>：</strong> 减少预填充对解码的中断，从而实现更低的 ITL。</p>
</li>
<li>
<p><strong>较大的</strong> <code>max_num_batched_tokens</code><strong>：</strong> 可以在批处理中加入更多的预填充任务，从而降低 TTFT。</p>
</li>
</ul>
</li>
<li>
<p>如果将 <code>max_num_batched_tokens</code> 设置为与 <code>max_model_len</code> 相同，则几乎等同于默认调度策略（但仍优先处理解码任务）。</p>
</li>
</ul>
<h2 id="注意事项"><a class="markdownIt-Anchor" href="#注意事项"></a> 注意事项</h2>
<ol>
<li>
<p>默认值 2048 的 <code>max_num_batched_tokens</code> 优化了 ITL，但可能导致吞吐量低于默认调度器。</p>
</li>
<li>
<p>如果需要更高的吞吐量，建议设置 <code>max_num_batched_tokens &gt; 2048</code>。</p>
</li>
</ol>
<h2 id="vllm-max_num_batched_tokens和-max_num_seqs"><a class="markdownIt-Anchor" href="#vllm-max_num_batched_tokens和-max_num_seqs"></a> VLLM <code>max_num_batched_tokens</code>和 <code>max_num_seqs</code></h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155902723.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>Conditional Variable</title>
    <url>/2025/01/26/Conditional-Variable/</url>
    <content><![CDATA[<p><strong>条件变量和互斥锁的配合</strong><br />
消费者 Thread:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="comment">// 为线程环境加锁，互访问工作线程的休眠和唤醒</span></span><br><span class="line">		<span class="function">unique_lock&lt;mutex&gt; <span class="title">lock</span><span class="params">(mtx)</span></span>;​</span><br><span class="line">		<span class="comment">// 如果任务队列为空，阻塞当前线程</span></span><br><span class="line">		<span class="keyword">while</span> (queue.<span class="built_in">empty</span>())</span><br><span class="line">		&#123;</span><br><span class="line">			conditional_lock.<span class="built_in">wait</span>(lock); <span class="comment">// 等待条件变量通知，开启线程</span></span><br><span class="line">		&#125;​</span><br><span class="line">		<span class="comment">// 取出任务队列中的元素</span></span><br><span class="line">		dequeued = queue.<span class="built_in">dequeue</span>();</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Submit a task:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">unique_lock&lt;mutex&gt; <span class="title">lock</span><span class="params">(mtx)</span></span>;</span><br><span class="line"><span class="comment">// 压入任务队列</span></span><br><span class="line">queue.<span class="built_in">enqueue</span>(task);</span><br><span class="line"><span class="comment">// 唤醒一个等待中的线程</span></span><br><span class="line">conditional_lock.<span class="built_in">notify_one</span>();</span><br></pre></td></tr></table></figure>
<p><strong>conditional_wait 做了什么事？</strong><br />
conditional_wait 将解锁互斥锁，允许其他人访问条件变量（用于发信号）。然后，当条件变量收到信号或广播时，等待列表中的一个或多个线程将被唤醒，并且该线程的互斥体将再次锁定。整个操作是原子的</p>
<p><strong>如果不用条件变量会怎么样？</strong><br />
消费者 Thread:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="function">unique_lock&lt;mutex&gt; <span class="title">lock</span><span class="params">(mtx)</span></span>;​</span><br><span class="line">		<span class="keyword">if</span> (!queue.<span class="built_in">empty</span>())</span><br><span class="line">		&#123;</span><br><span class="line">			dequeued = queue.<span class="built_in">dequeue</span>();</span><br><span class="line">			...</span><br><span class="line">		&#125;​		</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不停加锁释放锁 占用资源</p>
<p><strong>为什么需要锁？</strong><br />
保护共享条件变量，如果没有mutex，<code>signal()</code>可能发生在<code>cond</code>判断和<code>wait()</code>之间，会错过这次唤醒。</p>
]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>Continuous batching</title>
    <url>/2025/01/26/Continuous-batching/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://zhuanlan.zhihu.com/p/719610083?utm_psn=1817314936814198784">https://zhuanlan.zhihu.com/p/719610083?utm_psn=1817314936…</a></p>
</blockquote>
<p>由于类 GPT 的仅编码器模型推理分为预填充和解码两个阶段。在解码阶段一次推理只输出一个token，输出的 token 会与输入 tokens 拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。这样会造成大量的冗余计算。同时用由于仅编码器模型的 Self Attention 中带 Masked ，因此，在推理的时候，前面已经生成的 Token 不需要与后面的 Token 产生 Attention ，从而使得前面已经计算的 K 和 V 可以缓存起来。因此，KV Cache 应运而生。之前针对 KV Cache 技术进行了讲述，KV Cache 是一种典型的以空间换时间（或者叫以内存换计算）的优化技术提升推理速度从而降低延迟。除了从模型视角优化推理的性能，对于一个系统而言，还可以以更高的视角从系统层面来考虑优化整体的模型服务性能。</p>
<p>而本文将介绍大模型服务请求调度优化技术 Continuous batching 通过提高硬件利用率从而提升系统的系统的性能（吞吐量）。</p>
<span id="more"></span>
<h2 id="批处理基本概念"><a class="markdownIt-Anchor" href="#批处理基本概念"></a> 批处理基本概念</h2>
<h3 id="单处理"><a class="markdownIt-Anchor" href="#单处理"></a> 单处理</h3>
<p>单处理也就是不组成Batch或者说一个Batch中数据始终为1，也就是单个提示（Prompt）传过来直接送入到LLM进行推理。</p>
<p>单条数据虽然简单而且灵活，不过因为每次只能处理一条数据，对GPU资源的利用率较低。因此，我们通常会将多条数据放入一个批次进行处理，从而提升GPU的利用率。</p>
<h3 id="静态批处理static-batching"><a class="markdownIt-Anchor" href="#静态批处理static-batching"></a> 静态批处理（static batching）</h3>
<p>静态批处理指将多个Prompt打包进行一个批处理请求，并在批处理请求中所有Prompt完成后返回响应，批处理的大小在推理完成之前保持不变。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153820708.png" alt="image.png" /></p>
<p>与传统的深度学习模型不同，由于LLM推理具有迭代性质，一个批次中不同序列的生成长度不同。有的Prompt在批处理中较早“完成”，但需等待这一批次中Prompt最长的生成结束，因此，GPU 未得到充分利用。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153827429.png" alt="image.png" /></p>
<h3 id="动态批处理dynamic-batching"><a class="markdownIt-Anchor" href="#动态批处理dynamic-batching"></a> 动态批处理（Dynamic batching）</h3>
<p>动态批处理是指允许将一个或多个推理请求组合成单个批次（必须动态创建）以最大化吞吐量的功能。就 Triton 推理服务框架而言，Triton 会对这些输入请求进行批处理，没有任何延迟，但用户可以选择为调度程序分配有限的延迟，以收集更多推理请求供动态批处理程序使用。</p>
<p>如下图所示，在不使用动态批处理的情况下，所有请求都会按顺序处理，这意味着需要 <code>5X ms</code> 处理所有请求。这个过程非常浪费，因为每个批处理可以比顺序处理执行更多的数据。</p>
<p>使用动态批处理可以更有效地将请求打包到 GPU 内存中，从而显著加快的速度。它还减少了整体响应的延迟（需要 <code>3X ms</code> 处理所有请求），因为可以在更短的周期内处理更多的查询。</p>
<p>如果考虑使用延迟，以在一个批处理中收集更多推理请求，则需要 <code>2.5X ms</code> 处理所有请求。</p>
<p>当然以上都是理想状态，实际情况并非所有执行元素都可以完美并行，从而导致较大批次的执行时间更长。但可以看到，使用动态批处理可以改善模型服务时的延迟和吞吐量。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153833578.png" alt="image.png" /></p>
<h3 id="变长输入批处理ragged-batching"><a class="markdownIt-Anchor" href="#变长输入批处理ragged-batching"></a> 变长输入批处理（Ragged Batching）</h3>
<p>Triton 中提供动态批处理功能，它结合同一模型执行的多个请求，以提供更大的吞吐量。默认情况下，仅当请求中的每个输入具有相同的形状时，才能对请求进行动态批处理。为了在输入形状经常变化的情况下利用动态批处理，<strong>客户端需要将请求中的输入张量填充为相同的形状</strong>。</p>
<p>而 Ragged batching 是一种通过允许用户指定哪些输入<strong>不需要形状检查来避免显式填充</strong>的功能。</p>
<p>比如，Triton 中用户可以通过在模型配置中设置 <code>allow_ragged_batch</code> 字段来指定此类输入(ragged input)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input [</span><br><span class="line">  &#123;</span><br><span class="line">    name: &quot;input0&quot;</span><br><span class="line">    data_type: TYPE_FP32</span><br><span class="line">    dims: [ 16 ]</span><br><span class="line">    allow_ragged_batch: true</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>如何在一个批次请求中处理不规则的输入取决于后端实现。 ONNX Runtime 后端、TensorFlow 后端、PyTorch 后端和 TensorRT 后端等后端要求模型接受不规则的输入作为一维张量。这些后端将请求输入连接到一维张量中。</p>
<p>由于连接的输入不会跟踪每个请求的开始和结束索引，因此后端通常要求模型具有额外的输入（批量输入），以描述所形成的批次的各种信息。</p>
<p>下面是一个变长输入批处理的示例：</p>
<p>如果您的模型接受 1 个可变长度输入张量 INPUT，其形状为 [ -1, -1 ]。第一个维度是批量维度，第二个维度是变长内容。当客户端发送 3 个形状为 [ 1, 3 ]、[ 1, 4 ]、[ 1, 5 ] 的请求时。为了利用动态批处理，实现该模型的直接方法将期望 INPUT 形状设置为 [ -1, -1 ] 并假设所有输入都填充到相同的长度，以便所有请求都变为形状 [ 1, 5 ]，因此， Triton 可以批处理并将它们作为单个 [ 3, 5 ] 张量发送到模型。在这种情况下，填充张量以及对填充内容进行额外的模型计算都会产生开销。下面是输入配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input [</span><br><span class="line">  &#123;</span><br><span class="line">    name: &quot;input0&quot;</span><br><span class="line">    data_type: TYPE_FP32</span><br><span class="line">    dims: [ 16 ]</span><br><span class="line">    allow_ragged_batch: true</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>使用 Triton Ragged batching ，模型将被实现为期望 INPUT 形状 [ -1 ] 和额外的批处理输入 INDEX，形状 [ -1 ]，模型使用它来解释 INPUT 中的批处理元素。对于这种模型，客户端请求不需要填充，可以按原样发送（形状为 [ 1, 3 ]、[ 1, 4 ]、[ 1, 5 ]）。后端会将输入批处理为形状 [12] 的张量，其中包含3个请求的连接（ 3 + 4 + 5）。 Triton 还创建形状为 [ 3 ] 且值为 [ 3, 7, 12 ] 的批量输入张量，该张量给出了每个批量元素结束处的输入张量的偏移量。下面是输入配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">max_batch_size: 16</span><br><span class="line">input [</span><br><span class="line">  &#123;</span><br><span class="line">    name: &quot;INPUT&quot;</span><br><span class="line">    data_type: TYPE_FP32</span><br><span class="line">    dims: [ -1 ]</span><br><span class="line">    allow_ragged_batch: true</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line">batch_input [</span><br><span class="line">  &#123;</span><br><span class="line">    kind: BATCH_ACCUMULATED_ELEMENT_COUNT</span><br><span class="line">    target_name: &quot;INDEX&quot;</span><br><span class="line">    data_type: TYPE_FP32</span><br><span class="line">    source_input: &quot;INPUT&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>上面的示例使用 <code>BATCH_ACCUMULATED_ELEMENT_COUNT</code> 类型的不规则批处理。</p>
<p>使用ragged batching 需要实际的模型支持才行（如何处理ragged batch和index）。实际 LLM 推理中用到的比较多，比如：TensorRT-LLM 中 kernel 实现的时候已经考虑到了这种情况。在图像场景中，ragged batch一般用不上。</p>
<h3 id="连续批处理continuous-batching"><a class="markdownIt-Anchor" href="#连续批处理continuous-batching"></a> 连续批处理（Continuous Batching）</h3>
<p>无论是动态批处理还是静态批处理，通常在相同形状的输入和输出请求的场景，提高GPU的利用率。但对于自回归大模型推理场景而言，都不太适用（同一批次中的数据输入和输出长度都不一样）。为了提高生成式大模型推理场景GPU的利用率，Continuing Batching 应运而生。</p>
<p>Continuing Batching（有的地方也叫做 Inflight batching 或者 Iteration batching）指请求在到达时一起批量处理，但它不是等待批次中所有序列都完成，而是当一个输入提示生成结束之后，就会在其位置将新的输入Prompt插入进来，从而比静态批处理具备更高的 GPU 利用率。<strong>由于每次迭代的批处理大小是动态的，因此，有些地方也叫动态Batching，本文为了避免概念混淆后续都叫Continuing Batching</strong>。可以说，Continuous batching 几乎是所有优秀 LLM 推理框架的必备技术。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153942661.png" alt="image.png" /></p>
<h3 id="静态动态连续批处理对比"><a class="markdownIt-Anchor" href="#静态动态连续批处理对比"></a> 静态/动态/连续批处理对比</h3>
<ul>
<li>
<p><strong>静态批处理</strong>：客户端将多个Prompt打包进一个请求中，并在批次中所有序列完成后返回响应。通常，多数推理服务支持这种方法，但并不要求这样做。</p>
</li>
<li>
<p><strong>动态批处理</strong>：多个请求的Prompt在服务端内部动态打包进一个批次处理。通常，这种方法的表现不如静态批处理，但如果响应短或长度一致，可以接近最优。当请求具有不同参数时，这种方法效果不佳。</p>
</li>
<li>
<p><strong>连续批处理</strong>：将请求在到达时一起批量处理，它不是等待批次中所有序列完成，而是在迭代推理层级将序列组合在一起。它可以实现比静态批处理高10倍到20倍的吞吐量，目前是最先进的方法。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153948015.png" alt="image.png" /></p>
<h2 id="orca-原理"><a class="markdownIt-Anchor" href="#orca-原理"></a> ORCA 原理</h2>
<p>目前，主流 LLM 推理框架 Continuous batching 的实现都是借鉴论文 Orca: A Distributed Serving System for Transformer-Based Generative Models 而来， Continuous batching 更是在 vLLM 的推动下已成为 LLM 推理框架标准功能。只是不同框架实现有差别，主要体现在对prefill处理的方式上。将prefill单独处理还是和decoding融合，以什么样的粒度融合，有一些讲究。 下面来看看 ORCA 的工作原理。</p>
<p>由于现有系统在处理请求时，通常是按批次处理，这导致一些请求在批次中提前完成但无法及时返回给客户端，增加了延迟。同时，新到达的请求需要等待当前批次完全处理完毕，增加了排队时间。因此，ORCA提出了以下解决方案：</p>
<ul>
<li>迭代级调度（Iteration-level Scheduling）：提出一种新的调度机制，调度执行时以迭代为单位，而不是整个请求。这样，每次迭代后，检测到完成的请求，并立即将生成的Token返回给客户端。对于新到达的请求，有机会在当前的迭代执行后进行处理，从而减少等待时间。通过迭代级调度，调度器可以完全控制每次迭代处理的请求数量和哪些请求，具体如下图所示。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153954755.png" alt="image.png" /></p>
<ul>
<li>选择性批处理（Selective Batching）：在应用批处理和迭代级调度时，只对选定的少数操作(与形状不规则的输入张量兼容的操作)应用批处理。即所有非注意力操作，包括线性、层归一化、Add和GeLU操作，它们不需要区分不同请求的张量元素，而注意力操作需要请求的概念（即需要批量维度）来仅在相同请求的Token之间计算注意力。选择性批处理了解每个操作的不同特性；它将批次分割并对每个请求单独处理注意力操作，同时将其他操作应用到没有请求概念的Token级（而不是请求级）批处理。这样可以在不同的操作中灵活地处理请求，避免因不同请求处理不同数量的Token 而导致的批处理问题，具体如下图所示，可以看到Attn前后有Split和Merge操作。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154000884.png" alt="image.png" /></p>
<p>此外，ORCA作为一个分布式服务系统，除了实现上述两种技术。它还采用了模型并行策略（如层内和层间模型并行化），以支持大规模模型。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154006156.png" alt="image.png" /></p>
<p>ORCA的系统架构包括请求池、调度器和执行引擎。调度器负责从请求池中选择请求，执行引擎则负责执行模型的迭代。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154012034.png" alt="image.png" /></p>
<h3 id="orca-与现有系统fastertransformer流水线并行对比"><a class="markdownIt-Anchor" href="#orca-与现有系统fastertransformer流水线并行对比"></a> ORCA 与现有系统（FasterTransformer）流水线并行对比</h3>
<p>ORCA的调度器使得引擎中的Worker在多个批次之间进行流水线式执行（调度器不会等待一个调度批次返回），直到n_scheduled（当前调度批次的数量）达到n_workers（工作线程的数量）。通过这样做，调度器保持引擎中同时运行的批次数量为n_workers，这意味着引擎中的每个Worker线程都在处理一个批次，而没有空闲。图8a展示了3个ORCA工作线程的执行流水线，使用的最大批次大小为2。起初，调度器根据到达时间选择请求A和B，并调度引擎处理一个包含请求A和B的批次（这里称之为批次AB），其中工作线程Worker1、Worker2和Worker3依次处理该批次。调度器仅在调度器注入另外两个批次CD和EF之后，才等待批次AB的返回。一旦批次AB返回，请求A和B由于是请求池中最早到达的请求，将再次被选中和调度。</p>
<p>相比之下，现有服务系统和执行引擎之间的接口（例如：Triton和FasterTransformer的组合）由于请求级调度的限制不允许在当前运行批次完成之前注入另一个批次。也就是说，Triton不能在当前批次AB完成之前将下一个请求C注入到FasterTransformer。为了在这种限制下启用多个层间分区的流水线执行，FasterTransformer将一个请求批次分割成多个微批次，并在微批次之间流水线执行分区。在图8b中，FasterTransformer将批次AB分割成两个微批次。由于每个分区以批量方式处理一个微批次（它比原始批次小），批处理带来的性能提升可能会变小。此外，当微批次大小太大时，可能会造成比较大的流水线Bubble。</p>
<p>因此，FasterTransformer需要在批处理效率（较大的微批次大小）和流水线效率（较少的流水线Bubble）之间进行权衡，但ORCA由于迭代级调度而无需进行这种权衡，可以轻松地在不将批次分割成微批次的情况下流水线处理请求。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154018405.png" alt="image.png" /></p>
<h2 id="continuous-batching-在-llm-推理框架中的应用"><a class="markdownIt-Anchor" href="#continuous-batching-在-llm-推理框架中的应用"></a> Continuous batching 在 LLM 推理框架中的应用</h2>
<p>前面谈到 Continuous batching 已经广泛应用于 LLM 推理框架，从下图也可以窥见一二，同时下图还展示了主流 LLM 推理的优化技术、具体实现和主要特性。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154023550.png" alt="image.png" /></p>
<p>因此，下面介绍下 Continuous batching 在各大推理框架中的应用。</p>
<h3 id="vllm-中的-continuous-batching"><a class="markdownIt-Anchor" href="#vllm-中的-continuous-batching"></a> vLLM 中的 Continuous Batching</h3>
<p>在vLLM中，通过 LLM 引擎类 <code>LLMEngine</code> 接收请求并生成文本。它接收来自客户端的请求并从 LLM 生成文本。它包括一个分词器、一个语言模型（可能分布在多个 GPU 上）以及为中间状态分配的 GPU 内存空间（或称为 KV Cache）。该类利用迭代级调度和高效的内存管理来最大化服务的吞吐量。</p>
<p>而每一次迭代级调度都是在 <code>step()</code> 方法中完成，即完成一次推理过程（整个prefill阶段算1个次推理，decode阶段每迭代生成一个Token算1次推理）。</p>
<p>整个step()方法的执行流程如下所示：</p>
<ul>
<li>
<p>步骤 1：安排下一次迭代中要执行的序列以及要换入（CPU-&gt;GPU）换出（GPU-&gt;CPU）复杂的Token块。</p>
<ul>
<li>
<p>根据调度策略，序列可能被抢占/重新排序。</p>
</li>
<li>
<p>序列组（SG）是指由同一提示生成的一组序列。</p>
</li>
</ul>
</li>
<li>
<p>步骤2：调用分布式执行器执行模型。</p>
</li>
<li>
<p>步骤3：处理模型输出。这主要包括：</p>
<ul>
<li>
<p>解码相关输出。</p>
</li>
<li>
<p>根据采样参数（是否使用beam_search）使用模型输出来更新已调度的序列组。</p>
</li>
<li>
<p>释放已完成的序列组。</p>
</li>
</ul>
</li>
<li>
<p>最后，它创建并返回新生成的结果。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154031077.png" alt="image.png" /></p>
<p>vLLM 中使用三个双端队列来维护所有用户的请求的调度：waiting，running，swapped。</p>
<ul>
<li>
<p>waiting：由所有等待 Prefill 的请求组成。</p>
</li>
<li>
<p>running：由所有即将或者进行推理的请求组成。</p>
</li>
<li>
<p>swapped：由所有暂时换出到 CPU 内存中的请求组成。</p>
</li>
</ul>
<p>那么在 vLLM 中，三个队列如何协同工作来决定在当前step中应该运行哪些请求集呢？</p>
<p>当有空闲的 KV Block 内存： 将waiting中的请求放入到running队列（waiting -&gt; running）</p>
<p>当没有新的 KV Block 内存时，因此需要释放 KV Block 内存，以进行新一次迭代Token的生成，根据抢占策略不同，做出不同的响应：</p>
<ul>
<li>
<p>交换(Swapping)模式: 将被抢占序列的块交换到CPU内存中，并在序列恢复时将它们交换回来（running -&gt; swapped）。</p>
</li>
<li>
<p>重计算(Recomputation)模式: 丢弃被抢占序列的块，并在恢复序列时重新计算它们，将序列视为新的提示（running -&gt; waiting）。</p>
</li>
</ul>
<p>和ORCA不同之处在于，vLLM Batching时候prefill和decoding是分开的，一个Batching step要么处理decoding要么处理prefill。这样实现比OCRA更简单了，prefill阶段直接调用xformers处理计算密集的attn计算；decode阶段使用手写CUDA PagedAttention处理访存密集的attn计算。 其中，PagedAttention提出将KV缓存存储在非连续的内存空间中，它将每个序列的 KV Cache 划分为多个块，每个块包含固定数量Token的Key和Value。这种方法有效地控制了注意力计算期间的内存浪费。</p>
<p>不过因为Prefill阶段会抢占Decode阶段的step优先处理，因此，decode阶段都需要等待。如果输入prompt sequence length过长，会造成更长的延迟。</p>
<h3 id="deepspeed-mii-中的-dynamic-splitfuse"><a class="markdownIt-Anchor" href="#deepspeed-mii-中的-dynamic-splitfuse"></a> DeepSpeed-MII 中的 Dynamic SplitFuse</h3>
<p><strong>背景：</strong></p>
<p>在当前系统中，有两种主要方法来实现连续批处理。在 TGI 和 vLLM 中，生成阶段被抢占以执行提示处理（在 TGI 中称为 infill），然后继续生成。在 Orca 中，prefill 和 decode 阶段不被区分；相反，只要总序列数没有达到固定限制，Orca 就会将提示加入正在运行的批次中。这两种方法都在不同程度上需要暂停生成以处理长提示。因此，DeepSpeed-MII中提出了一种Continous Batching变种Dynamic SplitFuse，它利用动态提示和生成分解, 统一来进一步改善连续批处理和系统吞吐量。</p>
<p><strong>工作原理：</strong></p>
<p>DeepSpeed-MII 中利用动态分割融合（Dynamic SplitFuse）策略，通过从提示中取出部分令牌并与生成过程相结合，使得模型可以保持一致的前向传递大小（forward size）。具体来说，动态分割融合执行两个关键行为：</p>
<ol>
<li>
<p>将长提示分解成更小的块，并在多个前向传递（迭代）中进行调度，只有在最后一个前向传递中才执行生成。</p>
</li>
<li>
<p>短提示将被组合以精确填充目标Token预算。即使是短提示也可能被分解，以确保预算被精确满足，前向大小（forward sizes）保持良好对齐。</p>
</li>
</ol>
<p>动态分割融合（Dynamic SplitFuse）提升了以下性能指标：</p>
<ol>
<li>
<p><strong>更好的响应：</strong> 由于长提示不再需要极长的前向传递来处理，模型将提供更低的客户端延迟。在同一时间窗口内执行的前向传递更多。</p>
</li>
<li>
<p><strong>更高的效率：</strong> 短提示的融合到更大的令牌预算使模型能够持续运行在高吞吐量状态。</p>
</li>
<li>
<p><strong>更低的波动和更好的一致性：</strong> 由于前向传递的大小一致，且前向传递大小是性能的主要决定因素，每个前向传递的延迟比其他系统更加一致。生成频率也是如此，因为DeepSpeed-FastGen不需要像其他先前的系统那样抢占或长时间运行提示，因此延迟会更低。</p>
</li>
</ol>
<p>因此，DeepSpeed-FastGen 将以尽可能快且持续生成的速度从传入的提示中消耗Token，同时向系统添加Token，从而提高系统利用率，与其他最先进的LLM服务系统相比，为所有客户端的流式生成，提供了更低的延迟和更高的吞吐量。</p>
<p>下图对比了Orca、vLLM与DeepSpeed-MII的连续批处理策略。每个块表示一个前向传递的执行。箭头表示前向传递有一个或多个生成的Token序列。vLLM 在一个前向传递中要么生成Token，要么处理提示；提示处理抢占令牌生成。Orca 在生成Token过程中以完整长度处理提示。DeepSpeed-FastGen的动态分割融合则执行固定大小批次的动态组合，包括生成Token和处理提示Token。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154039577.png" alt="image.png" /></p>
<h3 id="vllm-与-deepspeed-fastgen-性能基准"><a class="markdownIt-Anchor" href="#vllm-与-deepspeed-fastgen-性能基准"></a> vLLM 与 DeepSpeed-FastGen 性能基准</h3>
<p>在 NVIDIA A100-80GB GPU 上使用 LLaMA-7B 模型在以下场景中对这两个系统进行了基准测试。当工作负载始终是长提示和短输出时，DeepSpeed-FastGen 表现更佳。在其他场景中，vLLM 表现出优越的性能。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154050020.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154054705.png" alt="image.png" /></p>
<h3 id="lmdeploy-中的-persistent-batching"><a class="markdownIt-Anchor" href="#lmdeploy-中的-persistent-batching"></a> LMDeploy 中的 Persistent Batching</h3>
<p>LMDeploy 中的 Persistent Batching（类似continuous batching的功能）是在其TurboMind 推理引擎中实现。TurboMind 结构如下所示，主要包括：LLaMA 架构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  +--------------------+</span><br><span class="line">  |        API         |</span><br><span class="line">  +--------------------+</span><br><span class="line">          |    ^</span><br><span class="line">    请 求  |    | 流式回调</span><br><span class="line">          v    |</span><br><span class="line">  +--------------------+    获取   +-------------------+</span><br><span class="line">  |  Persistent Batch  | &lt;-------&gt; |  KV Cache 管理器 |</span><br><span class="line">  +--------------------+    更新   +-------------------+</span><br><span class="line">             ^</span><br><span class="line">             |</span><br><span class="line">             v</span><br><span class="line">+------------------------+</span><br><span class="line">|      LLaMa推理实现      |</span><br><span class="line">+------------------------+</span><br><span class="line">| FT kernels &amp; utilities |</span><br><span class="line">+------------------------+</span><br></pre></td></tr></table></figure>
<p>Persistent Batching 的实现需要一个请求队列（推理请求首先先加入到请求队列中），其次，需要Persistent线程从请求队列中获取请求进行处理。</p>
<p>Persistent线程执行逻辑：</p>
<ul>
<li>
<p>该功能会预先准备好 N 个 batch slots。</p>
</li>
<li>
<p>当有空闲 slots 时，请求就会加入到 batch 中。当请求对应的 tokens 都生成完毕后，对应的 batch slot 会立刻被释放，接收新的请求。</p>
</li>
<li>
<p><strong>当一个 sequence 命中缓存时，它的历史 token 不必在每轮中都进行解码，所以它的 token 生成过程会即刻开始</strong>。</p>
</li>
<li>
<p>整个 batch 会自动扩缩容来避免不必要的计算。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154124299.png" alt="image.png" /></p>
<p>实现 Persistent Batching 时，通常会与 KV Cache 内存管理优化相结合进一步提升性能。在 TurboMind 中，通过 <a href="https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy/blob/main/src/turbomind/models/llama/SequenceManager.h">KV Cache 管理器</a> 实现，这是一个内存池类型的对象，并且在其中加入了 LRU 的实现，这样整个管理器可以被看作是一个 <strong>KV Cache的缓存</strong>。大致工作方式如下：</p>
<ul>
<li>
<p>KV Cache 由管理器分配。管理器会根据预先配置好的 slot 数量开辟空间。每个 slot 对应于一个 sequence 所需的 KV 缓存。分配的内存块大小可通过配置来实现预分配或者按需分配（或介于两者之间）。</p>
</li>
<li>
<p>当有新的请求，但是缓存池中没有空闲 slot时，根据 LRU 机制，管理器会踢除最近使用最少的 sequence，把它占据的 slot 分给新的请求。</p>
</li>
<li>
<p>如果sequence获取到了slot，类似缓存命中。它在缓存中的历史KV会被直接返回，而不用再进行context decoding 。</p>
</li>
<li>
<p>被踢除的 sequences 不会被完全的删除，而是会被转换成最简洁的形式，例如 token IDs 。当之后获取到相同的 sequence id 时 (即 <em>cache-miss</em> 状态)，这些 token IDs 将被 FMHA 的 context decoder 解码并被转回 KV 缓存。</p>
</li>
<li>
<p>踢除和转换均由 TurboMind 内部自动管理，所以对用户来说是透明的。<strong>从用户的使用角度来看，使用了 TurboMind 的系统就像是可以访问无限的设备内存</strong>。</p>
</li>
</ul>
<p>此外，MDeploy 也引入了 dynamic split &amp; fuse。</p>
<h3 id="tensorrt-llm-中的-in-flight-batching"><a class="markdownIt-Anchor" href="#tensorrt-llm-中的-in-flight-batching"></a> TensorRT-LLM 中的 in-flight batching</h3>
<p>TensorRT-LLM 依赖于一个称为批处理管理器的组件来支持请求的in-flight batching（在社区中，也称为continuous batching或Iteration batching）。该技术旨在减少队列中的等待时间，消除 padding 请求的需求并带来更高的 GPU 利用率。</p>
<p>更详细地说，此功能允许在循环生成Token的每次迭代中<strong>包含新到达的请求并返回新完成的请求</strong>。运行中批处理通过称为批处理管理器的 TensorRT-LLM 组件进行访问。该批处理管理器公开了钩子，供用户注册函数指针来定义 TensorRT-LLM 如何读取新请求以及如何将已完成的请求返回给用户。</p>
<h3 id="lightllm-中的-continues-batching"><a class="markdownIt-Anchor" href="#lightllm-中的-continues-batching"></a> LightLLM 中的 Continues Batching</h3>
<p>LightLLM 是一款纯 Python 的 LLM 推理框架，里面也实现了Continues Batching，相关代码逻辑在<a href="https://link.zhihu.com/?target=https%3A//github.com/ModelTC/lightllm/blob/main/lightllm/server/router/manager.py%23L160">RouterManager.loop_for_fwd()</a>中；Prefill 和 Decode 会在同一个 step 进行处理。同时，其针对PagedAttention进行了改进，提出了TokenAttention（一种在Token级别管理键和值缓存的注意力机制）来最大限度地减少内存碎片并实现高效的内存共享，而且还可以促进高效的内存分配和释放。它提供了一种更精确和细粒度的内存管理，从而优化内存利用率。另外，该框架也引入了 SplitFuse 特性。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154132494.png" alt="image.png" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>本文先介绍了不同批处理基本概念区别，然后，讲述了 LLM 推理框架中提升吞吐性能最重要的技术 Continuous batching。同时，分析了其在不同框架中的应用实现。</p>
<p>参考文档：</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/688736901">LLM 推理加速方式汇总</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV15b421a7Rm/%3Fspm_id_from%3D333.999.0.0%26vd_source%3Db12bd535d71dd38f94ddfaed6c3c441b">连续批处理</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/">LLM Inference: Continuous Batching and PagedAttention</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3Mzg5ODgxMg%3D%3D%26mid%3D2247489116%26idx%3D1%26sn%3D0f2771e71f2c3e020ab8e86684df0093%26chksm%3Dfd3bff0fca4c7619db3f3dc800df858150a12c7f192234625b871a4f15cfc699c5746846c5b8%26token%3D1935281044%26lang%3Dzh_CN%23rd">借着triton inference server聊一下各种batching方法</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide/Part_2-improving_resource_utilization">Dynamic Batching &amp; Concurrent Model Execution(triton)</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/ragged_batching.html">Ragged Batching(triton)</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/osdi22-yu.pdf">Orca: A Distributed Serving System for Transformer-Based Generative Models</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT-LLM/blob/v0.11.0/docs/source/advanced/batch-manager.md%23the-batch-manager-in-tensorrt-llm">The Batch Manager in TensorRT-LLM</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy/blob/v0.5.1/docs/zh_cn/inference/turbomind.md">TurboMind 框架(TurboMind)</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy/blob/v0.5.1/src/turbomind/models/llama/LlamaBatch.cc">LlamaBatch.cc(TurboMind)</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1iW4y1A77P/%3Fvd_source%3Db12bd535d71dd38f94ddfaed6c3c441b">LMDeploy 大模型量化部署实践</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/vllm-project/vllm/blob/v0.5.2/vllm/engine/llm_engine.py%23L817">llm_engine.py(vLLM)</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//docs.vllm.ai/en/latest/dev/engine/llm_engine.html">LLMEngine(vLLM)</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//docs.vllm.ai/en/latest/community/meetups.html">vLLM Meetups</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/688551989">从continuous batching到vLLM中的batching</a>*</p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/691045737">图解大模型计算加速系列：vLLM源码解析1，整体架构</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/676109470">大模型推理核心技术之Continuous Batching和我的WXG往事</a>**</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//blog.vllm.ai/2023/11/14/notes-vllm-vs-deepspeed.html">Notes on vLLM v.s. DeepSpeed-FastGen</a>*</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/chinese">DeepSpeed-FastGen：通过 MII 和 DeepSpeed-Inference 实现 LLM 高吞吐量文本生成</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/ModelTC/lightllm/tree/main/lightllm/server/router/req_queue/continues_batch">continues_batch(LightLLM)</a></p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/ModelTC/lightllm/blob/main/lightllm/server/router/model_infer/mode_backend/splitfuse/impl.py">SplitFuseBackend(LightLLM)</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>KV Cache</title>
    <url>/2025/01/26/KV-Cache/</url>
    <content><![CDATA[<h3 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> Self-Attention</h3>
<p><strong>自注意力机制允许模型将词与其他词关联起来。</strong></p>
<p>假设我们考虑一个序列长度为6, embedding维度为512的输入, 由于是自注意力, QKV都是相同的</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">softmax(\frac{QK^T}{\sqrt{d_k}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.627473em;vertical-align:-0.538em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.089473em;"><span style="top:-2.5864385em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622307142857143em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222307142857144em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17776928571428574em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>捕捉了两个token之间的相关性</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165928491.png" alt="image.png" /></p>
<p>上一步的结果再和V做矩阵乘, 得到与输入维度相同的输出矩阵, <strong>每行(即经过计算后的的新的embedding)不仅捕捉了词语的含义（由嵌入表示）或在句子中的位置（由位置编码表示），还捕捉了每个词与其他词的相互作用</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165937525.png" alt="image.png" /></p>
<p>给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165943584.png" alt="image.png" /></p>
<span id="more"></span>
<p><strong>Multi-Head Attention</strong></p>
<p>沿着d模型维度将QKV分割成较小的矩阵，并在这些较小的矩阵之间计算注意力. 所以每个头都在观察完整的句子，但是是每个embedding的不同方面。这样做的原因是<strong>我们希望每个头观察同一个词的不同方面</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165951462.png" alt="image.png" /></p>
<p>再给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170001367.png" alt="image.png" /></p>
<h3 id="kv-cache"><a class="markdownIt-Anchor" href="#kv-cache"></a> KV cache</h3>
<h4 id="next-token-prediction-task"><a class="markdownIt-Anchor" href="#next-token-prediction-task"></a> Next Token Prediction Task</h4>
<p>首先来了解一下LLama的训练（下词预测任务）：seq2seq的生成，但迭代T次，seq_len逐渐增加。</p>
<p>即输入序列的第一个token将映射到输出序列的第一个token, [SOS] -&gt; Love, 第二次迭代中将用[SOS] Love预测that, 以此类推</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170012232.png" alt="image.png" /></p>
<h4 id="motivation-behind-kv-cache"><a class="markdownIt-Anchor" href="#motivation-behind-kv-cache"></a> Motivation behind KV cache</h4>
<p>在推理的每一步，我们只关心模型输出的最后一个词元，因为我们已经有了之前的词元。然而，模型需要访问所有之前的词元来决定输出哪个词元，因为它们构成了模型的上下文（或称为“提示”）。<br />
有没有办法让模型在推理时对已经看到的词元进行更少的计算呢？</p>
<h4 id="kv-cache-2"><a class="markdownIt-Anchor" href="#kv-cache-2"></a> KV cache</h4>
<p>下句预测时的Self-Attention：</p>
<ul>
<li>timpstep=1时<code>seq_len=1</code>，给[SOS]时，预测Love；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170018823.png" alt="image.png" /></p>
<ul>
<li>timpstep=2时<code>seq_len=2</code>，给[SOS] 和 Love时，预测that</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170024662.png" alt="image.png" /></p>
<ul>
<li>timpstep=4时<code>seq_len=4</code>，给[SOS] 和 Love 和 can 和 quickly时，预测seize…</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170032874.png" alt="image.png" /></p>
<p>再来分析一下，<strong>每次个timestep的self-attention中我们到底需要哪些</strong>：因为我们只关注<strong>最后一个token的attention_output</strong>，如下图timestep=4，我们只需要attention_output的第4个token。</p>
<p>因此我们只需要<strong>Q的最后一个token</strong>和<strong>K的所有token</strong>相乘，得到最后一个token的<code>attention_score</code>，然后用<strong>V的所有token</strong>再与<code>attention_score</code>点积(相乘求和)，得到最后一个token的<code>attention_output</code>：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170040017.png" alt="image.png" /></p>
<p>由上分析可知，<strong>每个timestep，我们的Q只需要新增的那个token即可，而K和V要缓存之前timestep的token，保证token是全的</strong>。<strong>每次计算出来的attention_output就是那个新增的token的attention。</strong> 这样就可以节省大量计算开销。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170046580.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170053043.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170058416.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170105884.png" alt="image.png" /></p>
<p><strong>参数量的计算方法:</strong> <a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></p>
<h2 id="kv-cache占用的显存空间"><a class="markdownIt-Anchor" href="#kv-cache占用的显存空间"></a> KV Cache占用的显存空间</h2>
<p>Q, K的形状为:[b,head_num,s,per_head_hidden_size], 那么对于每个decoder layer，每个 token 的 K、V 矩阵都是 embedding_size=num_heads * head_size，再乘上 seqlen和 batch size，那就是每个layer的 kv Cache 所需的存储容量了。</p>
<p><strong>例如，在 LLaMA 2-13B 中，head_num = 40, layer_num = 40, dimension = 5120, 对于1个token的 KV Cache 一共需要 2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16) = 800KB</strong>。</p>
<p><strong>以模型支持的最大的上下文长度4096为例，对于4096个token, 需要800KB * 4096 = 3.2GB的KV cache, 每增加一个batch则需要3.2GB显存，对于一张A100 40GB来说，在加载完模型26GB之后，余下的显存也仅能支持4个batch</strong></p>
<ul>
<li><strong>Activation</strong>：Activation在整个Self-Attention的计算过程中，最大的Activation是qkv的MatMul计算，Q,K,V的形状为：[b,head_num,s,per_head_hidden_size], 占用显存大小为2 * batch_size * seq_len * embedding_size, K^T的形状为：[b,head_num,per_head_hidden_size,s], QK^T的形状为：[b,head_num,s,s]，占用显存大小为2 * batch_size * head_num * seq_len^2, <strong>以模型支持的最大的上下文长度4096为例，对于4096个token, 每一层需要保存的峰值激活为大小为QK^T和V的输入, 大小为Q的输出, 4 * batch_size * seq_len * embedding_size + 2 * batch_size * head_num * seq_len^2 = 4 * 4096 * 5120 + 2 * 40 * 4096^2 = 1.3GB</strong></li>
</ul>
<h2 id="kv-cache能省下的flops"><a class="markdownIt-Anchor" href="#kv-cache能省下的flops"></a> KV Cache能省下的FLOPs</h2>
<p><strong>每个token的 K、V 矩阵计算一共需要 2 (K+V) * 2 (mul+add) * embedding size * embedding size = 4 * 5120 * 5120 这么多计算量，乘以seqlen、num_layer和 batch size，一共省了 4096 * 40 * 4 * 5120 * 5120 = 17 TFLOPs的计算量</strong>，当然，因seqlen和embedding size和num layer而异。</p>
<h2 id="使用kv-cache后推理逻辑的变化"><a class="markdownIt-Anchor" href="#使用kv-cache后推理逻辑的变化"></a> 使用KV Cache后推理逻辑的变化</h2>
<p>当不使用 KV cache 时，每次生成下一个 token 的注意力计算都会对<strong>整条</strong>已生成序列（seq_len）进行自注意力计算。</p>
<p>而使用 KV cache 后，在生成新 token 时不需要重新计算所有先前 token 的 Key/Value 表示，<strong>只需对当前新生成的 token 计算 Query，并与已缓存的 Key/Value（即 KV cache）进行一次点积操作</strong>，得到注意力分布，从而减少计算量。此时，<strong>注意力的输出序列长度从整个历史序列长度缩减为 1</strong>（仅对当前 token 的预测所需）。</p>
<p>因此, 每层 Decoder 都会接收到一个长度为 <code>seq_len</code> 的张量, 尽管输入的长度为 1，但注意力层并非只对这 1 个 token 做内部计算后结束。因为我们已经缓存了所有之前步骤（token）的 Key 和 Value，因此在注意力计算时，当前生成的 token 的 Query（Q）会与已经缓存的（seq_len-1）个 Key/Value 一起作用，从而模拟完整上下文的自注意力过程。换句话说：</p>
<ul>
<li>
<p>Query 是当前新 token 对应的 Q（长度为1的Q）</p>
</li>
<li>
<p>Key/Value 则来自于之前缓存的所有历史 token（长度为过去累积的序列长度）</p>
</li>
</ul>
<p>因此，尽管这一轮输入激活只有一个 token，但注意力计算仍然是对所有历史信息进行查询（Q对缓存的K/V），确保模型没有丢失上下文。</p>
<h1 id="拓展资料"><a class="markdownIt-Anchor" href="#拓展资料"></a> <strong>拓展资料</strong></h1>
<p><a href="https://juejin.cn/post/7362789570217885759">https://juejin.cn/post/7362789570217885759</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM中的搜索、采样算法：greedy、beam_search、top_k、top_p、temperature</title>
    <url>/2025/01/26/LLM%E4%B8%AD%E7%9A%84%E6%90%9C%E7%B4%A2%E3%80%81%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95%EF%BC%9Agreedy%E3%80%81beam-search%E3%80%81top-k%E3%80%81top-p%E3%80%81temperature/</url>
    <content><![CDATA[<blockquote>
<p>随着chatGPT，chatGLM、Llama等LLM模型的火爆，生成式模型逐渐被大家所熟知和认可。</p>
<p>LLM看似很神奇，但究其本质还是一个概率相关的问题：神经网络根据输入的文本，执行多轮decoder。 每次decoder时候从预训练的模型里面生成一堆候选词（及其概率/分数），按照指定的解码算法或者说后处理算法，选择出候选词，知道满足结束🔚条件为止。</p>
<p>同样的LLM模型，如果采用的后处理算法、算法的参数设置不同，生成的结果很可能也会不同。</p>
<p>目前LLM模型中常见的后处理算法有：greedy、beam_search、topp、topk等。</p>
</blockquote>
<span id="more"></span>
<h3 id="1-搜索采样算法"><a class="markdownIt-Anchor" href="#1-搜索采样算法"></a> 1 搜索/采样算法</h3>
<h4 id="11-greedy"><a class="markdownIt-Anchor" href="#11-greedy"></a> 1.1、greedy</h4>
<p>Greedy search 是指在每一次选择下一个词时，根据此次decoder生成的概率表logits（shape == [batch_size, vocab_size] )，选择概率最高的那一个词（token）。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126164404188.png" alt="image.png" /></p>
<p>以上图为例：</p>
<p>从单词“The”开始，每次算法在选择下一个词时，都概率最高的那一个词，进而，最终的生成词序列为（“The”,“nice”,“woman”）。</p>
<p>这就是 “贪心策略”，永远选择分数或概率最大的token。Greedy Search存在的一个最大的问题在于，只考虑了当前的高概率词，忽略了在当前低概率词后面的高概率词。 如图1所示：词“has”在词“dog”后面，条件概率高达0.9，但词“dog”的条件概率只排第二，所以greedy search错过了词序列“The”、“dog”、“has”。</p>
<h4 id="12-beam-search"><a class="markdownIt-Anchor" href="#12-beam-search"></a> 1.2、beam search</h4>
<p>为了避免错过隐藏的高概率词，Beam Search通过参数num_beams的配置，可以在每个时刻，记录概率最高的_<strong>前num_beams</strong>_（beam_size）个路径，在下一个时刻可以有多个基础路径同时搜索。</p>
<p>以num_beams=2为例，过程图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126164417747.png" alt="image.png" /></p>
<p>可以看到：</p>
<p>在step=1时，概率是前num_beams的路径是（“The”、“nice”）、（“The”、“dog”）。</p>
<p>在step=2时，概率是前num_beams的路径是（“The”、“dog”、“has”），<em>p = 0.36</em>；路径（“The”、“nice”、“women”）， <em>p = 0.2 。</em></p>
<p>因此，两条路径中，找到了概率最高的路径，得到了更为合理的答案。</p>
<p>beam search生成的词序列比 greedy search生成的词序列的综合概率更高，但是也不能保证是整体概率最高的词序列：因为beam_search并没有遍历所有可能的词序列的组合。</p>
<ul>
<li>
<p>当num_beams = 1时， beam_search等同于 greedy search;</p>
</li>
<li>
<p>当num_beams = vocab_size时， beam_search会遍历所有可能的词序列组合，但计算量会呈指数级上升。</p>
</li>
</ul>
<p>beam search在做像是语音识别、翻译和摘要这类可以大致预测生成长度的场景中表现还可以；</p>
<p>但是在像是对话和故事生成这类开放生成领域效果就差得多了。</p>
<p>其次，我们已经看到beam search比较容易生成重复内容； 最后，高水平的人类语言不会按照下一个词条件概率最高的方式排列，在应用于生成式模型的时候，我们同样希望生成的内容具有多样性。 所以就需要在此基础上优化采样方式。</p>
<hr />
<h4 id="13-top_k"><a class="markdownIt-Anchor" href="#13-top_k"></a> 1.3、top_k</h4>
<p>Top-k采样是对前面“贪心策略”的优化，它从概率排名前K的token之中进行抽样，使得除了分数最高的token之外，其他分数或概率较高的token也有机会被选中，以达到有一定机率不选最大概率的词，其核心思想在于：从前K个概率最大的词中按它们的概率进行采样。</p>
<p>如下图示例中，假如K=3， 我们首先筛选概率值前3的token，然后使用 <em><strong>softmax</strong></em> 或者最大似然估计等方法重新计算采样概率，通过调整k的大小，即可控制采样列表的大小。 <em>当K=1时， topK等同于greedy search。</em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126165035897.png" alt="image.png" /></p>
<p>🤗 HuggingFace中的topK采样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">r&#x27;&#x27;&#x27; 🔗： https://github.com/huggingface/transformers/blob/d7bd325b5a44054341acc536339adab9ef8e8bb2/src/transformers/generation/logits_process.py#L415C38-L415C38 &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TopKLogitsWarper</span>(<span class="title class_ inherited__">LogitsWarper</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        top_k (`int`):</span></span><br><span class="line"><span class="string">            The number of highest probability vocabulary tokens to keep for top-k-filtering.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, top_k: <span class="built_in">int</span>, filter_value: <span class="built_in">float</span> = -<span class="built_in">float</span>(<span class="params"><span class="string">&quot;Inf&quot;</span></span>), min_tokens_to_keep: <span class="built_in">int</span> = <span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(top_k, <span class="built_in">int</span>) <span class="keyword">or</span> top_k &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`top_k` has to be a strictly positive integer, but is <span class="subst">&#123;top_k&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.top_k = <span class="built_in">max</span>(top_k, min_tokens_to_keep)</span><br><span class="line">        <span class="variable language_">self</span>.filter_value = filter_value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings(<span class="params">LOGITS_PROCESSOR_INPUTS_DOCSTRING</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:</span><br><span class="line">        top_k = <span class="built_in">min</span>(<span class="variable language_">self</span>.top_k, scores.size(-<span class="number">1</span>))  <span class="comment"># Safety check</span></span><br><span class="line">        <span class="comment"># Remove all tokens with a probability less than the last token of the top-k</span></span><br><span class="line">        indices_to_remove = scores &lt; torch.topk(scores, top_k)[<span class="number">0</span>][..., -<span class="number">1</span>, <span class="literal">None</span>]</span><br><span class="line">        scores = scores.masked_fill(indices_to_remove, <span class="variable language_">self</span>.filter_value)</span><br><span class="line">        <span class="comment"># scores.softmax(1)</span></span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<p><strong>topK特点</strong>：设置越大，生成的内容的随机性越大；</p>
<p>设置越小，生成的内容越固定； 设置为1时，和 <code>greedy</code> 效果一样。</p>
<p>但Top-k采样有个很大的问题，Top-K sampling不会根据下一个词的概率分布 ，动态调整候选池的大小。</p>
<p>比如某些情况下，模型的生成结果中出现1个概率非常大的token，比如说概率最高的 token 的概率是 <em>0.95</em>，而相比之下剩下的 token 概率都很低了。</p>
<p>topK由于需要采样K个词，这种情况下单纯使用topK会出现 <strong>采样到过低概率token的</strong>情况。</p>
<p>因此我们需要<strong>对顶部 token 的累计概率进行限制</strong>，这就是下面的<strong>TopP 采样</strong>。</p>
<h4 id="14-top_p"><a class="markdownIt-Anchor" href="#14-top_p"></a> 1.4、top_p</h4>
<p>为了避免单纯使用topK可能出现的采样到概率过低的token，提出了topp。 topp的主要做法是设置一个_<strong>概率界限值p</strong>_，然后从概率大的词中由大到小依次取K个，若还未取够K个词，所有取到的词的概率和就大于等于界限值p，则提前停止取词，这样取出的K个词中就不会出现概率特别低的词。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126165013919.png" alt="image.png" /></p>
<p>假如将 top-p P值设定为 0.75，即选择前 75% 概率的 tokens 作为候选。如图4所示：“老虎” 和 “苹果” 的概率加起来为 80% &gt; 75%，所以候选词就是这俩。</p>
<p>然后把这两个词再进行一次softmax之后，概率更新为 59.8%和40.2%，之后再使用如 torch.multinomial 等随机采样函数依据更新后的概率在 “老虎” 和 &quot;苹果&quot;进行选取。</p>
<p>🤗 HuggingFace中 TopP 的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TopPLogitsWarper</span>(<span class="title class_ inherited__">LogitsWarper</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off &lt;= prob_cut_off.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        top_p (`float`):</span></span><br><span class="line"><span class="string">            If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or</span></span><br><span class="line"><span class="string">            higher are kept for generation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, top_p: <span class="built_in">float</span>, filter_value: <span class="built_in">float</span> = -<span class="built_in">float</span>(<span class="params"><span class="string">&quot;Inf&quot;</span></span>), min_tokens_to_keep: <span class="built_in">int</span> = <span class="number">1</span></span>):</span><br><span class="line">        top_p = <span class="built_in">float</span>(top_p)</span><br><span class="line">        <span class="keyword">if</span> top_p &lt; <span class="number">0</span> <span class="keyword">or</span> top_p &gt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`top_p` has to be a float &gt; 0 and &lt; 1, but is <span class="subst">&#123;top_p&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(min_tokens_to_keep, <span class="built_in">int</span>) <span class="keyword">or</span> (min_tokens_to_keep &lt; <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`min_tokens_to_keep` has to be a positive integer, but is <span class="subst">&#123;min_tokens_to_keep&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.top_p = top_p</span><br><span class="line">        <span class="variable language_">self</span>.filter_value = filter_value</span><br><span class="line">        <span class="variable language_">self</span>.min_tokens_to_keep = min_tokens_to_keep</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings(<span class="params">LOGITS_PROCESSOR_INPUTS_DOCSTRING</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:</span><br><span class="line">        sorted_logits, sorted_indices = torch.sort(scores, descending=<span class="literal">False</span>)</span><br><span class="line">        cumulative_probs = sorted_logits.softmax(dim=-<span class="number">1</span>).cumsum(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Remove tokens with cumulative top_p above the threshold (token with 0 are kept)</span></span><br><span class="line">        sorted_indices_to_remove = cumulative_probs &lt;= (<span class="number">1</span> - <span class="variable language_">self</span>.top_p)</span><br><span class="line">        <span class="comment"># Keep at least min_tokens_to_keep</span></span><br><span class="line">        sorted_indices_to_remove[..., -<span class="variable language_">self</span>.min_tokens_to_keep :] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># scatter sorted tensors to original indexing</span></span><br><span class="line">        indices_to_remove = sorted_indices_to_remove.scatter(<span class="number">1</span>, sorted_indices, sorted_indices_to_remove)</span><br><span class="line">        scores = scores.masked_fill(indices_to_remove, <span class="variable language_">self</span>.filter_value)</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<p>TopP特点：</p>
<ul>
<li>
<p>采用概率阈值，相比于纯topK,可以避免采样到概率过低的tokens</p>
</li>
<li>
<p>P值越大，采样结果的多样性越高，但可能会采样到概率过低的token</p>
</li>
<li>
<p>P值越小，采样结果越单一。</p>
</li>
</ul>
<p>很多模型会把topK和topK结合使用。<em><strong>TopK-Top</strong></em> 采样法就是将TopK采样法和TopP采样法相结合。</p>
<h4 id="15-temperature"><a class="markdownIt-Anchor" href="#15-temperature"></a> 1.5、 Temperature</h4>
<p>上面所说的topk, top_p都是在动态选择不同词的范围，但并没有更改实际词出现的概率。那么，是否可以在出现上概率上也进行调节呢？例如，将高频和低频之间的_<strong>概率拉大或者减少</strong>_，也能够对多样性提供一些思路。</p>
<p>因此，Temperatue温度采样方式被提出：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathsize="2.074em"><msub><mi>P</mi><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow><mi>i</mi><mi mathvariant="normal">/</mi><mi>T</mi></mrow></msup><mrow><mstyle scriptlevel="0" displaystyle="false"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></msubsup></mstyle><msup><mi>e</mi><mrow><mi>j</mi><mi mathvariant="normal">/</mi><mi>T</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">{\huge P_{i} = \frac{e^{i/T} }{ {\textstyle \sum_{j=0}^{n}} {e^{j/T} }} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.335996739999999em;vertical-align:-2.0961207399999995em;"></span><span class="mord"><span class="mord sizing reset-size6 size10"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30791166827386696em;"><span style="top:-3.29em;margin-left:-0.13889em;margin-right:0.02410800385728062em;"><span class="pstrut" style="height:3.44em;"></span><span class="sizing reset-size10 size8 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel sizing reset-size6 size10">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord sizing reset-size6 size10"><span class="mopen nulldelimiter sizing reset-size10 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0799787849566056em;"><span style="top:-3.4830615236258438em;"><span class="pstrut" style="height:4.074em;"></span><span class="sizing reset-size10 size8 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mop sizing reset-size8 size10"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7809384763741563em;"><span style="top:-3.1552744744455157em;margin-left:0em;margin-right:0.02410800385728062em;"><span class="pstrut" style="height:3.44em;"></span><span class="sizing reset-size10 size8 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.9219961427193826em;margin-right:0.02410800385728062em;"><span class="pstrut" style="height:3.44em;"></span><span class="sizing reset-size10 size8 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4197272613307618em;"><span></span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9410833333333334em;"><span style="top:-3.5160833333333334em;margin-right:0.034722222222222224em;"><span class="pstrut" style="height:3.2em;"></span><span class="sizing reset-size8 size7 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mtight">/</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-4.304em;"><span class="pstrut" style="height:4.074em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.468em;"><span class="pstrut" style="height:4.074em;"></span><span class="sizing reset-size10 size8 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.988em;"><span style="top:-3.563em;margin-right:0.034722222222222224em;"><span class="pstrut" style="height:3.2em;"></span><span class="sizing reset-size8 size7 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.010665737704918em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size10 size6"></span></span></span></span></span></span></p>
<p>Temperature 采样中的温度与玻尔兹曼分布有关，由上面的公式可知，temperature采样本质上就是在 _<strong>Softmax 函数上添加了温度（T）</strong>_这个参数。</p>
<p>通过将logits除以温度来实现温度采样，然后将其输入Softmax并获得采样概率。</p>
<p>越低的温度(&lt; 1.0)使模型会对高概率的选择更为偏向，而高于1.0的温度，则会缩小高概率词和低概率词之间的差距。<br />
<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126165204926.png" alt="image.png" /></p>
<p>Temperature 采样最终的宏观效果是，<strong>在较低的温度下，我们的模型更具确定性，而在较高的温度下，则不那么确定</strong>。</p>
<p>目前在huggingface transformers中，temperature、topK、TopP的处理默认是串联的，先进行temperature，然后topk，最后topp。 当然，如果用户自己定义了分数过滤器，用户的过滤器优先。</p>
<h3 id="2-惩罚机制"><a class="markdownIt-Anchor" href="#2-惩罚机制"></a> 2、惩罚机制</h3>
<h4 id="21-n-grams"><a class="markdownIt-Anchor" href="#21-n-grams"></a> 2.1 n-grams</h4>
<p>生成成模型在实际的运行过程中，总会出现一些重复的例子，这是高频词选择的结果。为了解决重复问题，一个简单的方法是用n-grams惩罚，来自论文Paulus et al. (2017)和Klein et al. (2017)。</p>
<p>其基本思想在于：通常的n-grams惩罚通过配置下一个词重复出现n-gram的概率为0，来保证没有n-gram出现两次。</p>
<h4 id="22-repetition-penalty"><a class="markdownIt-Anchor" href="#22-repetition-penalty"></a> 2.2 repetition penalty</h4>
<p>还可以通过<strong>惩罚因子将出现过词的概率变小或者强制不使用重复词来解决。</strong></p>
<p>其核心思想在于：**对于之前出现过的词语，在后续预测的过程中，通过引入惩罚因子降低其出现的概率。**惩罚因子penalty == 1.0时，表示没有惩罚。</p>
<p>惩罚因子来自于同样广为流传的【 CTRL: A Conditional Transformer Language Model for Controllable Generation 】:📰：<a href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a> 。</p>
<p>🤗HuggingFace中的实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># from  transformers/src/transformers/generation/logits_process.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RepetitionPenaltyLogitsProcessor</span>(<span class="title class_ inherited__">LogitsProcessor</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [`LogitsProcessor`] that prevents the repetition of previous tokens through an exponential penalty. This technique</span></span><br><span class="line"><span class="string">    shares some similarities with coverage mechanisms and other aimed at reducing repetition. During the text</span></span><br><span class="line"><span class="string">    generation process, the probability distribution for the next token is determined using a formula that incorporates</span></span><br><span class="line"><span class="string">    token scores based on their occurrence in the generated sequence. Tokens with higher scores are more likely to be</span></span><br><span class="line"><span class="string">    selected. The formula can be seen in the original [paper](https://arxiv.org/pdf/1909.05858.pdf). According to the</span></span><br><span class="line"><span class="string">    paper a penalty of around 1.2 yields a good balance between truthful generation and lack of repetition.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, penalty: <span class="built_in">float</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(penalty, <span class="built_in">float</span>) <span class="keyword">or</span> <span class="keyword">not</span> (penalty &gt; <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;`penalty` has to be a strictly positive float, but is <span class="subst">&#123;penalty&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.penalty = penalty</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings(<span class="params">LOGITS_PROCESSOR_INPUTS_DOCSTRING</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, input_ids: torch.LongTensor, scores: torch.FloatTensor</span>) -&gt; torch.FloatTensor:</span><br><span class="line">        score = torch.gather(scores, <span class="number">1</span>, input_ids)</span><br><span class="line">        <span class="comment"># if score &lt; 0 then repetition penalty has to be multiplied to reduce the previous token probability</span></span><br><span class="line">        score = torch.where(score &lt; <span class="number">0</span>, score * <span class="variable language_">self</span>.penalty, score / <span class="variable language_">self</span>.penalty)</span><br><span class="line"></span><br><span class="line">        scores.scatter_(<span class="number">1</span>, input_ids, score)</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<h3 id="3-huggingface中的生成策略"><a class="markdownIt-Anchor" href="#3-huggingface中的生成策略"></a> 3、huggingFace中的生成策略</h3>
<p>huggingFace为了保证多样性生成，有很多其他参数控制策略，包括长度、badwords等，下面总结了其中的22个参数及其释义。</p>
<p>🗂: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py">https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py</a></p>
<p><strong>1、temperature (float, optional, defaults to 1.0)： 用于调节下一个标记概率的值。</strong></p>
<p><strong>2、top_k (int, optional, defaults to 50)： 用于top-k过滤的最高概率词汇标记的数量。</strong></p>
<p><strong>3、top_p (float, optional, defaults to 1.0) ： 如果设置为float &lt; 1，则只保留概率加起来达到top_p或更高的最小的最有可能的词汇集合进行生成。</strong></p>
<p>4、typical_p (float, optional, defaults to 1.0)： 局部典型性衡量在已经生成的部分文本的情况下，预测下一个目标标记的条件概率与预测下一个随机标记的预期条件概率的相似程度。如果设置为float &lt; 1，则保留最小的、概率相加为typical_p或更高的局部典型标记的集合，用于生成。</p>
<p>5、epsilon_cutoff (float, optional, defaults to 0.0)：如果设置为严格介于0和1之间的float，只有条件概率大于epsilon_cutoff的标记会被采样。在论文中，建议的值在3e-4到9e-4之间，取决于模型的大小。</p>
<p>6、eta_cutoff (float, optional, defaults to 0.0)： Eta采样是局部典型采样和ε采样的混合体。如果设置为严格介于0和1之间的浮点数，只有当一个标记大于eta_cutoff或sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits)))时，才会考虑它。后者是预期的下一个令牌概率，以 sqrt（eta_cutoff）为尺度。在论文中，建议值从3e-4到2e-3不等，取决于模型的大小。更多细节见截断抽样作为语言模型去平滑。</p>
<p><strong>7、diversity_penalty (float, optional, defaults to 0.0)： 如果一个beam路径得分在某一特定时间产生了与其他组的任何beam路径相同的标记，这个值将从beam的分数中减去。请注意，多样性惩罚只有在分组beam搜索被启用时才有效。</strong></p>
<p><strong>8、repetition_penalty (float, optional, defaults to 1.0) ：重复性惩罚的参数。1.0意味着没有惩罚。</strong></p>
<p>9、encoder_repetition_penalty (float, optional, defaults to 1.0)：对不在原始输入中的序列进行指数式惩罚。1.0意味着没有惩罚。</p>
<p><strong>10、length_penalty (float, optional, defaults to 1.0)：对长度的指数惩罚，用于beam search。它作为指数被应用于序列的长度，反过来又被用来划分序列的分数。由于分数是序列的对数可能性（即负数），length_penalty &gt; 0.0会促进更长的序列，而length_penalty &lt; 0.0会鼓励更短的序列。</strong></p>
<p><strong>11、no_repeat_ngram_size (int, optional, defaults to 0)：如果设置为int &gt; 0，所有该大小的ngrams只能出现一次。</strong></p>
<p><strong>12、bad_words_ids(List[List[int]], optional)： 不允许生成的token id的列表。为了获得不应该出现在生成的文本中的词的标记ID，使用tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids，这个能用于敏感词过滤。</strong></p>
<p>13、force_words_ids(List[List[int]] or List[List[List[int]]], optional)：必须生成的标记ID的列表。如果给定的是List[List[int]]，这将被视为一个必须包含的简单单词列表，与bad_words_ids相反。如果给定的是List[List[List[int]]]，这将触发一个disjunctive约束，即可以允许每个词的不同形式。</p>
<p>14、renormalize_logits (bool, optional, defaults to False) ：在应用所有的logits处理器或warpers（包括自定义的）之后，是否要重新规范化logits。强烈建议将此标志设置为 “True”，因为搜索算法假定分数对数是正常化的，但一些对数处理器或翘曲器会破坏正常化。</p>
<p>15、constraints (List[Constraint], optional)： 可以添加到生成中的自定义约束，以确保输出将包含使用Constraint对象所定义的某些令牌，并尽可能以最合理的方式进行。</p>
<p><strong>16、forced_bos_token_id (int, optional, defaults to model.config.forced_bos_token_id)：强制作为解码器_start_token_id之后第一个生成的令牌的id。对于像mBART这样的多语言模型非常有用，在这种情况下，第一个生成的标记需要是目标语言的标记。</strong></p>
<p>17、forced_eos_token_id (Union[int, List[int]], optional, defaults to model.config.forced_eos_token_id) ：当达到max_length时，强制作为最后生成的令牌的id。可以选择使用一个列表来设置多个序列结束的标记。</p>
<p><strong>18、remove_invalid_values (bool, optional, defaults to model.config.remove_invalid_values) - 是否删除模型的可能nan和inf输出，以防止生成方法崩溃。注意，使用remove_invalid_values会减慢生成速度。</strong></p>
<p>19、exponential_decay_length_penalty (tuple(int, float), optional)： 这个Tuple在生成一定数量的标记后，增加一个指数级增长的长度惩罚。该元组应包括： (start_index, decay_factor），其中start_index表示惩罚开始的位置，decay_factor表示指数衰减的系数。</p>
<p>20、suppress_tokens (List[int], optional) ：一个在生成时将被抑制的标记的列表。SupressTokens的logit处理器将把它们的log probs设置为-inf，这样它们就不会被采样了。</p>
<p>21、begin_suppress_tokens (List[int], optional)： 一个将在生成之初被抑制的标记的列表。SupressBeginTokens日志处理器将把它们的日志probs设置为-inf，这样它们就不会被采样了。</p>
<p>22、forced_decoder_ids (List[List[int]], optional) ：一对整数的列表，表示从生成索引到标记索引的映射，在采样前将被强制。例如，[[1, 123]]意味着第二个生成的标记将总是一个索引为123的标记。</p>
<h3 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h3>
<ol>
<li><a href="https://finisky.github.io/nucleussampling/">https://finisky.github.io/nucleussampling/</a></li>
<li><a href="https://blog.csdn.net/jarodyv/article/details/128994176">https://blog.csdn.net/jarodyv/article/details/128994176</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/430961578?utm_id=0">https://zhuanlan.zhihu.com/p/430961578?utm_id=0_</a></li>
<li><a href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></li>
<li><a href="https://www.ctfiot.com/110138.html">https://www.ctfiot.com/110138.html</a></li>
<li><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L415C38-L415C38">https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py</a></li>
</ol>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>Mooncake阅读笔记：深入学习以Cache为中心的调度思想，谱写LLM服务降本增效新篇章</title>
    <url>/2025/01/26/Mooncake%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0%E4%BB%A5Cache%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E8%B0%83%E5%BA%A6%E6%80%9D%E6%83%B3%EF%BC%8C%E8%B0%B1%E5%86%99LLM%E6%9C%8D%E5%8A%A1%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88%E6%96%B0%E7%AF%87%E7%AB%A0/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://zhuanlan.zhihu.com/p/706097807">https://zhuanlan.zhihu.com/p/706097807</a></p>
</blockquote>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170549822.png" alt="image.png" /></p>
<p>这周，清华和Moonshot发了一个技术报告，介绍Kimi背后的LLM服务系统Mooncake，它采用分离式设计，将Prefill和Decode两阶段解耦，构建了一个全局KVCache Pool，实现以Cache为中心的调度。</p>
<p>Moonshot作为MaaS头部厂商，以其过硬的技术产品实力和明星的团队阵容闻名于世。和其他大模型公司不一样，他们很少发技术报告或对外做技术分享。这次Mooncake技术报告，让大家得对其技术得以管中窥豹。</p>
<span id="more"></span>
<p>论文的通信作者为Moonshot的许欣然与清华大学计算机系的章明星，两位均是重量级大咖，也分别在知乎宣传了Mooncake的工作。许欣然在AISys领域深耕多年，曾执掌MegEngine，工程经验丰富，如今在Moonshot担任工程副总裁一职。章明星过去研究领域是分布式系统、图数据库，其研究成果在2017年成为大陆作者单位在OSDI上的首篇一作论文，还曾经是ACM-ICPC World Finals，如今也开始关注大模型系统领域，大模型正汇聚着各行各业最顶尖人才，共襄盛举。</p>
<p><a href="https://zhuanlan.zhihu.com/p/705754254">ZHANG Mingxing：Mooncake (1): 在月之暗面做月饼，Kimi 以 KVCache 为中心的分离式推理架构866 赞同 · 147 评论文章</a>﻿<a href="https://zhuanlan.zhihu.com/p/705910725">a520 赞同 · 27 评论文章</a></p>
<p>Mooncake分离式架构动机是Prefill和Decode阶段性质不同，Prefill是计算密集，受限算力带宽用不满，Decode是访存密集性，受限带宽算力用不满，所以用同一种硬件部署两阶段往往顾此失彼，不是最有性价比。因此，最近很多工作对二者进行拆分，和Mooncake最相似的是今年5月份发布的微软和华盛顿大学的工作Splitwise，它里面列出了Prefill和Decode不同的七个Insights值得大家仔细读一下。因为Mooncake开发也需要一段时间，它和Splitwise应该是不谋而合的同期工作。</p>
<p>拆分Prefill/Decode之后，LLM推理系统就更像一个分布式内存系统+流处理系统，这就是传统计算机系统研究者最擅长的领域。某大佬和我讲的sys三板斧，batch， cache，调度都可以招呼上。比如，Decode可以进一步拆成Attention和非Attention算子分离调度，也是章明星团队近期的一个工作叫Attention Offloading。</p>
<p>高屋建瓴的分析二位作者的知乎已经讲得很透彻了，本文对技术报告具体系统设计做了一些注释，一方面用费曼学习法帮助自己理解，另一方面也给大家一些参考，方便读者读Mooncake文章时交叉验证。</p>
<h2 id="3-overview-of-mooncakes-disaggregated-archtecture"><a class="markdownIt-Anchor" href="#3-overview-of-mooncakes-disaggregated-archtecture"></a> 3 Overview of Mooncake’s Disaggregated Archtecture</h2>
<p>如下面Figure 1所示，Mooncake采用了<strong>分离式</strong>架构。这里分离有两层含义：</p>
<p>一是，将Prefill与Decode计算资源分开，这与前人工作无异，如splitwise和distserve等；Prefill阶段优化目标是利用request间存在共同前缀的机会，尽可能复用KVCache，同时满足TTFT（Time To First Token） SLO，最大化MFU（论文中似乎笔误成minimum）和KVCache小于CPU内存限制。Decode优化目标为最大化吞吐，满足TBT（Time between tokens ，Decode阶段两个Token之间的时间）SLO和KVCache小于GPU显存限制。</p>
<p>二是，将KVCache和计算分离开，它将GPU集群的CPU、DRAM、SSD和RDMA资源分组组成Distributed KVCache Pool，KVCache也是分块以Paged方式管理，KVCache Blocks如何在Pool中调度请求和复用KVCache乃本文精髓。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170557038.png" alt="image.png" /></p>
<p>我们跟随论文的安排，跟Figure 4先走马观花过一遍Mooncake处理一个request的流程。一个request到达（tokenized之后的），调度程序会选择一对Prefill Instance和Decode Instance，模型参数要在两个Instance都有副本，并启动包含四个步骤的工作流程：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170602401.png" alt="image.png" /></p>
<p>s1）<strong>KVCache Reuse</strong>：Prefill Instance将request分成token blocks，考虑request之间存在共同前缀，需要尽可能将token block调度到Prefix KVCache最长的节点处理，来resuse KVCache。为此作者提出一种以KVCache为中心的调度，将在章节5 KVCache-centric Scheduling中详细介绍。</p>
<p>s2）<strong>Incremental Prefill</strong>：使用Prefix Cache，Prefill Instance只需要计算Prefix Cache没覆盖的部分。长序列计算需要多卡并行，用TeraPipe方式做流水并行。将在章节5 Implementation of the Prefill Pool中详细介绍。</p>
<p>s3）<strong>KVCache Transfer</strong>：和Splitwise一样，分离是设计需要将KVCache从Prefill Instance搬运到Decode Instance。Mooncake通过异步传输，与上述Incremental Prefill步骤重叠，将每个模型层生成的KVCache流式传输到目标Decode Instance的CPU内存，以减少等待时间。</p>
<p>s4）<strong>Decoding</strong>：在Decoding Instance的CPU DRAM中接收到所有KVCache后，请求Continous Batching处理。</p>
<p>工作流中，s3, s4平平无奇，前人设计如Splitwise、DistServe和vLLM等已经覆盖。s1和s2比较精彩，本文接下来的章节4和章节5来详细介绍。</p>
<h2 id="4-implementation-of-the-prefill-pool"><a class="markdownIt-Anchor" href="#4-implementation-of-the-prefill-pool"></a> 4 Implementation of the Prefill Pool</h2>
<p>章明星说原本章叫“Prefill: To Seperate or Not. Is it a Question?” 。这说明分离式设计并不是共识。</p>
<p>Prefill和Decode的计算性质不同，前者吃计算，后者吃带宽。这会带来很多次生影响，比如Batching方式就不同，Prefill不需要batching，Decode需要大Batching。处理Prefill和Decode有<strong>融合派</strong>和<strong>分离派</strong>两大流派。</p>
<p>**融合派：**将Prefill放到Decode step间隙或者和某个Decode step一起推理。2022年OSDI的LLM Continous Batching 开山之作Orca将Prefill和Decode Step融合在一个Batching Step做forward，Orca时代还没有Paged Attention，还需要Selective Batching来将Attention解Batching。2023年vLLM做Batching时，prefill和decoding则是独立forward的，一个Batching step要么处理decoding要么处理prefill。prefill直接调用xformers处理计算密集的prefill attn计算；decoding手写CUDA PA处理访存密集的attn计算。后来，以Sarathi和FastGen为代表，将prefill序列拆成chunk，chunk prefilling可以插入到decode phase中，甚至和decode step融合。比如，flash attention的_flash_attn_with_kvcache_函数支持q部分有kvcache（decode）部分没有（prefill）。章明星也提到，对于_短的prefill chunk和decode step融合和纯做decode step延迟可能相同_，这相当于prefill白嫖decode没用满的算力。对这段Continous Batching发展历史感兴趣可以读：<a href="https://zhuanlan.zhihu.com/p/676109470">方佳瑞：大模型推理核心技术之Continuous Batching和我的WXG往事</a>。融合派的缺点是，Prefill和Decode在相同设备上，二者并行度被绑定。如果prompt长度有限，prefill阶段占比很小基本不到10%，所以忍一忍也无所谓。不过，对于长序列当Prefill比例升高，其Prefill并行度不够灵活的缺陷就暴露出来。</p>
<p>**分离派：**考虑Prefill/Decode性质差异，人们开始尝试把Prefill和Decode放到不同的设备来分别处理，比如，Splitwise和DistServe，Mooncake也是对他们的继承和发展。分离派可以给Prefill和Decode设置不同的并行度，二者有更大的灵活性。许欣然提到分离派_允许整个系统往 “算力” 和 “带宽” 的两个方向独立发展，对硬件优化也是更友好的_。在机房里，分离派可以用不同GPU混部来降本，比如H800做Prefill，H20做Decode，二者用RDMA互联。分离派遇到的最大挑战是如何在不同设备之间传输KVCache，集群需要高速网络来互联，而网络成本不容小觑。所以，分离派的硬件往往需求很高端，目前得是是A/H卡，硬件成本高且无法Scale也为人诟病。</p>
<p>分离派一个优势就是Prefill并行度灵活，为了降低长序列Prefill的TTFT，可以分配多卡甚至多机GPU并行处理Prefill，比如长序列我们就多分配一些GPU，短序列少点。如何做Prefill并行？长序列Prefill的batch size是1，没法用数据并行。张量并行性能不满足，它通信量很大无法扩展出节点，而且GQA的head number也限制了它并行度。那是否可以用序列并行（SP）？Ulysses通信量远低于TP，Ring可以和计算重叠，这里也感恩Mooncake引用了我们的最近的工作USP。但是SP推理需要在每个卡上replicate模型参数的，对大模型不利，如果用ZeRO方式shard参数，通信量都增加了很多；而且，SP每层都要通信，占用宝贵的网络带宽，网络带宽还得留着给KVCache送Decode Instance用。</p>
<p>Mooncake为Prefill阶段设计了Chunked Pipeline Parallelism (CPP) ，其实就是TeraPipe。TeraPipe正是vLLM核心作者Zhuohan Li的2021年的工作，当时是用在流水线训练里，这里只用它的forward过程即可。TeraPipe将模型沿着Layer切分成N个stage，放在不同设备上，将输入沿着序列维度切分成若干chunk。这样就可以在一个推理任务里，不同chunk流水在不同stage上的计算就可以流水起来起来。如下图所示，切分方式看左图，流水方式看右图一个Forward过程即可。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170613063.png" alt="image.png" /></p>
<p>TeraPipe用于训练时不能均分序列，一个token要和之前所有token做计算（因为Causal Attention引起的），位置越靠后计算量越大，均分序列的话Attention计算会负载不均衡，所以越靠后sequence chunk越小才好，所以TeraPipe论文用动态规划来找最佳划分策略。在Mooncake中，章明星说TeraPipe_只有 forward 没有 backword 的话不需要啥动态规划来平衡每个 Chunk 的计算量，只要给每个 Chunk 设置一个最小的值就行_。这点我是有些疑惑的，我觉得forward也需要和训练类似的负载均衡策略，如下图上半部分。对这个问题，我新开了一个文章讨论：<a href="https://zhuanlan.zhihu.com/p/706475158">方佳瑞：为Token-level流水并行找PMF：从TeraPipe，Seq1F1B，HPipe到PipeFusion</a></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170619193.png" alt="image.png" /></p>
<p>来自TeraPipe论文</p>
<p>TeraPipe做推理好处 1）它仅在每个stage的边界处需要跨节点通信，这可以很容易地与计算重叠。这导致更好的MFU并且KVCache传输的网络资源争用更少。2）短和长上下文均适用。原文给了解释原因是bringing no significant overhead for short context prefill and avoiding frequent dynamic adjustment of node partitioning。我理解是：layer分布在不同设备不需要改变，长短上下文都TeraPipe，长文相当于用上了多卡资源，尽管有些气泡，相当于并行加速了，可以满足TTFT。短文本来也不用并行TTFT单GPU也可以满足，因为TeraPipe通信少，所以和一个设备做Prefill时间一样，因此和_单个设备做Prefill比_没有明显开销。</p>
<p>我觉得这里还有一些问题可以深入讨论一下，第一，是流水并行的气泡问题，可以放一些Prefill阶段不同GPU的扩展性。第二，TeraPipe可以和SP组成混合并行，更容易去扩展到多机。第三，TeraPipe方式切分参数会导致Prefill并行度没法变化，切分成8个stage就必须一直做PP=8的并行了，因此，不能弹性改变Prefill的计算资源。当然，Mooncake可能在集群里放置很多Prefill Instance，每个Instance的并行度不同，然后在Instance之间做request-level的load balance。</p>
<p>这里安利一下我们团队的DiT扩散模型的并行推理工作PipeFusion也用了TeraPipe式的token-level切分的流水并行，因为DiT不是Causal Attention所以更适合TeraPipe方式推理。</p>
<h3 id="42-layer-wise-prefill"><a class="markdownIt-Anchor" href="#42-layer-wise-prefill"></a> 4.2 Layer-wise Prefill</h3>
<p>这一节比较晦涩，我理解是在Prefill阶段对KVCache做CPU Offloading（或者传输到Decode Instance？）。这些操作都是异步的，而且是Layer-wise的，也就是一层Prefill一个Layer的KVCache Block算出来，就立刻transfer（发给decode Instance）或dump（cpu offload）来最小化GPU显存占用。</p>
<p>为何着急忙慌赶人家KVCache走呢？我理解是因为Prefill机器比Decode少，因为Prefill负载占LLM推理的比例低，但是Prefill产生的KVCache和Decode消耗KVCache一样多，所以Decode那边为了把硬件榨干，需要让KVCache刚好用满GPU显存，那Prefill显存肯定不够，必须Offload。这也是Mooncake论文Figure 1中之所以写成“Prefill阶段KVCache &lt; DRAM”，“Decode阶段KVCache &lt; VRAM”的原因。</p>
<p>Mooncake论文Figure 5试图证明Layer-wise有效果。这个图画的草率了，全文没有提到Serialized是什么意思。我理解他是想说Splitwise论文中Fig. 11（下图），也就是KVCache从Prefill Instance通过Layer-wise方式传输给Decode Instance，这个可以和Prefill计算重叠起来，甚至和Decode第一个step部分计算重叠起来。Splitwise采用异步流式来和Prefill计算重叠，我觉得Mooncake也是类似。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170626067.png" alt="image.png" /></p>
<p>Splitwise论文Fig 11，Optimize KVCache transformers</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170631547.png" alt="image.png" /></p>
<p>Mooncake论文Figure 5</p>
<h2 id="5-kvcache-centric-scheduling"><a class="markdownIt-Anchor" href="#5-kvcache-centric-scheduling"></a> 5 KVCache-centric Scheduling</h2>
<p>这一章开始讲调度了。</p>
<p>KVCache Pool用Paged方式管理KVCache（本章简称Cache）。如下图所示，黄色是已经有的Prefix Cache Blocks，其他request算了，本request可以复用。粉色是本request自己算的Incremental Cache Blocks。这Pool也会用一些Cache策略来新陈代谢，比如LRU、LFU之类的。</p>
<p>Prefill节点接收到一个request，它根据prefix cache ID将前prefix cache从远程CPU内存加载到本地GPU内存，以启动request的prefill计算。如果不存在prefix cache，则需要自己Prefill计算了。这种选择平衡了三个目标：尽可能多地重用KVCache（三板斧中Caching）、平衡不同预填充节点的工作负载，并保证TTFT SLO（三板斧中Scheduling）。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170637887.png" alt="image.png" /></p>
<h3 id="51-prefill-global-scheduling"><a class="markdownIt-Anchor" href="#51-prefill-global-scheduling"></a> 5.1 Prefill Global Scheduling</h3>
<p>本节介绍如何给Prefill计算做reuse kvcache blocks和load balance。在Mooncake它将一个request切分成token blocks处理，类似FastGen和Sarathin中的Chunk。路由这些token blocks到哪台机器，要考虑因素很多。MoonShot用户很多，request之间有重叠部分，可以reuse共同前缀，prefix cache hit length越长计算越少。我猜测这个就是Kimi Context Caching能力的来源。但是，都路由到prefix cache hit length最长的机器，机器之间会负载不均衡，为了load balance，还需要考虑distribution of reusable KVCache blocks。本节也callback了KVCache为中心的宗旨，是让request主动去找KVCache。</p>
<p>先说怎么找到max prefix cache来尽可能reuse kvcache blocks。</p>
<p>一个token block经过prefill计算产生一个KVCache block，而且一个token block prefill计算需要所有Prefix token blocks的Prefix KVCache blocks。这些KVCache blocks都是在Distributed KVCache Pool中，不一定在本地内存，怎么快速检索到众多前缀KVCache blocks呢？需要建立一个token block -&gt; KVCache block的映射关系，根据映射关系去检索prefix token blocks的KVCache blocks。这个映射关系是一个Hash Table。</p>
<p>为了快速找到一个token block最大前缀max prefix token blocks，Hash设计有讲究。每个Block的Hash Key是基于前一个Block的Hash Key计算得到，图中B=Hash(A+b)。如果两个Block Hash Key相同，不仅该token block的KVCache block，那么之前所有prefix KVCache也都可以复用。如果不这样设计，你可能要反复遍历所有KVCache Hash，而用这种方式只需要遍历一次，检索代价从多项式降低到线性。这个技巧很巧妙，来自vllm。</p>
<p>再说怎么做load balance。</p>
<p>借助上面算出来的max prefix cache length信息，注意每个机器都不一样。可以用request长度+此机器的prefix长度+队列中的等待时间来估计TTFT。将request分配给估计最短TTFT的机器，并相应更新该机器的缓存和队列时间。如果无法实现SLO，直接返回HTTP 429 Too Many Requests。</p>
<h3 id="52-cache-load-balancing"><a class="markdownIt-Anchor" href="#52-cache-load-balancing"></a> 5.2 Cache Load Balancing</h3>
<p>本节介绍如何KVCache负载根据使用频率做再平衡。</p>
<p>在Mooncake集群中，每个Prefill机器管理其自己本地的prefix caches。这些caches的使用频率差异很大。例如，系统提示几乎被每个请求访问，而存储来自本地长文档内容的cache可能只被一个用户使用。我们希望不同机器cache使用频率相近，因此要调整cache在分布式集群的位置。</p>
<p>因为很难预测一个cache未来使用频率。Mooncake提出了一种基于启发式的自动热点迁移方案。上一节所述，request可能并不总是被定向到具有最长prefix cache length的Prefill机器上。在这种情况下，如果估计的prefill时间短于cache传输时间，因为要排队等待，cache位置和request转发到另一个机器。该机器主动检索KVCache并将其从远端机器拉取到本地。另外，如果远程机器的最佳prefix cache length不长，还可以重计算。</p>
<p>这两种策略合在一起，不仅减少了prefill时间，还促进了热点缓存的自动复制，使其能够更广泛地分布在多台机器上。</p>
<h2 id="6-overload-oriented-scheduling"><a class="markdownIt-Anchor" href="#6-overload-oriented-scheduling"></a> 6 Overload-oriented Scheduling</h2>
<p>现有的LLM服务通常假设所有请求都会被处理，并根据请求吞吐量、TTFT和TBT进行优化。然而，处理每个请求既不经济也不现实，尤其是在请求激增时，集群资源增长跟不上请求增长，导致过载。为了平衡成本和用户体验，系统应在负载达到阈值前尽可能处理请求，之后拒绝或推迟剩余请求。</p>
<p>本节描述了为Moonshot设计的early rejection policy，应该就是下图氪金之后和Kimi一起登月的背后原理。我没花时间看，就不班门弄斧分析了，但是这一环节对线上服务很重要。当然，作为用户还是希望Kimi不要拒绝服务。</p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>本文是阅读Mooncake技术报告的学习笔记。通过Mooncake还是学到了很多干货，这里也感谢作者团队能够分享技术。 短短一年内，创业团队能做出Mooncake这种完整的系统工作，并在线上服务海量用户，实打实节约成本，并给社区一些方向层面的输出，是非常了不起的成就。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>RadixAttention</title>
    <url>/2025/01/26/RadixAttention/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://zhuanlan.zhihu.com/p/715740300">https://zhuanlan.zhihu.com/p/715740300</a></p>
</blockquote>
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2312.07104">https://arxiv.org/pdf/2312.07104</a></p>
<p>博客地址：<a href="https://link.zhihu.com/?target=https%3A//lmsys.org/blog/2024-01-17-sglang/">https://lmsys.org/blog/2024-01-17-sglang/</a></p>
<h2 id="prefix-cache"><a class="markdownIt-Anchor" href="#prefix-cache"></a> Prefix Cache</h2>
<p>在讲 RadixAttention 之前我觉得有必要简单讲一下 Prefix Cache：</p>
<p>假设我们现在有这样一个场景，多次请求的 Prompt 可能会共享同一个前缀（Prefix），例如文档查询：</p>
<p>假设我们有一个很长的prompt如下所示是一个 markdown 表格:</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155952377.png" alt="image.png" /></p>
<p>然后我们有下面两条 Query 分别将表格与问题拼接起来进行查询表格：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155959088.png" alt="image.png" /></p>
<p>明显这两个 Prompt 都有共同的很长的前缀，如果我们把 Prompt 中共同的前缀产生的 KV Cache 给缓存下来，无疑会极大的减少 Prefill 阶段的时间从而减少 TTFT，并且如果不缓存，这个共同前缀的 KV Cache 将会被存两次，导致浪费显存，因此缓存下来还能减少显存从而增大吞吐！</p>
<p>这就是 Prefix Cache 所做的事情：将共享前缀的 KV Cache 给缓存起来，并且是一个能<strong>无损同时增加 TTFT 和吞吐</strong>的技术。</p>
<span id="more"></span>
<h2 id="radixattention"><a class="markdownIt-Anchor" href="#radixattention"></a> RadixAttention</h2>
<p>上面的 Prefix Cache 有这么几个问题还没有说明：</p>
<ul>
<li>
<p>怎么实现？用什么数据结构存储这些 Prefix Cache？</p>
</li>
<li>
<p>你要存多少个 Prompt 的 KV Cache？存满了该怎么办？</p>
</li>
</ul>
<p>RadixAttention 我理解相比于上述提到的 Prefix Cache，引入了树结构来进一步增强 Prefix Cache（如何管理前缀 token）。</p>
<ol>
<li>RadixAttention 引入树结构从而引入了 Prompt 之间的序列性和相关性，从而可以更好的共享前缀。</li>
</ol>
<p>让我们思考这样这一个问题，从一个 serving 系统的角度考虑，因为收到了很多的 Prompt，从而在没有打满 Prefix Cache pool 的大小情况下，你该怎么存储和管理这些共享 Prefix 的 Cache？简单的用一个 list 来存储？这样子你就失去了序列之间的相关性和顺序性。 以下图中的 (3) 图举例，你如果用一个 list 存储这两个 Prompt 的 Cache，那么比如第三个 Prompt 来了，第三个 Prompt 是：“this problem …”，是接着第二个 Prompt 续写的，那么你肯定就需要复用前两个 Prompt 的 Cache，如果仅使用 list 来存储，会导致失去序列之间的顺序性从而最多只能共享一个前缀。而引入树结构我们可以很简单的判断出第三个 Prompt 应该拼接在第二个 Prompt 后面，因此遍历树之后系统可以直接判断出要复用前两个 prompt 的 Cache。</p>
<ol start="2">
<li>RadixAttention 提出使用 LRU 策略来管理前缀树，具体来说就是 Prefix Cache pool 大小有限（显存有限），因此存到一定数目的 Prefix 的 Cache 后可以根据 LRU 策略来驱逐，从而在不至于打爆 Prefix Cache pool 的基础上还能拥有更大的 Cache hit rate。</li>
</ol>
<p>RadixAttention LRU 策略示例：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126160009501.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>PagedAttention</title>
    <url>/2025/01/26/PagedAttention/</url>
    <content><![CDATA[<p>论文链接: <a href="https://arxiv.org/pdf/2309.06180">https://arxiv.org/pdf/2309.06180</a></p>
<p>论文作者 Presentation: <a href="https://www.youtube.com/watch?v=Oq2SN7uutbQ">https://www.youtube.com/watch?v=Oq2SN7uutbQ</a></p>
<h1 id="llm推理的两阶段"><a class="markdownIt-Anchor" href="#llm推理的两阶段"></a> LLM推理的两阶段</h1>
<p>一个常规的LLM推理过程通常分为两个阶段：<strong>prefill和decode</strong>。<strong>通常会使用KV cache技术加速推理</strong>。</p>
<h2 id="kv-cache"><a class="markdownIt-Anchor" href="#kv-cache"></a> KV cache</h2>
<h5 id="kv-cache占用的显存空间"><a class="markdownIt-Anchor" href="#kv-cache占用的显存空间"></a> KV Cache占用的显存空间</h5>
<p>Q, K的形状为:[b,head_num,s,per_head_hidden_size], 那么对于每个decoder layer，每个 token 的 K、V 矩阵都是 embedding_size=num_heads * head_size，再乘上 seqlen和 batch size，那就是每个layer的 kv Cache 所需的存储容量了。</p>
<p><strong>例如，在 LLaMA 2-13B 中，head_num = 40, layer_num = 40, dimension = 5120, 对于1个token的 KV Cache 一共需要 2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16) = 800KB</strong>。</p>
<p><strong>以模型支持的最大的上下文长度4096为例，对于4096个token, 需要800KB * 4096 = 3.2GB的KV cache, 每增加一个batch则需要3.2GB显存，对于一张A100 40GB来说，在加载完模型26GB之后，余下的显存也仅能支持4个batch</strong></p>
<ul>
<li><strong>Activation</strong>：Activation在整个Self-Attention的计算过程中，最大的Activation是qkv的MatMul计算，Q,K,V的形状为：[b,head_num,s,per_head_hidden_size], 占用显存大小为2 * batch_size * seq_len * embedding_size, K^T的形状为：[b,head_num,per_head_hidden_size,s], QK^T的形状为：[b,head_num,s,s]，占用显存大小为2 * batch_size * head_num * seq_len^2, <strong>以模型支持的最大的上下文长度4096为例，对于4096个token, 每一层需要保存的峰值激活为大小为QK^T和V的输入, 大小为Q的输出, 4 * batch_size * seq_len * embedding_size + 2 * batch_size * head_num * seq_len^2 = 4 * 4096 * 5120 + 2 * 40 * 4096^2 = 1.3GB</strong></li>
</ul>
<h5 id="kv-cache能省下的flops"><a class="markdownIt-Anchor" href="#kv-cache能省下的flops"></a> KV Cache能省下的FLOPs</h5>
<p><strong>每个token的 K、V 矩阵计算一共需要 2 (K+V) * 2 (mul+add) * embedding size * embedding size = 4 * 5120 * 5120 这么多计算量，乘以seqlen、num_layer和 batch size，一共省了 4096 * 40 * 4 * 5120 * 5120 = 17 TFLOPs的计算量</strong>，当然，因seqlen和embedding size和num layer而异。</p>
<h2 id="llm推理"><a class="markdownIt-Anchor" href="#llm推理"></a> LLM推理</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153612578.png" alt="image.png" /></p>
<h3 id="prefill"><a class="markdownIt-Anchor" href="#prefill"></a> Prefill</h3>
<p><strong>预填充阶段。在这个阶段中，我们把整段prompt喂给模型做forward计算。如果采用KV cache技术，在这个阶段中我们会把prompt过后得到的保存在cache</strong> <em><strong>k和cache</strong></em> **v中。**这样在对后面的token计算attention时，我们就不需要对前面的token重复计算了，可以帮助我们节省推理时间。</p>
<p>在上面的图例中，我们假设prompt中含有3个token，prefill阶段结束后，这三个token相关的KV值都被装进了cache。</p>
<h3 id="decode"><a class="markdownIt-Anchor" href="#decode"></a> Decode</h3>
<p><strong>生成response的阶段</strong>。在这个阶段中，<strong>我们根据prompt的prefill结果，一个token一个token地生成response。</strong></p>
<span id="more"></span>
<p>同样，如果采用了KV cache，则每走完一个decode过程，我们就把对应response token的KV值存入cache中，以便能加速计算。例如对于图中的t4，它与cache中t0~t3的KV值计算完attention后，就把自己的KV值也装进cache中。对t6也是同理。</p>
<p><strong>由于Decode阶段的是逐一生成token的，因此它不能像prefill阶段那样能做大段prompt的并行计算，所以在LLM推理过程中，Decode阶段的耗时一般是更大的。</strong></p>
<p>从上述过程中，我们可以发现使用KV cache做推理时的一些特点：</p>
<ul>
<li>
<p><strong>随着prompt数量变多和序列变长，KV cache也变大，对gpu显存造成压力</strong></p>
</li>
<li>
<p><strong>由于输出的序列长度无法预先知道，所以我们很难提前为KV cache量身定制存储空间</strong></p>
</li>
</ul>
<p>下图展示了一个13B的模型在A100 40GB的gpu上做推理时的显存占用分配（others表示forward过程中产生的activation的大小，这些activation你可以认为是转瞬即逝的，即用完则废，因此它们占据的显存不大），从这张图中我们可以直观感受到推理中KV cache对显存的占用。<strong>因此，如何优化KV cache，节省显存，提高推理吞吐量，就成了LLM推理框架需要解决的重点问题。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153620319.png" alt="image.png" /></p>
<h1 id="为kv-cache分配存储空间的常规方式"><a class="markdownIt-Anchor" href="#为kv-cache分配存储空间的常规方式"></a> 为KV cache分配存储空间的常规方式</h1>
<p>对于训练好的模型，一种常用的部署方式是将其打包成一个推理服务（server），它接收客户端发送来的请求（request），读取请求中的数据（prompt）来做推理。一个请求中可以只有1个prompt，也可以包含多个prompt。</p>
<p>在常规的推理框架中，当我们的服务接收到一条请求时，它会为这条请求中的prompts分配gpu显存空间，其中就包括对KV cache的分配。由于推理所生成的序列长度大小是无法事先预知的，所以<strong>大部分框架会按照(batch_size, max_seq_len)这样的固定尺寸，在gpu显存上预先为一条请求开辟一块连续的矩形存储空间</strong>。然而，这样的分配方法很容易引起“gpu显存利用不足”的问题，进而影响模型推理时的吞吐量。你可能觉得这个描述有点抽象，别着急，我们来具体看一个例子。</p>
<p>下图展示了一个常规的推理框架是如何为请求中的prompt在gpu显存上分配KV cache的。在本例中，我们假设一个请求只发送1条prompt（本例中共有3条请求）：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153626947.png" alt="image.png" /></p>
<p>我们假设<code>max_seq_len = 8</code>，所以当第1条请求(prompt1)过来时，我们的推理框架为它安排了(1, 8)大小的连续存储空间。</p>
<p>当第2条请求（prompt2）过来时，同样也需要1块(1, 8)大小的存储空间。但此时prompt1所在的位置上，只剩3个空格子了，所以它只能另起一行做存储。对prompt3也是同理。</p>
<p><strong>仔细观察这3条prompt的KV cache排布，你是不是隐约觉得这种排布似乎没有充分利用起gpu的显存？</strong>：</p>
<ul>
<li>
<p><strong>浅色块</strong>：观察图中的浅色块，它是prefill阶段prompt的KV cache，是无论如何都会被使用的空间，它不存在浪费。</p>
</li>
<li>
<p><strong>中色块</strong>：观察图中的中色块，它是decode阶段的KV cache，其中<code>&lt;eos&gt;</code>表示序列生成的截止符。虽然这些中色块最终都会被我们用上，但是在decode阶段一个个token生成时，我们并不能预知哪些块会被最终用上。例如对于prompt2，当你生成when的时候，你无法知道下一个会生成<code>&lt;eos&gt;</code>，还是会生成别的词。<strong>所以这些中色块都是一种“潜在的浪费”，我们称中色块的部分为预留碎片（reservation fragment）。</strong></p>
</li>
<li>
<p><strong>深色块</strong>：观察图中的深色块，它也是decode阶段的KV cache，但直到序列生成完毕，它都没有被用上。<strong>由于这些深色块是预留的KV cache的一部分，所以我们称其为内部碎片（internal fragment）。</strong></p>
</li>
<li>
<p><strong>灰色块</strong>：观察图中的灰色块，它不是我们预留的KV cache的一部分，且最终也没有被用上，<strong>我们称这些灰色块为外部碎片（external fragment）</strong>。想象一下，此时新来了一条prompt4，它也要求显存中的8个格子作为KV cache。**此时你的显存上明明有9个空格子，但因为它们是不连续的碎片，所以无法被prompt4所使用。**这时prompt4的这条请求只好在队列中等待，直到gpu上有足够显存资源时再进行推理，这不就对模型推理的吞吐量造成显著影响了吗？</p>
</li>
</ul>
<p>**观察整个KV cache排布，你会发现它们的毛病在于太过“静态化”。**当你无法预知序列大小时，你为什么一定要死板地为每个序列预留KV cache空间呢？**为什么不能做得更动态化一些，即“用多少占多少”呢？**这样我们就能减少上述这些存储碎片，使得每一时刻推理服务能处理的请求更多，提高吞吐量，这就是vLLM在做的核心事情，我们先通过一张实验图来感受下vLLM在显存利用上的改进效果（VS 其它推理框架）：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153633933.png" alt="image.png" /></p>
<p>不难发现，相比于别的推理框架，vLLM几乎能做到将显存完全打满。</p>
<p>读到这里，你可能会有以下疑问：</p>
<ul>
<li>
<p><strong>问题1：vLLM是通过什么技术，动态地为请求分配KV cache显存，提升显存利用率的？</strong></p>
</li>
<li>
<p><strong>问题2: 当采用动态分配显存的办法时，虽然明面上同一时刻能处理更多的prompt了，但因为没有为每个prompt预留充足的显存空间，如果在某一时刻整个显存被打满了，而此时所有的prompt都没做完推理，那该怎么办？</strong></p>
</li>
</ul>
<p>在后文的第三～四章，我们将回答问题1。第五章回答问题2。</p>
<h1 id="pagedattention原理"><a class="markdownIt-Anchor" href="#pagedattention原理"></a> PagedAttention原理</h1>
<p>在本节中，<strong>我们先来回答问题1：vLLM通过一种名为PagedAttention的技术，动态地为请求分配KV cache显存，提升显存利用率。</strong></p>
<p><strong>整体上来说，PagedAttention的设计灵感来自操作系统中虚拟内存的分页管理技术</strong>。所以本节会先通过通俗易懂的方式，和大家一起快速回顾操作系统的虚拟内存技术，在这个过程中和大家一起具象化感受PagedAttention的设计思想。然后再来详细介绍PagedAttention的运作流程。</p>
<h2 id="31-操作系统的虚拟内存"><a class="markdownIt-Anchor" href="#31-操作系统的虚拟内存"></a> 3.1 操作系统的虚拟内存</h2>
<h5 id="1不使用虚拟内存"><a class="markdownIt-Anchor" href="#1不使用虚拟内存"></a> （1）不使用虚拟内存</h5>
<p>我们知道程序运行时，会将代码、数据等内容存放在物理内存上。<strong>在最原始的做法中（没有操作系统，例如单片机），程序直接对物理内存进行操作，决定使用它的哪些存储地址。</strong></p>
<p><strong>如果你只跑一个进程，那还好说。但如果需要运行多个进程时，麻烦就来了</strong>：由于我直接操作了物理内存地址，所以我在为自己的进程分配物理内存时，还要考虑别的进程是如何分配物理内存的（别人已经占用的我不能用）。这样不同进程间的耦合性太高了，给开发带来难度。</p>
<p><strong>有没有一种办法，让各个进程间的开发能够相互独立呢？一种直觉的做法是：</strong></p>
<ul>
<li>
<p><strong>给每个进程分配一个虚拟内存</strong>。每个进程在开发和运行时，可以假设这个虚拟内存上只有自己在跑，这样它就能大胆操作。</p>
</li>
<li>
<p><strong>虚拟内存负责统一规划代码、数据等如何在物理内存上最终落盘</strong>。这个过程对所有进程来说都是透明的，进程无需操心</p>
</li>
</ul>
<p>虚拟内存的核心思想可简化成下图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153643116.png" alt="image.png" /></p>
<h5 id="2虚拟内存的分段管理"><a class="markdownIt-Anchor" href="#2虚拟内存的分段管理"></a> （2）虚拟内存的分段管理</h5>
<p><strong>在分段式内存管理中，虚拟内存会尽量为每个进程在物理内存上找到一块连续的存储空间，让进程加载自己的全部代码、数据等内容</strong>。我们来看一个具体的例子：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153649549.png" alt="image.png" /></p>
<p>在这个例子中，3个进程的虚拟内存各自为它们在物理内存上映射了一块连续的存储空间。在某一时刻，我释放了进程2，同时想运行进程4。这时我尴尬地发现，<strong>虽然物理内存上有640M的空间剩余，但因为是碎片化的，我的进程4无法加载进去</strong>，因此它只能等待（回想一下本文第二部分对传统KV cache显存分配的分析）。</p>
<p>在这个情况下，如果我硬要运行进程4，也是有办法的：我可以先把进程3从物理内存上<strong>交换（swap）<strong>到磁盘上，然后把进程4装进来，然后再把进程3从磁盘上加载回来。通过这种方法我</strong>重新整合了碎片</strong>，让进程4能够运行。</p>
<p>**但这种办法的显著缺点是：**如果进程3过大，同时内存到磁盘的带宽又不够，整个交换的过程就会非常卡顿。这就是分段式内存管理的缺陷。</p>
<p>这时，我自然而然会想到：**我为什么要给所有进程都预分配一个固定的存储块（段）呢？**假设这个进程是一个浏览器，我难道会一下就用到这个进程里所有的功能吗？就不能进程运行到哪里，或者我想用哪个具体功能时，再加载这部分相关的内容去内存，以此让整个内存分配更加动态？</p>
<h5 id="3虚拟内存的分页管理"><a class="markdownIt-Anchor" href="#3虚拟内存的分页管理"></a> （3）虚拟内存的分页管理</h5>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153655368.png" alt="image.png" /></p>
<p>**我们可以将进程1、进程2想成是两本书。代码分布在书的不同page上。我们希望想读哪一页，就加载哪一页，而不是一下把两本书都加载进来。**同时，当我们不想读某些页的时候，我们也能根据页码将其清空。</p>
<p>现在，我们希望读进程1和进程2的page1，我们就将其加载到物理内存上。虚拟内存会帮我们做好映射，把来自不同进程的这两页分别加载到物理内存对应位置。</p>
<p><strong>虚拟内存的分业管理技术总结起来就是：</strong></p>
<ul>
<li>
<p><strong>将物理内存划分为固定大小的块，我们称每一块为页（page）</strong>。从物理内存中模拟出来的虚拟内存也按相同的方式做划分</p>
</li>
<li>
<p>对于1个进程，我们不需要静态加载它的全部代码、数据等内容。我们想用哪部分，或者它当前跑到哪部分，我们就动态加载这部分到虚拟内存上，然后由虚拟内存帮我们做物理内存的映射。</p>
</li>
<li>
<p>对于1个进程，虽然它在物理内存上的存储不连续（可能分布在不同的page中），但它在自己的虚拟内存上是连续的。<strong>通过模拟连续内存的方式，既解决了物理内存上的碎片问题，也方便了进程的开发和运行。</strong></p>
</li>
</ul>
<h2 id="32-pagedattention"><a class="markdownIt-Anchor" href="#32-pagedattention"></a> 3.2 PagedAttention</h2>
<h5 id="1处理单个请求"><a class="markdownIt-Anchor" href="#1处理单个请求"></a> （1）处理单个请求</h5>
<p>现在，你已经知道虚拟内存分页管理的基本原理和优势，趁热打铁，我们马上来看以其为灵感的PagedAttention技术是如何操作的。我们还是从具体的例子讲起。</p>
<p>假设现在你向模型server发送一条请求，prompt为<code>Four score and seven years ago our</code>，你希望模型能做续写。PagedAttention的运作流程如下图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153701635.png" alt="image.png" /></p>
<p>在图中：</p>
<ul>
<li>
<p><strong>请求（request）可理解为操作系统中的一个进程</strong></p>
</li>
<li>
<p><strong>逻辑内存（logical KV blocks）可理解为操作系统中的虚拟内存，每个block类比于虚拟内存中的一个page。每个block的大小是固定的，在vLLM中默认大小为16，即可装16个token的K/V值</strong></p>
</li>
<li>
<p><strong>块表（block table）可理解为操作系统中的虚拟内存到物理内存的映射表</strong></p>
</li>
<li>
<p><strong>物理内存（physical KV blocks）可理解为操作系统中的物理内存，物理块在gpu显存上，每个block类比于虚拟内存中的一个page</strong></p>
</li>
</ul>
<p>图中带圈的序号表示操作步骤，我们就按这个顺序来看看。</p>
<h6 id="i-prefill阶段"><a class="markdownIt-Anchor" href="#i-prefill阶段"></a> (i) Prefill阶段</h6>
<ul>
<li>
<p><strong>划分逻辑块</strong>：vLLM拿到这条prompt，先按照设定好的block大小B（本例中B=4），为prompt划分逻辑块（Logical KV blocks）。由于prompt中有7个token，所以vLLM用2个逻辑块（block 0， block 1）来装它们的KV值。其中，在逻辑块1中目前只装了&quot;years&quot;, “ago”, &quot;hour&quot;这3个token的KV值，有1个位置是空余的。这个位置就被称为保留位（reservation）</p>
</li>
<li>
<p><strong>划分物理块</strong>：划分好逻辑块后，我们就可以将其映射到物理块中去了。物理块是实际存放KV值的地方。我们通过一张block table来记录逻辑块和物理块的映射关系，block table的主要内容包括：</p>
</li>
<li>
<p><strong>逻辑块和物理块的映射关系（physical block number）</strong>：例如逻辑块0对应物理块7</p>
</li>
<li>
<p><strong>每个物理块上被填满的槽位（# filled）</strong>：例如在prefill阶段，对物理块7，其4个槽位都被填满；对物理块1，其3个槽位被填满。</p>
</li>
<li>
<p><strong>正常计算prompt的KV值，并通过划分好的关系填入物理块中。</strong></p>
</li>
</ul>
<h6 id="iidecode阶段-生成第1个词"><a class="markdownIt-Anchor" href="#iidecode阶段-生成第1个词"></a> （ii）Decode阶段-生成第1个词</h6>
<ul>
<li>
<p><strong>使用KV cache计算attention，生成第1个词fathers</strong>。不难发现，当我们计算时，我们使用的是逻辑块，即形式上这些token都是连续的。与此同时，vLLM后台会通过block table这个映射关系，帮我们从物理块上获取数据做实际计算。<strong>通过这种方式，每个request都会认为自己在一个连续且充足的存储空间上操作，尽管物理上这些数据的存储并不是连续的。</strong></p>
</li>
<li>
<p><strong>基于新生成的词，更新逻辑块、物理块和block table</strong>。对于block table，vLLM将它filled字段由3更新至4。</p>
</li>
<li>
<p><strong>分配新的逻辑块和物理块</strong>。当fathers更新进去后，逻辑块已装满。所以vLLM将开辟新的逻辑块2，并同时更新对应的block table和物理块。</p>
</li>
</ul>
<h6 id="iiideocde阶段-生成第2个词"><a class="markdownIt-Anchor" href="#iiideocde阶段-生成第2个词"></a> （iii）Deocde阶段-生成第2个词</h6>
<p>类比步骤（2）来进行。</p>
<h5 id="2处理多个请求"><a class="markdownIt-Anchor" href="#2处理多个请求"></a> （2）处理多个请求</h5>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153709241.png" alt="image.png" /></p>
<p>有了（1）的解释，大家看懂这张图应该不难。通过多个请求（prompt）同时做推理的例子，大家可以更好感受到PagedAttention是如何通过动态存储KV cache的方式，来更充分利用gpu显存的。</p>
<h1 id="pagedattention在不同解码场景下的例子"><a class="markdownIt-Anchor" href="#pagedattention在不同解码场景下的例子"></a> PagedAttention在不同解码场景下的例子</h1>
<p>通过前文的解释，我们已经基本掌握了PagedAttention的设计思想、运作流程。你可能隐隐能感受到它在显存管理上的“灵活性”，和减少碎片化显存的能力。<strong>但可能你觉得还不够具象，所以在本节中，我们通过更具体的场景，再假设一下对PagedAttention优势的理解。</strong></p>
<p><strong>我们知道，根据实际需求，大模型的解码方式也比较复杂</strong>，例如：</p>
<ul>
<li>
<p><strong>Parallel Sampling</strong>：我给模型发送一个请求，希望它对prompt做续写，并给出三种不同的回答。我们管这个场景叫parallel sampling。在这个场景中，我们可以将prompt复制3次后拼接成1个batch喂给模型，让它做推理。但我们也需注意到，这种方式会产生prompt部分KV cache的重复存储。</p>
</li>
<li>
<p><strong>Beam Search</strong>：束搜索，这是LLM常用的deocde策略之一，即在每个decode阶段，我不是只产生1个token，而是产生top k个token（这里k也被称为束宽）。top k个token必然对应着此刻的top k个序列。我把这top k个序列喂给模型，假设词表的大小为|V|，那么在下一时刻，我就要在k * |V|个候选者中再选出top k，以此类推。不难想象每一时刻我把top k序列喂给模型时，它们的前置token中有大量的KV cache是重复的。</p>
</li>
<li>
<p><strong>Shared prefix</strong>：在某些大模型中，所有请求可能都会共享一个前置信息（比如system message: “假设你是一个有帮助的AI助手…&quot;），这些前置信息没有必要重复存储KV cache</p>
</li>
<li>
<p><strong>其余一般场景</strong>：在一些更通用的场景中，虽然两个prompt可能完全没有关系，但它们中某些KV cache却是可以共用的。例如两个prompt的相同位置（position）恰好出现了完全一样的序列，比如它们的结尾都是好想下班。假设这个相同序列已经存在于KV cache中，那也没有必要重复计算和存储了。</p>
</li>
</ul>
<p><strong>在下文里，我们会详细解释PagedAttention在Parallel Sampling和Beam Search场景上的优势。剩余两个场景读者可以自行做类比分析。</strong></p>
<h2 id="41-parallel-sampling"><a class="markdownIt-Anchor" href="#41-parallel-sampling"></a> 4.1 Parallel Sampling</h2>
<p>下面说明在parallel sampling的场景下，vLLM（PagedAttention）是怎么做到节省显存的。</p>
<p>**传统KV cache怎么做：**假设模型的max_seq_len = 2048。传统KV cache可能在显存中分配两块长度是2048的空间。由于prompt一致，这两块2048的空间中存在大量重复的KV cache。</p>
<p><strong>vLLM怎么做：</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153717004.png" alt="image.png" /></p>
<p>假定我们发给模型1个request，这个request中包含2个prompt/sample，记为Sample A1和Sample A2，这两个prompt完全一致，都为<code>Four score and seven years ago our</code>，我们希望模型对这两个prompt分别做续写任务。</p>
<p><strong>（1）首先，Prefill阶段，vLLM拿到Sample A1和Sample A2，根据其中的文字内容，为其分配逻辑块和物理块。</strong></p>
<ul>
<li>
<p><strong>分配逻辑块</strong>：对于A1，vLLM为其分配逻辑块block0和block1；对于A2，vLLM为其分配逻辑块block0和block1。<strong>需要注意的是，A1的逻辑块和A2的逻辑块是独立的（尽管它们都叫block0和block1）</strong>，你可以将A1和A2视作操作系统中两个独立运行的进程。</p>
</li>
<li>
<p><strong>分配物理块</strong>：对于A1和A2，虽然逻辑块独立，但因为它们的文字完全相同，所以可以<strong>在物理内存上共享相同的空间</strong>。所以A1的逻辑块block0/1分别指向物理块block7/1；A2的逻辑块block0/1分别指向物理块block7/1。我们设每个物理块下映射的逻辑块数量为ref count，所以对物理块block7/1来说，它们的ref count都为2。</p>
</li>
</ul>
<p><strong>（2）然后，进入decode阶段，A1和A2各自做推理，得到第一个token，分别为fathers和mothers。</strong></p>
<ul>
<li>
<p><strong>将生成的token装入逻辑块</strong>：对于A1和A2来说，将其生成的token装入各自的逻辑块block1。</p>
</li>
<li>
<p><strong>触发物理块copy-on-write机制</strong>：由于fathers/mothers是两个完全不同的token，因此对物理块block1触发复制机制，即在物理内存上新开辟一块空间。此时物理块block1只和A2的逻辑块block1映射，将其ref count减去1；物理块block3只和A1的逻辑块block1映射，将其ref count设为1。</p>
</li>
</ul>
<p>总结起来，vLLM节省KV cache显存的核心思想是，对于相同数据对应的KV cache，能复用则尽量复用；无法复用时，再考虑开辟新的物理空间。</p>
<h3 id="42-beam-search"><a class="markdownIt-Anchor" href="#42-beam-search"></a> 4.2 Beam Search</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153724678.png" alt="image.png" /></p>
<p><strong>我们从右往左来看这张图。虚线位置表示“当前decoding时刻”，beam width = 4。图中所有的block皆为逻辑块。</strong></p>
<p>因为beam width = 4，这意味着根据beam search算法，在当前阶段我们生成了top 4个概率最大的token（我们记这4个token为beam candidate 0/1/2/3），它们分别装在block5，block6，block7和block8中。</p>
<p>现在我们继续使用beam search算法做decoding，继续找出top 4个最可能的next token。经过我们的计算，这top 4 next token，有2个来自beam candidate 1，有2个来自beam candidate 2。因此我们在block6中引出block9和block10，用于装其中两个top 2 next token；对block7也是同理。</p>
<p>现在，block9/10/11/12中装的top 4 next token，就成为新的beam candidates，可以按照和上述一样的方式继续做beam search算法。<strong>而对于block5和block8，它们已经在beam search的搜索算法中被淘汰了，后续生成的token也不会和它们产生关系，所以可以清除掉这两个逻辑块，并释放它们对应的物理块的内存空间。</strong></p>
<p>好，我们继续往左边来看这幅图。block3引出block5/6/7，block4引出block8，这意味着当前这4个top4 token，是上一个timestep下candidate1和candidate3相关序列生成的（candidate0和2的block没有画出，是因为它们所在的序列被beam search算法淘汰了，因此没有画出的必要）。<strong>由于block8已经被淘汰，所以block4也相继被淘汰，并释放对应的物理内存空间。</strong></p>
<p><strong>由此往左一路推，直到block0为止（block0代表着prompt，因此被beam seach中所有的序列共享）。这一路上，我们都根据最新时刻的beam search decoding结果，释放掉不再被需要的逻辑块和对应的物理内存空间，达到节省显存的目的。</strong></p>
<h1 id="调度和抢占"><a class="markdownIt-Anchor" href="#调度和抢占"></a> 调度和抢占</h1>
<p>到目前为止，我们已经回答了“vLLM是如何优化KV cache显存分配”的问题，现在我们来回答另一个重要的问题：</p>
<ul>
<li><strong>当采用动态分配显存的办法时，虽然明面上同一时刻能处理更多的prompt了，但因为没有为每个prompt预留充足的显存空间，如果在某一时刻整个显存被打满了，而此时所有的prompt都没做完推理，那该怎么办？</strong></li>
</ul>
<h3 id="51-总原则"><a class="markdownIt-Anchor" href="#51-总原则"></a> 5.1 总原则</h3>
<p>当有一堆请求来到vLLM服务器上时，vLLM需要一个调度原则来安排如何执行这些请求，这个调度原则概括如下：</p>
<ul>
<li>
<p><strong>先来的请求先被服务（First-Come-First-Serve, FCFS）</strong></p>
</li>
<li>
<p><strong>如有抢占的需要，后来的请求先被抢占（preemption）</strong></p>
</li>
</ul>
<p><strong>（1）先来的请求先被服务</strong>这个很好理解，当有一堆请求到达vLLM服务器时，vLLM肯定优先处理来得早的请求</p>
<p><strong>（2）后来的请求先被抢占</strong>想象一下，当一堆请求来到vLLM服务器做推理，导致gpu显存不足时，vLLM会怎么做呢？</p>
<p><strong>最直接的办法，就是暂停这堆请求中最后到达的那些请求的推理，同时将它们相关的KV cache从gpu上释放掉，以便为更早到达的请求留出足够的gpu空间，让它们完成推理任务</strong>。如果不这样做的话，各个请求间相互争夺gpu资源，最终将导致没有任何一个请求能完成推理任务。等到先来的请求做完了推理，vLLM调度器认为gpu上有足够的空间了，就能恢复那些被中断的请求的执行了。</p>
<p><strong>在资源不足的情况下，暂时中断一些任务的执行，这样的举动就被称为“抢占（preemption）”。</strong></p>
<h3 id="52-终止和恢复被抢占的请求"><a class="markdownIt-Anchor" href="#52-终止和恢复被抢占的请求"></a> 5.2 终止和恢复被抢占的请求</h3>
<p>对于这些因gpu资源不足而被抢占的任务，vLLM要完成两件事：</p>
<ul>
<li>
<p><strong>暂停它们的执行，同时将与之相关的KV cache从gpu上释放掉</strong></p>
</li>
<li>
<p><strong>等gpu资源充足时，重新恢复它们的执行</strong></p>
</li>
</ul>
<p>针对这两件事，vLLM分别设计了**Swapping（交换策略）和Recomputation（重计算策略）**来解决。我们来细看这两个策略。</p>
<h5 id="1swapping"><a class="markdownIt-Anchor" href="#1swapping"></a> （1）Swapping</h5>
<p>对于被抢占的请求，vLLM要将其KV cache从gpu上释放掉，那么：</p>
<ul>
<li>
<p><strong>问题1：该释放哪些KV cache？</strong></p>
</li>
<li>
<p><strong>问题2：要把这些KV cache释放到哪里去？</strong></p>
</li>
</ul>
<p>**先看问题1。**由前文PagedAttention原理可知，一个请求可能对应多个block。我们既可以选择释放掉部分block，也可以选择释放掉全部block，或者更科学地，我们可以预测一下哪些block被使用的频率最低，然后释放掉这些低频block（但这种方式实现起来难度较大，性价比不是很高）。在vLLM中，采取的是all-or-nothing策略，即释放被抢占请求的所有block。</p>
<p>**再来看问题2。**对于这些被选中要释放的KV block，如果将它们直接丢掉，那未免过于浪费。vLLM采用的做法是将其从gpu上交换（Swap）到cpu上。这样等到gpu显存充份时，再把这些block从cpu上重载回来。</p>
<h5 id="2recomputation"><a class="markdownIt-Anchor" href="#2recomputation"></a> （2）Recomputation</h5>
<p>知道了Swapping机制，重计算的过程也很好理解了：当vLLM调度器任务gpu资源充足时，对于那些被抢占的请求，它会将其卸载到cpu上的KV block重新加载进gpu中，继续完成推理任务。</p>
<h5 id="3总结"><a class="markdownIt-Anchor" href="#3总结"></a> （3）总结</h5>
<p>好，到这里，<strong>我们总结一下vLLM对请求的调度处理流程：</strong></p>
<ul>
<li>
<p>当一堆请求来到vLLM服务器上时，按照**First-Come-First-Serve（FCFS）**原则，优先处理那些最早到来的请求。</p>
</li>
<li>
<p>当gpu资源不足时，为了让先来的请求能尽快做完推理，<strong>vLLM会对那些后到来的请求执行“抢占”</strong>，即暂时终止它们的执行。</p>
</li>
<li>
<p>**一旦vLLM决定执行抢占操作，它会暂停处理新到来的请求。**在此期间，它会将被抢占的请求相关的KV block全部交换（swap）至cpu上。<strong>等交换完成后，vLLM才会继续处理新到来的请求。</strong></p>
</li>
<li>
<p>当vLLM认为gpu有足够资源时，它会将cpu上的KV block重新加载回gpu，恢复被抢占请求的执行（recomputation）</p>
</li>
</ul>
<h1 id="分布式管理"><a class="markdownIt-Anchor" href="#分布式管理"></a> 分布式管理</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126153735588.png" alt="image.png" /></p>
<p>在本文的最后部分，我们再来看看分布式环境下vLLM的整体架构。本文不再对vLLM的性能实验部分做说明，感兴趣的朋友可以自行阅读。</p>
<p>在LLM推理实操中，某些场景下单卡是完成不了推理的，需要多卡。那么对于多gpu这种更普适性的情况，vLLM是怎么处理的呢？</p>
<p>上图显示了在分布式场景下，vLLM的整体运作流程：</p>
<ul>
<li>
<p>首先，vLLM有一个中央调度器（Scheduler），它负责计算和管理每张卡上KV cache从逻辑块到物理块的映射表(block tables)</p>
</li>
<li>
<p>在做分布式计算时，Schedular会将映射表广播到各张卡上，每张卡上的Cache engine接收到相关信息后，负责管理各卡上的KV block</p>
</li>
</ul>
<p>上图中给出的例子，是用张量模型并行（megatron-lm）做分布式推理时的情况，所以图中每个worker上写的是model shard。<strong>在张量并行中，各卡上的输入数据相同，只是各卡负责计算不同head的KV cache</strong>。所以这种情况下，各卡上的逻辑块-物理块的映射关系其实是相同的（用的同一张block table），只是各卡上物理块中实际存储的数据不同而已。</p>
<h1 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h1>
<ul>
<li>
<p><a href="https://aijishu.com/a/1060000000458185">https://aijishu.com/a/1060000000458185</a></p>
</li>
<li>
<p><a href="https://my.oschina.net/IDP/blog/11046053">https://my.oschina.net/IDP/blog/11046053</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>投机解码 (Speculative Decoding)</title>
    <url>/2025/01/26/%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81-Speculative-Decoding/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://www.53ai.com/news/finetuning/2024071109285.html">https://www.53ai.com/news/finetuning/2024071109285.html</a></p>
</blockquote>
<h1 id="一-背景"><a class="markdownIt-Anchor" href="#一-背景"></a> 一、背景</h1>
<p>关于 Speculative Decoding 的综述文章 [2401.07851] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding，如下图Figure 3 所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154418463.png" alt="image.png" /></p>
<span id="more"></span>
<h1 id="二-自回归解码autoregressive-decoding"><a class="markdownIt-Anchor" href="#二-自回归解码autoregressive-decoding"></a> 二、自回归解码（Autoregressive Decoding）</h1>
<h3 id="21-概述"><a class="markdownIt-Anchor" href="#21-概述"></a> 2.1. 概述</h3>
<p>当前的主流 LLM 基本都是 Decoder Only 的 Transformer 模型，其推理过程可以分为两个阶段：</p>
<ul>
<li>
<p>Prefill：根据输入 Tokens（Recite, the, first, law, of, robotics） 生成第一个输出 Token（A），通过一次 Forward 就可以完成，在 Forward 中，输入 Tokens 间可以并行执行（类似 Bert 这些 Encoder 模型），因此执行效率很高。</p>
</li>
<li>
<p>Decoding：从生成第一个 Token（A） 之后开始，采用自回归方式一次生成一个 Token，直到生成一个特殊的 Stop Token（或者满足用户的某个条件，比如超过特定长度） 才会结束，假设输出总共有 N 个 Token，则 Decoding 阶段需要执行 N-1 次 Forward，这 N-1 次 Forward 只能串行执行，效率很低。另外，在生成过程中，需要关注的 Token 越来越多（每个 Token 的生成都需要 Attention 之前的 Token），计算量也会适当增大。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/65db84a0f0924ade9b633620554eaac7" alt="image.gif" /></p>
<h3 id="22-kv-cache"><a class="markdownIt-Anchor" href="#22-kv-cache"></a> 2.2 KV Cache</h3>
<p>如下图所示，在 LLM 推理中最关键的就是下图中的 Multi-Head Attention，其主要的计算集中在左图中灰色的 Linear（矩阵乘）和 Scaled Dot-Product Attention 中的 MatMul 矩阵乘法：</p>
<blockquote>
<p>🔗 原文链接：<a href="https://www.53ai.com/news/finetuning/2024071109285.html">https://www.53ai.com/news/finetuning/2024071109285.html</a></p>
</blockquote>
<h1 id="一-背景-2"><a class="markdownIt-Anchor" href="#一-背景-2"></a> 一、背景</h1>
<p>关于 Speculative Decoding 的综述文章 [2401.07851] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding，如下图Figure 3 所示：</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/fb671e3290db46af8cb88da84646f3c7" alt="" /></p>
<h1 id="二-自回归解码autoregressive-decoding-2"><a class="markdownIt-Anchor" href="#二-自回归解码autoregressive-decoding-2"></a> 二、自回归解码（Autoregressive Decoding）</h1>
<h3 id="21-概述-2"><a class="markdownIt-Anchor" href="#21-概述-2"></a> 2.1. 概述</h3>
<p>当前的主流 LLM 基本都是 Decoder Only 的 Transformer 模型，其推理过程可以分为两个阶段：</p>
<ul>
<li>
<p>Prefill：根据输入 Tokens（Recite, the, first, law, of, robotics） 生成第一个输出 Token（A），通过一次 Forward 就可以完成，在 Forward 中，输入 Tokens 间可以并行执行（类似 Bert 这些 Encoder 模型），因此执行效率很高。</p>
</li>
<li>
<p>Decoding：从生成第一个 Token（A） 之后开始，采用自回归方式一次生成一个 Token，直到生成一个特殊的 Stop Token（或者满足用户的某个条件，比如超过特定长度） 才会结束，假设输出总共有 N 个 Token，则 Decoding 阶段需要执行 N-1 次 Forward，这 N-1 次 Forward 只能串行执行，效率很低。另外，在生成过程中，需要关注的 Token 越来越多（每个 Token 的生成都需要 Attention 之前的 Token），计算量也会适当增大。</p>
</li>
</ul>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/65db84a0f0924ade9b633620554eaac7" alt="" /></p>
<h3 id="22-kv-cache-2"><a class="markdownIt-Anchor" href="#22-kv-cache-2"></a> 2.2 KV Cache</h3>
<p>如下图所示，在 LLM 推理中最关键的就是下图中的 Multi-Head Attention，其主要的计算集中在左图中灰色的 Linear（矩阵乘）和 Scaled Dot-Product Attention 中的 MatMul 矩阵乘法：</p>
<p><img src="https://bj.bcebos.com/brainbox-online/app/brainBox/image/webScissorStorage/b3160b698f034a1c97ff33af81ce1943" alt="" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154703961.png" alt="image.png" /></p>
<p>如上右图中的 Mask 是一个下三角矩阵，也是因为这个下三角矩阵实现了 LLM Decoder 的主要特性，每个 Token 都只能看到当前位置及之前的 Token。</p>
<p>如下图所示，其中的 QKT 可以理解为一个相关性矩阵，如动图所示，4 个 Token 对应 4 个 Step，其中：</p>
<ul>
<li>
<p>Step 2 依赖 Step 1 的结果，相关性矩阵的第 1 行不用重复计算</p>
</li>
<li>
<p>Step 3 依赖 Step 1 和 Step 2 的结果，相关性矩阵的第 1 行和第 2 行不用重复计算</p>
</li>
<li>
<p>Step 4 依赖 Step 1、Step 2 和 Step 3 的结果，相关性矩阵的第 1 行、第 2 行和第 3 行不用重复计算</p>
</li>
</ul>
<p>在 Decoding 阶段 Token 是逐个生成的，上述的计算过程中每次都会依赖之前的结果，此时最简单的思路就是 Cache 之前计算过的中间结果，在计算当前 Token 时直接从 Cache 中读取而不是重新计算，如下图所示，上面是没有 Cache 的情况，下面是有 Cache 的情况：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/3f7b476db20e4776a3b79b3f2289fd97" alt="" /></p>
<p>如下表所示，在 T4 GPU 上以 GPT2 模型为例验证有无 Cache 对推理时延的影响，其加速效果非常明显，因此也成为 LLM 推理的标配：</p>
<table>
<thead>
<tr>
<th>GPT2/T4</th>
<th>无 KV Cache</th>
<th>有 KV Cache</th>
<th>加速比</th>
</tr>
</thead>
<tbody>
<tr>
<td>Output Token 1000</td>
<td>52.53s</td>
<td>9.12s</td>
<td>5.76x</td>
</tr>
</tbody>
</table>
<p>当然，KV Cache 也有一定不足，其相当于使用空间换时间，占用的显存会大幅增加，尤其是对于参数规模较大的模型。比如，对于 Vicuna-13B 模型（FP16 推理，其 Transformer layer num=40, embedding size=5120）来说，在 Sequence Length=1024，batch size=8 下，KV 缓存所占显存大小为 2 * 2 * 40 * 5120 * 1024 * 8 = 6.7G。</p>
<h3 id="23-访存瓶颈"><a class="markdownIt-Anchor" href="#23-访存瓶颈"></a> 2.3 访存瓶颈</h3>
<p>因为 Decoding 阶段 Token 逐个处理，使用 KV Cache 之后，上面介绍的 Multi-Head Attention 里的矩阵乘矩阵操作全部降级为矩阵乘向量。</p>
<p>除此之外，Transformer 模型中的另一个关键组件 FFN 中主要也包含两个矩阵乘法操作，但是 Token 之间不会交叉融合，也就是任何一个 Token 都可以独立计算，因此在 Decoding 阶段不用 Cache 之前的结果，但同样会出现矩阵乘矩阵操作降级为矩阵乘向量。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154850851.png" alt="image.png" /></p>
<p>矩阵乘向量操作是明显的访存 bound，而以上操作是 LLM 推理中最主要的部分，这也就导致 LLM 推理是访存 bound 类型。</p>
<p>基于 V100 GPU，FP16 精度，LLM 推理 Prefill 阶段和 Decoding 阶段的 Roofline Model 可以近似表示如下（理论上限），其中</p>
<ul>
<li>
<p>三角表示 Prefill 阶段：假设 Batch size 为 1，Sequence Length 越大，计算强度越大，通常都会位于 Compute Bound 区域。</p>
</li>
<li>
<p>圆表示 Decoding 阶段：Batch size 越大，计算强度越大，理论性能峰值越大，通常都会位于 Memory Bound 区域。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154910705.png" alt="image.png" /></p>
<h3 id="24-decoding-阶段优化"><a class="markdownIt-Anchor" href="#24-decoding-阶段优化"></a> 2.4 Decoding 阶段优化</h3>
<p>如下图 Figure 4 所示，Prefill 阶段在比较小 Batch Size 下可以获得比较大的计算强度，相应的吞吐也很高；而 Decoding 阶段需要比较大的 Batch Size 才能获得相对高的计算强度及吞吐（图片来自 [2308.16369] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills ）：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154942664.png" alt="image.png" /></p>
<p>针对 Decoding 阶段计算强度比较低的情况，有两种优化的思路：</p>
<ul>
<li>
<p>在不同请求间 Batching 的方式（Continuous Batching），可以增大 Decoding 阶段的 Batch Size，进而更充分发挥 GPU 算力，但是其无法降低单个请求的 Latency，而且如果用户请求比较少，比如只有单一用户，那么永远无法实现 Batching。</p>
</li>
<li>
<p>在单一请求内一次验证多个 Decoding Step（Speculative Decoding），虽然一次 Decoding Step 计算量线性增加，但由于是访存瓶颈，因此计算时间不会明显增加，如果 Decoding Step 数量减少的比例更多，那么就很有可能降低整个请求的时延。</p>
</li>
</ul>
<p>如下图所示为 Batch size 为 1 和 512 时 LLM 中几个主要 OP 的计算耗时，可以看出，将 Batch size 从 1 增加到 512，计算量增加 512 倍，但是其整体时间只增加为原来的 3 倍左右（图片来自 openppl-public · GitHub），如果平均每次验证通过 5 个 Token，那么总的 Decoding Step 数目将降为 1/5，总的 Decoding 时间变为原来的 3/5（忽略其他部分开销），那么即可以实现单一请求的加速：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154949627.png" alt="image.png" /></p>
<p>实际的生成环境中可能要综合考虑上述的两种优化思路，比如如果不同用户间 Continuous Batching 已经达到比较大的 Batch Size，各种矩阵运算接近或超过 Roofline 的交叉点，那么再使用 Speculative Decoding 并不会有什么收益，这一点也是当前提到投机采样最容易被忽略的。</p>
<h1 id="三-并行解码parallel-decoding"><a class="markdownIt-Anchor" href="#三-并行解码parallel-decoding"></a> 三、并行解码（Parallel Decoding）</h1>
<h3 id="31-方案概述"><a class="markdownIt-Anchor" href="#31-方案概述"></a> 3.1 方案概述</h3>
<p>Parallel Decoding 的第一个工作是 2018 发表在 NIPS 上的 Blockwise Parallel Decoding for Deep Autoregressive Models。作者提出分块并行解码（Blockwise Parallel Decoding）方案，在精度不变的情况下可以获得 2x 加速，在牺牲少量精度的情况下可以获得 7x 加速。</p>
<p>假设输出序列的长度为 m，那么 Autoregressive Decoding 要执行 m 步才能获得最终结果，随着模型的增大，每一步的时延也会增大，整体时延也会放大至少 m 倍，在 Blockwise Parallel Decoding 中，作者期望通过 l 步就可完成整个预测，其中 l 远小于 m。</p>
<h3 id="32-实现细节"><a class="markdownIt-Anchor" href="#32-实现细节"></a> 3.2 实现细节</h3>
<p>具体方案如下图所示，分为三步：</p>
<ol>
<li>Predict 阶段</li>
</ol>
<p>令 p1=p（p 表示原始模型），然后额外训练一系列的辅助模型 p2，…，pk，其中 pi(yj+i|y&lt;=j,x)，也就是说，第 i 个模型是根据之前 1-j 个输出生成第 j + i 个输出。比如，当前已经生成了 I、saw、a、dog、ride 5 个 Token，后续要生产 in、the、bus，那么</p>
<ul>
<li>
<p>第 1 个模型（原始模型）负责生成 in 所在位置的 Token</p>
</li>
<li>
<p>第 2 个模型负责生成 the 所在位置的 Token</p>
</li>
<li>
<p>第 3 个模型负责生成 bus 所在位置的 Token</p>
</li>
</ul>
<ol>
<li>Verify 阶段</li>
</ol>
<p>上一步得到了 K 个新的输出，但是还无法知道这 K 个输出是否全部正确，因此需要进一步验证。分别将前 j-1 个新的 Token 与输入合并，并预测下一个 Token，如果下一个 Token 与第 j 个 Token 相同，则接受这个 Token。例如，上一步得到了 in，the，car，则需使用模型 p1（原始模型）分三组验证（这三组可以并行执行）：</p>
<ul>
<li>
<p>第一组：输入 I saw a dog ride，待验证 Token 为 <strong>in</strong>，实际预测的 Top1 为 <strong>in</strong>，<strong>结果相同</strong>，接受 in 这个 Token</p>
</li>
<li>
<p>第二组：输入 I saw a dog ride in，待验证 Token 为 <strong>the</strong>，实际预测的 Top1 为 <strong>the</strong>，<strong>结果相同</strong>，接受 <strong>the</strong> 这个 Token</p>
</li>
<li>
<p>第三组：输入 I saw a dog ride in the，待验证 Token 为 <strong>car</strong>，实际预测的 Top1 为 <strong>bus</strong>，<strong>结果不相同</strong>，不接受 car 这个 Token</p>
</li>
</ul>
<ol>
<li>Accept 阶段</li>
</ol>
<p>假设上一步生成了 10 个 Token，在第 5 个 Token 出不一致，那么就可以将前 4 个 Token 和输入合并，然后开始下一次生成。还是用前两步的例子，因为第三组 car 和 bus 不一致，因此只接受 in 和 the，则下一次迭代的输入为 I saw a dog ride in the。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126154959329.png" alt="image.png" /></p>
<p>如下图 Figure 3 所示为模型的实现方式，在模型的最后一个 Transformer Decoder 层额外的加几个 head，分别为 p2，…，pk：</p>
<ul>
<li>
<p>在 Blockwise Parallel Decoding 的 Predict 阶段，原始模型 p1 和辅助模型 p2，…，pk 都是相互独立的，可以并行执行，因此时间和生成一个 Token 时间一致。</p>
</li>
<li>
<p>在 Blockwise Parallel Decoding 的 Verify 阶段，需要上一步中生成的 K 个 Token 里选择符合要求的最长前缀，因为可以一次生成多个 Token（&lt;=K），所以可降低整体生成的步数，也就帮助降低整体时延。</p>
</li>
<li>
<p>在 Blockwise Parallel Decoding 的 Accept 阶段，因为只接受第一个不一致的 Token 之前的 Token，并且验证时使用的就是原始模型 p1 ，这也就保证了最终结果是与原始序列预测的结果是完全一致的（与 Greedy Decoding 结果完全一致）。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155010317.png" alt="image.png" /></p>
<p>并行验证过程如下所示，经过一次前向推理即可验证获得新的 <strong>in</strong>、<strong>the</strong>、<strong>car</strong> 三个 Token：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155016265.png" alt="image.png" /></p>
<p>综上所述，由于 Predict 阶段 p1 和 Verify 阶段都使用的原始模型，因此在理想情况下（每次生成的 K 个 Token 都能接受），总的解码次数从 m 降低到 2m/K。</p>
<h3 id="33-评估结果"><a class="markdownIt-Anchor" href="#33-评估结果"></a> 3.3. 评估结果</h3>
<p>作者使用 WMT 2014 的英语-德语翻译数据集。Baseline 模型是在 8 个 P100 GPU 上训练了 1,000,000 steps 的 Transformer 模型。使用 Greddy Decoding，在 newstest2023 development 数据集上 BLEU 得分为 25.56.</p>
<p>作者针对不同的猜测 Token 数，使用原始数据或者 Beam Search 生成的蒸馏数据分别进行训练，每个模型都在相同的硬件额外训练 1,000,000 steps。基于上述模型，作者统计了相应的 BLEU 得分和对应的平均接受 Token 数，如下图所示：</p>
<ul>
<li>
<p>Regular ●：表示冻结 backbone，使用原始数据，对应的平均接受 Token 数很小，最大只有 1.76</p>
</li>
<li>
<p>Distillation ◼：表示冻结 backbone，使用蒸馏数据，对应的平均接受 Token 数很小，最大只有 1.91，BLEU 得分也相应提高</p>
</li>
<li>
<p>Fine Tuning ▲：表示不冻结 backbone，使用原始数据，对应的平均接受 Token 数相对增大，最大为 3.01</p>
</li>
<li>
<p>Both ◆：表示不冻结 backbone，使用蒸馏数据，对应的平均接受 Token 数明显增大，最大有 4.95，BLEU 得分相应提高</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155022384.png" alt="image.png" /></p>
<p>对应上述的机器翻译任务，当 k=8 时，对应的平均接受 Token 数为 4.7，相应的整个任务的加速比达到 3.3 倍。如下图所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155027608.png" alt="image.png" /></p>
<h1 id="四-投机解码speculative-decoding"><a class="markdownIt-Anchor" href="#四-投机解码speculative-decoding"></a> 四、投机解码（Speculative Decoding）</h1>
<h3 id="41-方案概述"><a class="markdownIt-Anchor" href="#41-方案概述"></a> 4.1. 方案概述</h3>
<p>Google 和 Deepmind 于 2022 年提出投机采样方案 Fast Inference from Transformers via Speculative Decoding，其思路很简单，使用一个高效的小模型来生成多个候选 Token，然后让 LLM 来验证。</p>
<p>其方法可以概括为由一个小模型一次猜一批可能的结果，再由大模型并行地验证这些结果是否要接受。 投机解码算法的提出，主要源于两点观察：</p>
<ul>
<li>
<p>和early exit的想法类似，在一些相对简单的问题下，我们可以用小模型（或者大模型的前面几层）的输出得到很好的结果。 如果我们用小模型去回答这些简单的问题，在遇到难题的情况下再调用大模型，就可以整体的生成效率。</p>
</li>
<li>
<p>大模型在做推理任务的时候，一次只能生成一个token，无法并行计算。如果我们能让大模型一次处理一批tokens，就能利用上算例的并行能力。（大模型推理的时候batch size往往为1）</p>
</li>
</ul>
<p>投机解码利用了上面两个观察，先用小模型猜后续的若干个tokens，如果当前的问题比较简单，则小模型有更大的可能猜对多个token。 然后再用大模型并行的验证这一些token是否符合大模型的输出。由于现代计算机的并行能力，我们可以近似的认为大模型处理一个token和处理w个token的用时是几乎一样的。 假设我们一次猜n个tokens，平均有m个token会被最终接收，那么在这个过程中： 我们调用了n次小模型D，1次大模型T，生成了m个token，平均每个token的用时为 (nD+T)/m。只要nD显著地小于(m-1)T，就能实现很好的加速效果。</p>
<p>对于大模型来说，decoding的时候有几种方案：</p>
<ul>
<li>
<p>Greedy：每次选择logit最大的token</p>
</li>
<li>
<p>归一化logits后按照分布采样</p>
</li>
<li>
<p>Top k： 保留最大的k个token</p>
</li>
<li>
<p>Top p： 从大到小保留概率分布和为p的token</p>
</li>
<li>
<p>（top k/top p）+（greedy/sampling）</p>
</li>
</ul>
<p>投机是一个加速推理的技术，为了保证这样得到的结果performance不下降，这一系列工作认为只要保证最后的概率分布一样即可。因此，只需要大模型验证的方法能保证整个过程输出的结果的概率分布不变。</p>
<p>验证操作弥补了小模型和大模型之间的概率分布的gap，思路是对于小模型的每一次猜测，根据大模型和小模型的概率分布去判断这一次猜测有多大概率是正确的。相当于是从小模型的采样到大模型的采样之间做了一个映射，可以把小模型和大模型的概率分别看成若干个随机事件，然后将小模型的随机事件和大模型的随机时间做映射，如果两边的随机事件的结果一致，我们就认为这个猜测是正确的。</p>
<p>如果在某一部中我们认为小模型的猜测是错误的，那么后面的结果都是无效的。此时用大模型最后一步得到的概率分布做一个采样后退出。这一步既是保证输出同分布必须的，又可以保证每次至少输出一个token。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155037577.png" alt="image.png" /></p>
<h3 id="42-实现细节"><a class="markdownIt-Anchor" href="#42-实现细节"></a> 4.2 实现细节</h3>
<p>假设 Mp 为目标模型，模型推理就是给定前缀输入 x&lt;t，从模型获得对应的分布 p(xt|x&lt;t)，要做的就是加速这个推理过程；假设 Mq 为针对相同任务的更高效的近似模型，给定前缀输入 x&lt;t，从模型可以获得对应的分布 q(xt|x&lt;t)。核心思想为：</p>
<ol>
<li>
<p>使用更高效的模型 Mq 来生成输出 ?∈ℤ+ 个 Token</p>
</li>
<li>
<p>使用目标模型 Mp 来并行的评估上一步 Mq 生成的 Token，接受能够满足同一分布的 Token</p>
</li>
<li>
<p>从调整后的分布中生成一个额外的 Token（根据第一个出错 Token 之前的 Token 生成），来修复第一个出错的 Token，如果所有 Token 都被接受，则额外新增一个新生成的 Token，以此来保证每次至少生成一个新的 Token。这样，即使在最坏情况下，目标模型相当于完全串行运行，运行次数也不会超过常规模式直接串行运行目标模型的次数；当然，也很可能能够生成更多的 Token，最多可以达到 ?+1，这取决于Mp 和 Mq 的相似度。</p>
</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155045187.png" alt="image.png" /></p>
<p>如下图 Figure 5 所示，作者提供了一个简单的示例，包含不同的 ?（验证的 Token 数目），其中紫色为执行目标模型 Mp 的 decoder，蓝色为执行近似模型 Mq 的 decoder，黄色和橙色为调用 encoder。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155058820.png" alt="image.png" /></p>
<h3 id="43-评估结果"><a class="markdownIt-Anchor" href="#43-评估结果"></a> 4.3. 评估结果</h3>
<p>作者基于 T5X 代码库验证了 T5-XXL 模型的加速效果。相关的设置如下：</p>
<ul>
<li>模型：标准的 encoder-decoder T5 1.1 版本模型</li>
</ul>
<ol>
<li>
<p>目标模型 Mp：T5-XXL（11B）</p>
</li>
<li>
<p>近似模型 Mq：T5-Large（800M），T5-Base（250M），T5-Small（75M）</p>
</li>
</ol>
<ul>
<li>任务：</li>
</ul>
<ol>
<li>
<ol>
<li>
<p>英语到德语翻译，基于 WMT EnDe 数据集微调</p>
</li>
<li>
<p>文本总结，基于 CCN/DM 数据集微调</p>
</li>
</ol>
</li>
</ol>
<ul>
<li>
<p>硬件：TPU-v4</p>
</li>
<li>
<p>推理参数：</p>
</li>
</ul>
<ol>
<li>
<ol>
<li>
<p>Batch-size：1</p>
</li>
<li>
<p>Argmax sampling（temp=0）和 standard sampling（temp=1）</p>
</li>
</ol>
</li>
</ol>
<p>结果如下 Table 2 所示，最小的近似模型 T5-Small（75M）获得最高的加速比（模型很小，推理最快，而且模型生成质量相比 Base 模型没有下降太多，? 表示高效模型的质量与目标模型的接近程度），比如 T5-Small 在 EnDe 任务上，当 temp=0 时获得 3.4 倍加速，temp=1 时获得 2.6 倍加速：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155108487.png" alt="image.png" /></p>
<p>如下所示为 Huggingface 官方的测试结果（Assisted Generation: a new direction toward low-latency text generation）：</p>
<ul>
<li>
<ul>
<li>
<p>Assistant Model</p>
<pre><code>- facebook/opt-125m
</code></pre>
<ul>
<li>
<p>Model Names:</p>
<ul>
<li>
<p>1.3B: facebook/opt-1.3b</p>
</li>
<li>
<p>6.7B: facebook/opt-6.7b</p>
</li>
<li>
<p>30B: facebook/opt-30b</p>
</li>
<li>
<p>66B: facebook/opt-66b</p>
</li>
</ul>
</li>
<li>
<p>Dataset used as input prompt:</p>
<ul>
<li>C4 (en, validation set)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126155120150.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>llama.cpp</title>
    <url>/2025/01/26/llama-cpp/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://zhuanlan.zhihu.com/p/996110863">https://zhuanlan.zhihu.com/p/996110863</a></p>
</blockquote>
<p>原文： <a href="https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/">Understanding how LLM inference works with llama.cpp</a></p>
<p><em>译者的话: llama.cpp出道以来，很少有官方文档，但是本文通过代码驱动的讲解， 讲清楚了llama.cpp的原理，个人推荐一读。</em></p>
<p>在这篇文章中，我们将深入探讨大型语言模型（LLMs）的内部结构，以便更好地理解它们是如何工作的。为帮助我们进行这次探索，我们将使用 llama.cpp 的源码，它是 Meta 的 LLaMA 模型的纯 C++ 实现。作者个人认为，llama.cpp 是理解 LLM 深层原理的一个优秀学习工具，它的代码简洁明了，不涉及过多的抽象。我们将使用特定的提交版本。</p>
<p>本文的重点是 LLM 的推理部分，即：已训练好的模型如何基于用户输入的提示生成响应。这篇文章主要写给那些非机器学习和人工智能领域的工程师，旨在帮助他们更好地理解 LLM，<strong>本文从工程角度而非 AI 角度探讨 LLM 的内部工作原理，因此不要求读者具备深厚的数学或深度学习知识</strong>。（<em>译者：这正是本文最妙的地方</em>）在文章中，我们将从头到尾介绍 LLM 的推理过程，涵盖以下主题：</p>
<ol>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/%23understanding-tensors-with-ggml">张量</a>：概述数学运算如何以张量的形式实现， 并可能潜在转移到 GPU 上处理。</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/%23tokenization">分词</a>：将用户输入的提示分解为令牌列表，LLM 使用这些令牌作为输入。</p>
</li>
<li>
<p><strong>嵌入Embedding：将令牌转换为高维向量的过程。</strong></p>
</li>
<li>
<p>Transformer：大语言模型架构的核心部分，负责实际的推理过程，我们将重点介绍自注意力机制。</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/%23sampling">采样</a>：选择下一个预测令牌的过程，我们将探讨两种采样技术。</p>
</li>
<li>
<p><a href="https://link.zhihu.com/?target=https%3A//www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/%23optimizing-inference">KV 缓存</a>：一种常见的优化技术，用于加快长提示的推理速度，我们将介绍一个基本的 kv 缓存实现。</p>
</li>
</ol>
<p>通过阅读本文，你将有望对 LLM 的工作过程有一个端到端的理解，并且能够探索更高级的主题，这些主题将在最后一节中详细说明。</p>
<span id="more"></span>
<h2 id="从提示到输出的高级流程"><a class="markdownIt-Anchor" href="#从提示到输出的高级流程"></a> 从提示到输出的高级流程</h2>
<p>作为一个大型语言模型（LLM），LLaMA 的工作原理是接收一个输入文本（即“提示”），并预测下一个应该生成的标记（token）或词汇。</p>
<p>为了说明这个过程，我们以维基百科量子力学条目中的第一句话为例。我们的提示是：</p>
<blockquote>
<p><strong>Quantum mechanics is a fundamental theory in physics that</strong></p>
</blockquote>
<p>LLM 会尝试根据训练时学到的知识继续这句话。使用 llama.cpp，我们得到如下的续写：</p>
<blockquote>
<p><strong>provides insights into how matter and energy behave at the atomic scale.</strong></p>
</blockquote>
<p>让我们先来看一下这个过程的高级流程。LLM 的核心功能是每次只预测一个标记。生成完整的句子（或更多内容）是通过反复应用 LLM 模型到相同的提示上，并将之前的输出标记附加到提示后形成的。这种模型被称为自回归模型。因此，我们主要关注单个标记的生成，流程可以简化为以下高级图所示：</p>
<p><em>LLM 通过每次迭代生成一个标记，然后将其添加到输入提示中，不断重复该过程，直到生成完整的输出。这就是 LLM 如何从输入提示生成文本的基础。</em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170732132.png" alt="image.png" /></p>
<p>从用户提示生成单个标记的完整流程包括多个阶段，如分词、嵌入、Transformer 神经网络和采样。本文将介绍这些阶段。</p>
<p>根据图示，整个流程如下：</p>
<ol>
<li>
<p><strong>分词</strong>：分词器将提示分解为一个标记列表。根据模型的词汇表，某些单词可能会被分解成多个标记。每个标记由一个唯一的数字表示。</p>
</li>
<li>
<p><strong>嵌入embedding转换</strong>：每个数字标记被转换为一个嵌入向量。嵌入是一个固定大小的向量，以一种更适合 LLM 处理的方式表示标记。所有嵌入向量组合在一起形成嵌入矩阵。</p>
</li>
<li>
<p><strong>输入 Transformer</strong>：嵌入矩阵作为 Transformer 的输入。Transformer 是 LLM 的核心神经网络，由多层链组成。每一层接收输入矩阵，并利用模型参数执行各种数学运算，最主要的是自注意力机制。该层的输出作为下一层的输入。</p>
</li>
<li>
<p><strong>logits 生成</strong>：最后的神经网络将 Transformer 的输出转换为 logits。每个可能的下一个标记都有一个相应的 logits，表示该标记作为句子“正确”延续的概率。</p>
</li>
<li>
<p><strong>采样</strong>：使用多种采样技术之一，从 logits 列表中选择下一个标记。</p>
</li>
<li>
<p><strong>生成输出</strong>：所选标记作为输出返回。要继续生成更多的标记，所选标记会被附加到第 1 步的标记列表中，然后重复该过程。这可以一直进行，直到生成所需数量的标记，或者 LLM 发出特殊的结束流（EOS）标记。</p>
</li>
</ol>
<p>接下来的部分将详细探讨这些步骤。但在此之前，我们需要熟悉张量的概念。</p>
<h2 id="理解张量及其在-ggml-中的应用"><a class="markdownIt-Anchor" href="#理解张量及其在-ggml-中的应用"></a> 理解张量及其在 ggml 中的应用</h2>
<p>张量是神经网络中执行数学运算的主要数据结构。<strong>llama.cpp</strong> 使用的是 <strong>ggml</strong>，这是一种纯 C++ 实现的张量库，相当于 Python 生态系统中的 <strong>PyTorch</strong> 或 <strong>TensorFlow</strong>。我们将通过 ggml 来理解张量是如何操作的。</p>
<p>张量可以表示一个多维数组的数值。它可能包含一个单一的数值（标量）、一个向量（一维数组）、一个矩阵（二维数组）甚至是三维或四维数组。通常，实际应用中不需要使用更多维度。</p>
<p>理解两种类型的张量是非常重要的：</p>
<ol>
<li>
<p><strong>数据张量</strong>：这些张量持有实际数据，包含一个多维数组的数值。</p>
</li>
<li>
<p><strong>运算张量</strong>：这些张量仅表示一个或多个其他张量之间运算的结果，只有在实际计算时才会包含数据。</p>
</li>
</ol>
<p>我们接下来将详细探讨这两类张量之间的区别。</p>
<h3 id="张量的基本结构"><a class="markdownIt-Anchor" href="#张量的基本结构"></a> 张量的基本结构</h3>
<p>在 <strong>ggml</strong> 中，张量由 <code>ggml_tensor</code> 结构体表示。为便于理解，我们稍微简化了一下它的结构，简化后的样子如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ggml.h</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> &#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">ggml_type</span>    type;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">ggml_backend</span> backend;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>     n_dims; <span class="comment">//张量的维度数量，例如一维向量、二维矩阵等</span></span><br><span class="line">    <span class="comment">// number of elements</span></span><br><span class="line">    <span class="type">int64_t</span> ne[GGML_MAX_DIMS];</span><br><span class="line">    <span class="comment">// stride in bytes</span></span><br><span class="line">    <span class="type">size_t</span>  nb[GGML_MAX_DIMS];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">ggml_op</span> op; <span class="comment">// 表示张量是哪个操作的结果（例如加法、乘法等）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * src[GGML_MAX_SRC];<span class="comment">// 张量的输入源（如果它是计算结果）</span></span><br><span class="line"></span><br><span class="line">    <span class="type">void</span> * data; <span class="comment">//指向实际数据的指针，可能是 NULL，如果该张量仅代表一个操作的结果</span></span><br><span class="line"></span><br><span class="line">    <span class="type">char</span> name[GGML_MAX_NAME];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>前几个字段比较容易理解：</p>
<ul>
<li>
<p><strong>type</strong>：包含张量元素的基本类型。例如，<code>GGML_TYPE_F32</code> 表示每个元素是一个 32 位浮点数， 也可以是F16或者其他整形量化。</p>
</li>
<li>
<p><strong>ggml_backend</strong>：指示张量是基于 CPU 还是基于 GPU 存储的。我们稍后会讨论这一点。</p>
</li>
<li>
<p><strong>n_dims</strong>：张量的维度数量，可以是 1 到 4 维。</p>
</li>
<li>
<p><strong>ne</strong>：表示每个维度中的元素数量。ggml 采用行优先顺序，意味着 <code>ne[0]</code> 表示每行的大小，<code>ne[1]</code> 表示每列的大小，依此类推。</p>
</li>
<li>
<p><strong>nb</strong>：这个字段稍微复杂一些，它包含步长信息，即每个维度中连续元素之间的字节数。在第一个维度中，步长等于元素的大小；在第二个维度中，它等于每行的大小乘以元素的大小，以此类推。</p>
<ul>
<li>例如，对于一个 4x3x2 的张量：</li>
</ul>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170811556.png" alt="image.png" /></p>
<p>一个 32 位浮点数张量的例子，维度为 {4, 3, 2}，步长为 {4, 16, 48}。</p>
<p>使用步长的目的是为了在进行某些张量操作时无需复制任何数据。例如，在二维张量上执行转置操作，将行转换为列时，只需要交换 <code>ne</code>（维度大小）和 <code>nb</code>（步长），而指向相同的底层数据即可实现这个操作，无需对数据本身进行复制。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ggml.c (the function was slightly simplified).</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * <span class="built_in">ggml_transpose</span>(</span><br><span class="line">        <span class="keyword">struct</span> ggml_context * ctx,</span><br><span class="line">        <span class="keyword">struct</span> ggml_tensor  * a) &#123;</span><br><span class="line">    <span class="comment">// Initialize `result` to point to the same data as `a`</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * result = <span class="built_in">ggml_view_tensor</span>(ctx, a);</span><br><span class="line"></span><br><span class="line">    result-&gt;ne[<span class="number">0</span>] = a-&gt;ne[<span class="number">1</span>];</span><br><span class="line">    result-&gt;ne[<span class="number">1</span>] = a-&gt;ne[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    result-&gt;nb[<span class="number">0</span>] = a-&gt;nb[<span class="number">1</span>];</span><br><span class="line">    result-&gt;nb[<span class="number">1</span>] = a-&gt;nb[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    result-&gt;op   = GGML_OP_TRANSPOSE;</span><br><span class="line">    result-&gt;src[<span class="number">0</span>] = a;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述函数中，<code>result</code> 是一个新张量，它被初始化为指向与源张量 <code>a</code> 相同的多维数值数组。通过交换 <code>ne</code>（维度大小）和 <code>nb</code>（步长），可以执行转置操作，而无需复制任何数据。</p>
<p><em>译者注：这里ggml_view_tensor和GGML_OP_TRANSPOSE发挥了重要作用，</em> <em><strong>ggml_view_tensor</strong>__：</em> <code>_ggml_view_tensor_</code><em>函数创建了一个新的张量</em><code>_result_</code><em>，这个张量指向原始张量</em><code>_a_</code><em>的相同数据。这意味着</em><code>_result_</code><em>和</em><code>_a_</code><em>共享相同的内存空间，但它们的维度和步长可以不同。将</em> <code>_result-&gt;op_</code> <em>设置为</em> <code>_GGML_OP_TRANSPOSE_</code> <em>之后，</em><code>_ggml_</code> <em>系统知道这个张量是通过转置另一个张量得到的，而不是一个直接包含数据的张量。这个标记在后续的计算中很重要，因为</em> <code>_ggml_</code> <em>在需要计算时会按照这个操作类型来执行相应的计算逻辑。这在后面会马上讲到。</em></p>
<h3 id="张量操作与视图"><a class="markdownIt-Anchor" href="#张量操作与视图"></a> 张量操作与视图</h3>
<p>正如之前提到的，有些张量包含实际数据，而另一些张量则表示其他张量之间运算的理论结果。回到 <code>ggml_tensor</code> 结构体：</p>
<ul>
<li>
<p><code>**op**</code>：可以是张量之间支持的任何操作。如果设置为 <code>GGML_OP_NONE</code>，则表示张量包含数据。其他值表示不同的操作。例如，<code>GGML_OP_MUL_MAT</code> 表示该张量不包含数据，而是表示两个其他张量之间矩阵乘法的结果。</p>
</li>
<li>
<p><code>**src**</code>：这是一个指向要进行运算的张量的指针数组。例如，如果 <code>op == GGML_OP_MUL_MAT</code>，那么 <code>src</code> 将包含指向两个要相乘的张量的指针。如果 <code>op == GGML_OP_NONE</code>，则 <code>src</code> 为空。</p>
</li>
<li>
<p><code>**data**</code>：指向实际张量数据的指针，如果该张量表示一个操作，则为 <code>NULL</code>。它也可能指向另一个张量的数据，在这种情况下，它被称为视图。例如，在上面的 <code>ggml_transpose()</code> 函数中，结果张量就是原始张量的视图，只是维度和步长被交换了。<code>data</code> 指向相同的内存位置。</p>
</li>
</ul>
<p>矩阵乘法函数很好地展示了这些概念：通过指向相同的数据并修改维度和步长，张量可以通过视图避免数据复制。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ggml.c (simplified and commented)</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * <span class="built_in">ggml_mul_mat</span>(</span><br><span class="line">        <span class="keyword">struct</span> ggml_context * ctx,</span><br><span class="line">        <span class="keyword">struct</span> ggml_tensor  * a,</span><br><span class="line">        <span class="keyword">struct</span> ggml_tensor  * b) &#123;</span><br><span class="line">    <span class="comment">// Check that the tensors&#x27; dimensions permit matrix multiplication.</span></span><br><span class="line">    <span class="built_in">GGML_ASSERT</span>(<span class="built_in">ggml_can_mul_mat</span>(a, b));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set the new tensor&#x27;s dimensions</span></span><br><span class="line">    <span class="comment">// according to matrix multiplication rules.</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int64_t</span> ne[<span class="number">4</span>] = &#123; a-&gt;ne[<span class="number">1</span>], b-&gt;ne[<span class="number">1</span>], b-&gt;ne[<span class="number">2</span>], b-&gt;ne[<span class="number">3</span>] &#125;;</span><br><span class="line">    <span class="comment">// Allocate a new ggml_tensor.</span></span><br><span class="line">    <span class="comment">// No data is actually allocated except the wrapper struct.</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * result = <span class="built_in">ggml_new_tensor</span>(ctx, GGML_TYPE_F32, <span class="built_in">MAX</span>(a-&gt;n_dims, b-&gt;n_dims), ne);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set the operation and sources.</span></span><br><span class="line">    result-&gt;op   = GGML_OP_MUL_MAT;</span><br><span class="line">    result-&gt;src[<span class="number">0</span>] = a;</span><br><span class="line">    result-&gt;src[<span class="number">1</span>] = b;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述函数中，<code>result</code> 不包含任何数据。它只是表示矩阵 <code>a</code> 和 <code>b</code> 相乘后的理论结果。</p>
<h3 id="计算张量"><a class="markdownIt-Anchor" href="#计算张量"></a> 计算张量</h3>
<p>上面的 <code>ggml_mul_mat()</code> 函数或其他任何张量操作，都不会立即进行计算，它只是为操作准备好张量。换一种方式理解，它是在构建一个计算图，其中每个张量操作都是一个节点，操作的来源是该节点的子节点。在矩阵乘法的情况下，计算图会有一个父节点，其操作为 <code>GGML_OP_MUL_MAT</code>，同时有两个子节点。</p>
<p>在 <code>llama.cpp</code> 中的一个实际例子中，下面的代码实现了自注意力机制，这是每个 Transformer 层的一部分，后续会对此进行更深入的探讨：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">ggml_cgraph</span> * <span class="built_in">llm_build_llama</span>(<span class="comment">/* ... */</span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// K,Q,V are tensors initialized earlier</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ = <span class="built_in">ggml_mul_mat</span>(ctx0, K, Q);</span><br><span class="line">    <span class="comment">// KQ_scale is a single-number tensor initialized earlier.</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_scaled = <span class="built_in">ggml_scale_inplace</span>(ctx0, KQ, KQ_scale);</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_masked = <span class="built_in">ggml_diag_mask_inf_inplace</span>(ctx0, KQ_scaled, n_past);</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_soft_max = <span class="built_in">ggml_soft_max_inplace</span>(ctx0, KQ_masked);</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQV = <span class="built_in">ggml_mul_mat</span>(ctx0, V, KQ_soft_max);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码是一系列张量操作，并构建了一个计算图，与原始 Transformer 论文中描述的计算图完全一致。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170856916.png" alt="image.png" /></p>
<p>要实际计算结果张量（这里是 KQV），需要执行以下步骤：</p>
<ol>
<li>
<p><strong>加载数据</strong>：数据被加载到每个叶子张量的 <code>data</code> 指针中。在这个例子中，叶子张量是 K、Q 和 V。</p>
</li>
<li>
<p><strong>构建计算图</strong>：使用 <code>ggml_build_forward()</code> 函数将输出张量（KQV）转换为计算图。这个函数比较简单，以深度优先顺序排列节点。</p>
</li>
<li>
<p><strong>运行计算图</strong>：通过 <code>ggml_graph_compute()</code> 运行计算图，该函数对每个节点执行 <code>ggml_compute_forward()</code> 操作，按深度优先顺序计算。<code>ggml_compute_forward()</code> 负责主要的数学计算，完成数学运算并将结果填充到张量的 <code>data</code> 指针中。</p>
</li>
<li>
<p><strong>结果输出</strong>：在这个过程结束时，输出张量的 <code>data</code> 指针指向最终计算结果。</p>
</li>
</ol>
<h3 id="将计算任务转移到-gpu"><a class="markdownIt-Anchor" href="#将计算任务转移到-gpu"></a> 将计算任务转移到 GPU</h3>
<p>由于 GPU 的高度并行性，许多张量操作（如矩阵加法和乘法）可以在 GPU 上更高效地完成。当 GPU 可用时，可以将张量标记为 <code>tensor-&gt;backend = GGML_BACKEND_GPU</code>。在这种情况下<code>ggml_compute_forward()</code> 会尝试将计算任务转移到 GPU 进行。GPU 会执行张量操作，并将结果存储在 GPU 的内存中（而不是张量的 <code>data</code> 指针中）。</p>
<p>例如，在之前的自注意力计算图中，假设 K、Q、V 是固定的张量，计算可以转移到 GPU 上完成。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170908379.png" alt="image.png" /></p>
<p>这个过程首先将 K、Q、V 复制到 GPU 内存中。然后由 CPU 按照张量逐个驱动计算，但实际的数学运算会被转移到 GPU 进行。当计算图中的最后一个操作完成时，结果张量的数据会从 GPU 内存复制回 CPU 内存。</p>
<p><strong>注意</strong>：在实际的 Transformer 中，K、Q、V 并不是固定的，KQV 也不是最终的输出。后面我们将对此进行详细说明。</p>
<p>在理解了张量的工作机制之后，我们可以回到 LLaMA 的流程。</p>
<h2 id="分词tokenization"><a class="markdownIt-Anchor" href="#分词tokenization"></a> 分词Tokenization</h2>
<p>推理的第一步是分词。分词是将提示（prompt）拆分为称为“词元”的较短字符串列表的过程。词元必须是模型词汇表的一部分，词汇表是LLM（大型语言模型）在训练时使用的词元列表。例如，LLaMA的词汇表由32,000个词元组成，随模型一同分发。</p>
<p>对于我们的示例提示，分词将提示拆分为11个词元（空格被替换为特殊的元符号‘▁’ (U+2581)）：</p>
<blockquote>
<p>|Quant|um|▁mechan|ics|▁is|▁a|▁fundamental|▁theory|▁in|▁physics|▁that|</p>
</blockquote>
<p>在分词过程中，LLaMA使用了基于字节对编码（BPE）算法的SentencePiece分词器。这种分词器非常有趣，因为它是基于子词的，这意味着一个词可能由多个词元表示。例如，在我们的提示中，‘Quantum’被拆分为‘Quant’和‘um’。在训练过程中，词汇表的生成通过BPE算法保证常用词作为单个词元包含在词汇表中，而罕见词则被分解为子词。在上面的示例中，单词‘Quantum’不在词汇表中，但‘Quant’和‘um’作为两个独立的词元存在。空格不会被特殊处理，它们如果足够常见，也会作为元字符包含在词元中。</p>
<p>基于子词的分词具有多种优势：</p>
<p>它允许LLM学习像‘Quantum’这样的罕见词的含义，同时通过将常见的后缀和前缀表示为独立词元，保持词汇表的相对小型化。 它无需使用语言特定的分词方案即可学习语言特定的特性。引用BPE编码论文中的例子： 考虑德语的复合词如Abwasser|behandlungs|anlange（污水处理厂），分段的、可变长度的表示形式比将该词编码为固定长度的向量更加直观。</p>
<p>同样，这种分词方式在解析代码时也非常有用。例如，一个名为model_size的变量将被分词为model|_|size，这使得LLM能够“理解”该变量的用途（这也是为变量赋予有意义名称的另一个原因！）。 在llama.cpp中，分词是通过llama_tokenize()函数完成的。该函数接受提示字符串作为输入，并返回词元列表，其中每个词元由一个整数表示。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.h</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> llama_token;</span><br><span class="line"></span><br><span class="line"><span class="comment">// common.h</span></span><br><span class="line"><span class="function">std::vector&lt;llama_token&gt; <span class="title">llama_tokenize</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="keyword">struct</span> llama_context * ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="comment">// the prompt</span></span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">const</span> std::string &amp; text,</span></span></span><br><span class="line"><span class="params"><span class="function">        <span class="type">bool</span>   add_bos)</span></span>;</span><br></pre></td></tr></table></figure>
<p>分词过程首先将提示拆分为单个字符的词元。接着，它会迭代地尝试将每两个连续的词元合并为一个更大的词元，只要合并后的词元是词汇表的一部分。这样可以确保生成的词元尽可能大。对于我们的示例提示，分词步骤如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Q|u|a|n|t|u|m|▁|m|e|c|h|a|n|i|c|s|▁|i|s|▁a|▁|f|u|n|d|a|m|e|n|t|a|l|</span><br><span class="line"></span><br><span class="line">Qu|an|t|um|▁m|e|ch|an|ic|s|▁|is|▁a|▁f|u|nd|am|en|t|al|</span><br><span class="line"></span><br><span class="line">Qu|ant|um|▁me|chan|ics|▁is|▁a|▁f|und|am|ent|al|</span><br><span class="line"></span><br><span class="line">Quant|um|▁mechan|ics|▁is|▁a|▁fund|ament|al|</span><br><span class="line"></span><br><span class="line">Quant|um|▁mechan|ics|▁is|▁a|▁fund|amental|</span><br><span class="line"></span><br><span class="line">Quant|um|▁mechan|ics|▁is|▁a|▁fundamental|</span><br></pre></td></tr></table></figure>
<p>请注意，每个中间步骤都符合模型词汇表的有效分词规则。然而，只有最后一步会被用作LLM（大型语言模型）的输入。</p>
<h2 id="嵌入embedding"><a class="markdownIt-Anchor" href="#嵌入embedding"></a> 嵌入embedding</h2>
<p>这些词元将作为LLaMA的输入，用于预测下一个词元。此处的关键函数是llm_build_llama()函数：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified)</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">ggml_cgraph</span> * <span class="built_in">llm_build_llama</span>(</span><br><span class="line">         llama_context &amp; lctx,</span><br><span class="line">     <span class="type">const</span> llama_token * tokens,</span><br><span class="line">                   <span class="type">int</span>   n_tokens,</span><br><span class="line">                   <span class="type">int</span>   n_past);</span><br></pre></td></tr></table></figure>
<p>该函数接受由<code>tokens</code>和<code>n_tokens</code>参数表示的词元列表作为输入。然后，它构建LLaMA的完整张量计算图，并将其作为<code>ggml_cgraph</code>结构返回。在此阶段实际上并不会进行任何计算。目前可以忽略<code>n_past</code>参数，它目前设置为零。稍后我们在讨论<code>kv cache</code>时将再次提到它。</p>
<p>除了词元，该函数还使用模型权重或模型参数。这些是LLM（大型语言模型）在训练过程中学习的固定张量，作为模型的一部分包含在内。这些模型参数在推理开始前预先加载到<code>lctx</code>中。</p>
<p>现在我们将开始探索计算图结构。该计算图的第一部分涉及将词元转换为嵌入。嵌入是每个词元的固定向量表示，它比纯整数更适合深度学习，因为它捕捉到了单词的语义意义。该向量的大小是模型维度，不同模型之间有所不同。例如，在LLaMA-7B中，模型维度为<code>n_embd=4096</code>。模型参数包括一个将词元转换为嵌入的词元嵌入矩阵。由于我们的词汇大小为<code>n_vocab=32000</code>，因此这是一个32000 x 4096的矩阵，每一行都包含一个词元的嵌入向量：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126170950263.png" alt="image.png" /></p>
<p>每个词元都有一个在训练过程中学习到的关联嵌入，可以通过词元嵌入矩阵进行访问。</p>
<p>计算图的第一部分从词元嵌入矩阵中提取每个词元的相关行：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified)</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">ggml_cgraph</span> * <span class="built_in">llm_build_llama</span>(<span class="comment">/* ... */</span>) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * inp_tokens = <span class="built_in">ggml_new_tensor_1d</span>(ctx0, GGML_TYPE_I32, n_tokens);</span><br><span class="line">    <span class="built_in">memcpy</span>(</span><br><span class="line">        inp_tokens-&gt;data,</span><br><span class="line">        tokens,</span><br><span class="line">        n_tokens * <span class="built_in">ggml_element_size</span>(inp_tokens));</span><br><span class="line"></span><br><span class="line">    inpL = <span class="built_in">ggml_get_rows</span>(ctx0, model.tok_embeddings, inp_tokens);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//</span></span><br></pre></td></tr></table></figure>
<p>代码首先创建一个名为<code>inp_tokens</code>的新的一维整数张量，用于存储数值化的词元。接着，它将词元值复制到该张量的数据指针中。最后，它创建了一个新的<code>GGML_OP_GET_ROWS</code>张量操作，将词元嵌入矩阵<code>model.tok_embeddings</code>与我们的词元组合起来。</p>
<p>当稍后计算该操作时，它将从嵌入矩阵中提取相应的行，如上图所示，创建一个新的<code>n_tokens x n_embd</code>矩阵，仅包含按原始顺序排列的词元嵌入：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171005906.png" alt="image.png" /></p>
<p>嵌入过程为每个原始词元创建一个固定大小的嵌入向量。当这些向量堆叠在一起时，它们构成了提示（prompt）的嵌入矩阵。</p>
<h2 id="transformer"><a class="markdownIt-Anchor" href="#transformer"></a> Transformer</h2>
<p>计算图的主要部分被称为Transformer。Transformer是一种神经网络架构，是大型语言模型（LLM）的核心，负责执行主要的推理逻辑。在接下来的部分中，我们将从工程角度探讨Transformer的一些关键方面，重点关注自注意力机制。如果你想对Transformer架构有直观的了解，我建议阅读《<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>》。</p>
<h3 id="自注意力机制"><a class="markdownIt-Anchor" href="#自注意力机制"></a> 自注意力机制</h3>
<p>我们首先深入了解下什么是自注意力机制，然后再回顾它在整体Transformer架构中的作用。</p>
<p>自注意力机制是一种机制，它接收一系列词元，并生成该序列的紧凑向量表示，考虑到词元之间的关系。这是LLM架构中唯一计算词元间关系的地方，因此它构成了语言理解的核心，涵盖了对词汇关系的理解。由于涉及跨词元的计算，从工程角度来看，它也是最有趣的部分，尤其是对于较长序列来说，计算量可能会非常大。</p>
<p>自注意力机制的输入是<code>n_tokens x n_embd</code>的嵌入矩阵，其中每一行或向量表示一个独立的词元。这些向量中的每一个都将被转换为三个不同的向量，分别称为“键”（key）、“查询”（query）和“值”（value）向量。这种转换通过将每个词元的嵌入向量与固定的<code>wk</code>、<code>wq</code>和<code>wv</code>矩阵（这些矩阵是模型参数的一部分）相乘来实现：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171012624.png" alt="image.png" /></p>
<p>将词元的嵌入向量与wk、wq和wv参数矩阵相乘，会为该词元生成“键”（key）、“查询”（query）和“值”（value）向量。</p>
<p>这个过程会对每个词元重复进行，也就是执行n_tokens次。理论上可以通过循环来完成，但为了提高效率，所有行会通过矩阵乘法在一次操作中进行转换，矩阵乘法正是实现这一点的。相关代码如下所示：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified to remove use of cache)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// `cur` contains the input to the self-attention mechanism</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * K = <span class="built_in">ggml_mul_mat</span>(ctx0,</span><br><span class="line">    model.layers[il].wk, cur);</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * Q = <span class="built_in">ggml_mul_mat</span>(ctx0,</span><br><span class="line">    model.layers[il].wq, cur);</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * V = <span class="built_in">ggml_mul_mat</span>(ctx0,</span><br><span class="line">    model.layers[il].wv, cur);</span><br></pre></td></tr></table></figure>
<p>最终，我们得到三个矩阵 K、Q 和 V，它们的大小均为 <code>n_tokens x n_embd</code>，分别包含每个词元的键（key）、查询（query）和值（value）向量堆叠在一起。</p>
<p>自注意力机制的下一步是将包含查询向量的矩阵 Q 与包含键向量的矩阵 K 的转置相乘。对于不太熟悉矩阵操作的人来说，此操作实际上是为每对查询和键向量计算一个联合得分。我们使用符号 S(i,j) 来表示查询 i 与键 j 的得分。</p>
<p>这个过程生成了 <code>n_tokens^2</code> 个得分，每个查询-键对都有一个得分，并将其打包在一个称为 KQ 的矩阵中。随后，该矩阵会进行掩码操作，以移除对角线以上的元素：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171037410.png" alt="image.png" /></p>
<p>通过将矩阵 Q 与 K 的转置相乘，计算每个查询-键对的联合得分 S(i,j)。此处显示的是前四个词元的结果，以及每个得分所对应的词元。掩码步骤确保仅保留每个词元与其前面词元之间的得分。为了简化说明，省略了中间的缩放操作。</p>
<p>掩码操作是一个关键步骤。对于每个词元，它只保留与其前面词元之间的得分。在训练阶段，这一约束确保LLM仅根据之前的词元预测当前词元，而不是未来的词元。此外，正如我们稍后将更详细探讨的，它还允许在预测未来词元时进行显著优化。</p>
<p>自注意力机制的最后一步是将掩码后的得分矩阵<code>KQ_masked</code>与之前的值向量相乘。这样的矩阵乘法操作会生成所有前面词元值向量的加权和，其中权重是得分<code>S(i,j)</code>。例如，对于第四个词元“ics”，它会生成“Quant”、“um”、“▁mechan”和“ics”这几个词元的值向量的加权和，权重为<code>S(3,0)</code>到<code>S(3,3)</code>，这些得分是由“ics”的查询向量与之前所有词元的键向量计算出来的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171048245.png" alt="image.png" /></p>
<p>KQV矩阵包含了值向量的加权和。例如，突出显示的最后一行是前四个值向量的加权和，权重为对应的突出显示的得分。</p>
<p>KQV矩阵标志着自注意力机制的结束。之前我们已经在一般张量计算的上下文中介绍了实现自注意力机制的相关代码，但现在你能够更好地理解它。</p>
<h3 id="transformer的层"><a class="markdownIt-Anchor" href="#transformer的层"></a> Transformer的层</h3>
<p>自注意力机制是Transformer层的一个组成部分。每一层除了自注意力机制外，还包含多个其他的张量操作，主要是矩阵加法、乘法和激活函数操作，这些都是前馈神经网络的一部分。我们不会详细探讨这些操作，只需要注意以下几点：</p>
<ul>
<li>
<p>前馈网络中使用了大型、固定的参数矩阵。在LLaMA-7B中，这些矩阵的大小为<code>n_embd x n_ff = 4096 x 11008</code>。</p>
</li>
<li>
<p>除了自注意力机制之外，其他所有操作都可以看作是逐行或逐词元进行的。正如之前提到的，只有自注意力机制包含跨词元的计算。这一点在后面讨论kv缓存时会非常重要。</p>
</li>
<li>
<p>输入和输出的大小始终为<code>n_tokens x n_embd</code>：每个词元对应一行，每行的大小等于模型的维度。</p>
</li>
</ul>
<p>为完整起见，我还包含了LLaMA-7B中单个Transformer层的图示。请注意，未来的模型架构可能会稍有不同。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171056771.png" alt="image.png" /></p>
<p>LLaMA-7B中一个Transformer层的完整计算图，包含自注意力机制和前馈机制。每一层的输出作为下一层的输入。在自注意力阶段和前馈阶段都使用了大型参数矩阵，这些矩阵构成了该模型的大部分70亿个参数。</p>
<p>在Transformer架构中有多个层。例如，在LLaMA-7B中有32个层（n_layers=32）。这些层是相同的，除了每层都有自己的一组参数矩阵（例如用于自注意力机制的各自的<code>wk</code>、<code>wq</code>和<code>wv</code>矩阵）。第一层的输入是上文描述的嵌入矩阵。第一层的输出随后被用作第二层的输入，依此类推。我们可以将其看作每一层都生成了一组嵌入，但这些嵌入不再直接与单个词元相关，而是与词元关系的某种更复杂的理解相关联。</p>
<h3 id="计算logits"><a class="markdownIt-Anchor" href="#计算logits"></a> 计算logits</h3>
<p>Transformer的最后一步是计算logits。logit是一个浮点数，表示某个特定词元是“正确”下一个词元的概率。logit值越高，表示相应词元是“正确”词元的可能性越大。</p>
<p>logits的计算是通过将最后一个Transformer层的输出与一个固定的<code>n_embd x n_vocab</code>参数矩阵（在<code>llama.cpp</code>中也称为<code>output</code>）相乘来完成的。这个操作为词汇表中的每个词元生成一个logit。例如，在LLaMA中，它会生成<code>n_vocab=32000</code>个logits：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171103132.png" alt="image.png" /></p>
<p>Transformer的最后一步通过将最后一层的输出与一个固定的参数矩阵（也称为'output'）相乘来计算logits。这里只关注结果的最后一行，它包含词汇表中每个可能的下一个词元的logit值。</p>
<p>Logits是Transformer的输出，告诉我们最可能的下一个词元是什么。至此，所有的张量计算都已结束。以下是简化和带注释的<code>llm_build_llama()</code>函数版本，总结了本节中描述的所有步骤：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified and commented)</span></span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">ggml_cgraph</span> * <span class="built_in">llm_build_llama</span>(</span><br><span class="line">         llama_context &amp; lctx,</span><br><span class="line">     <span class="type">const</span> llama_token * tokens,</span><br><span class="line">                   <span class="type">int</span>   n_tokens,</span><br><span class="line">                   <span class="type">int</span>   n_past) &#123;</span><br><span class="line">    ggml_cgraph * gf = <span class="built_in">ggml_new_graph</span>(ctx0);</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * cur;</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * inpL;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a tensor to hold the tokens.</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * inp_tokens = <span class="built_in">ggml_new_tensor_1d</span>(ctx0, GGML_TYPE_I32, N);</span><br><span class="line">    <span class="comment">// Copy the tokens into the tensor</span></span><br><span class="line">    <span class="built_in">memcpy</span>(</span><br><span class="line">        inp_tokens-&gt;data,</span><br><span class="line">        tokens,</span><br><span class="line">        n_tokens * <span class="built_in">ggml_element_size</span>(inp_tokens));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the embedding matrix.</span></span><br><span class="line">    inpL = <span class="built_in">ggml_get_rows</span>(ctx0,</span><br><span class="line">        model.tok_embeddings,</span><br><span class="line">        inp_tokens);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iteratively apply all layers.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> il = <span class="number">0</span>; il &lt; n_layer; ++il) &#123;</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * K = <span class="built_in">ggml_mul_mat</span>(ctx0, model.layers[il].wk, cur);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * Q = <span class="built_in">ggml_mul_mat</span>(ctx0, model.layers[il].wq, cur);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * V = <span class="built_in">ggml_mul_mat</span>(ctx0, model.layers[il].wv, cur);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ = <span class="built_in">ggml_mul_mat</span>(ctx0, K, Q);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_scaled = <span class="built_in">ggml_scale_inplace</span>(ctx0, KQ, KQ_scale);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_masked = <span class="built_in">ggml_diag_mask_inf_inplace</span>(ctx0,</span><br><span class="line">            KQ_scaled, n_past);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQ_soft_max = <span class="built_in">ggml_soft_max_inplace</span>(ctx0, KQ_masked);</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * KQV = <span class="built_in">ggml_mul_mat</span>(ctx0, V, KQ_soft_max);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Run feed-forward network.</span></span><br><span class="line">        <span class="comment">// Produces `cur`.</span></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// input for next layer</span></span><br><span class="line">        inpL = cur;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cur = inpL;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate logits from last layer&#x27;s output.</span></span><br><span class="line">    cur = <span class="built_in">ggml_mul_mat</span>(ctx0, model.output, cur);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Build and return the computation graph.</span></span><br><span class="line">    <span class="built_in">ggml_build_forward_expand</span>(gf, cur);</span><br><span class="line">    <span class="keyword">return</span> gf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了实际执行推理，返回的计算图通过<code>ggml_graph_compute()</code>函数进行计算，如前所述。然后将最后一个张量的数据指针中的logits复制到一个浮点数组中，为下一步“采样”做好准备。</p>
<h2 id="采样"><a class="markdownIt-Anchor" href="#采样"></a> 采样</h2>
<p>拿到logits列表后，下一步是根据它们选择下一个词元。这个过程称为采样。针对不同的使用场景，有多种采样方法可用。在本节中，我们将介绍两种基本的采样方法，稍后会在未来的文章中讨论更高级的采样方法，如语法采样。</p>
<h3 id="贪婪采样"><a class="markdownIt-Anchor" href="#贪婪采样"></a> 贪婪采样</h3>
<p>贪婪采样是一种简单直接的方法，它选择与最高logit值相关联的词元。</p>
<p>对于我们的示例提示，以下词元具有最高的logit值：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>token</td>
<td>logit</td>
</tr>
<tr>
<td>▁describes</td>
<td>18.990</td>
</tr>
<tr>
<td>▁provides</td>
<td>17.871</td>
</tr>
<tr>
<td>▁explains</td>
<td>17.403</td>
</tr>
<tr>
<td>▁de</td>
<td>16.361</td>
</tr>
<tr>
<td>▁gives</td>
<td>15.007</td>
</tr>
</tbody>
</table>
<p>因此，贪婪采样将确定性地选择<code>▁describes</code>作为下一个词元。贪婪采样在重新评估相同的提示时，最适合需要确定性输出的场景。</p>
<h3 id="温度采样"><a class="markdownIt-Anchor" href="#温度采样"></a> <strong>温度采样</strong></h3>
<p>温度采样是一种概率性方法，这意味着相同的提示在重新评估时可能会产生不同的输出。它使用一个称为温度（temperature）的参数，这个浮点值介于0到1之间，影响结果的随机性。过程如下：</p>
<ol>
<li>
<p>对logits按从高到低排序，并使用<strong>softmax</strong>函数进行归一化，确保它们的总和为1。这种变换将每个logit转换为一个概率值。Softmax 的作用是将logits<strong>转换为概率</strong>：Softmax 将模型的输出（logits）转换为可以解释为概率的值。这在多分类问题中尤为重要，模型预测的结果可以解释为每个类别的概率。Softmax 函数确保所有输出值的和为 1，满足概率的定义。</p>
</li>
<li>
<p>应用一个阈值（默认设置为0.95），只保留概率累加值低于阈值的前几个词元。这一步有效地移除了低概率的词元，防止“坏”或“错误”的词元被采样。</p>
</li>
<li>
<p>剩余的logits除以温度参数并再次归一化，使它们的总和为1并代表概率。</p>
</li>
<li>
<p>根据这些概率随机采样一个词元。例如，在我们的提示中，词元<code>▁describes</code>的概率为p=0.6，这意味着它大约有60%的概率被选择。在重新评估时，可能会选择不同的词元。</p>
</li>
</ol>
<p>第3步中的温度参数用于增加或减少随机性。较低的温度值会抑制低概率词元，使得在重新评估时更有可能选择相同的词元。因此，较低的温度值减少了随机性。相反，较高的温度值会使概率分布趋于“平坦”，增加低概率词元的影响，从而增加每次重新评估时选择不同词元的可能性，增加随机性。他是softmax函数的一个参数。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171132918.png" alt="image.png" /></p>
<p>我们的示例提示的归一化下一个词元概率。较低的温度抑制低概率词元，而较高的温度则强调这些低概率词元。<code>temp=0</code>与贪婪采样基本相同。</p>
<p>采样一个词元标志着LLM（大型语言模型）一次完整迭代的结束。在初始词元被采样后，它会被添加到词元列表中，整个过程再次运行。输出会随着每次迭代增加一个词元，逐步成为LLM的输入。</p>
<p>理论上，后续的迭代可以以相同方式进行。然而，随着词元列表的增长，性能可能会下降，因此需要采用一些优化技术。这些技术将在接下来介绍。</p>
<h2 id="优化推理"><a class="markdownIt-Anchor" href="#优化推理"></a> <strong>优化推理</strong></h2>
<p>随着输入给LLM的词元列表增长，Transformer的自注意力阶段可能成为性能瓶颈。词元列表越长，意味着相乘的矩阵越大。每次矩阵乘法都由许多较小的数值运算组成，这些运算称为浮点运算，其性能受限于GPU的每秒浮点运算能力（FLOPS）。在Transformer推理计算中，计算得出对于一个52B参数的模型，在A100 GPU上，当词元数量达到208时，性能开始因为过多的浮点运算而下降。为解决这一瓶颈，最常用的优化技术是<strong>kv缓存</strong>。</p>
<h3 id="kv缓存"><a class="markdownIt-Anchor" href="#kv缓存"></a> <strong>KV缓存</strong></h3>
<p>回顾一下，每个词元都有一个关联的嵌入向量，该嵌入向量通过与参数矩阵<code>wk</code>和<code>wv</code>相乘进一步转化为键（key）和值（value）向量。<strong>KV缓存</strong>是用来缓存这些键和值向量的，通过缓存它们，我们可以节省每次迭代重新计算所需的浮点运算。</p>
<p>缓存的工作方式如下：</p>
<ol>
<li>
<p>在初始迭代期间，所有词元的键和值向量都会按照之前的描述进行计算，并保存到KV缓存中。</p>
</li>
<li>
<p>在后续迭代中，仅需要计算最新词元的键和值向量。缓存的键值向量与新词元的键值向量一起被拼接，形成K和V矩阵。这避免了重新计算所有先前词元的键值向量，从而大大提高了效率。</p>
</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171139202.png" alt="image.png" /></p>
<p>在后续迭代中，只计算最新词元的键向量，其他的从缓存中提取，并与新计算的键向量一起组成K矩阵。新计算的键向量也会被保存到缓存中。对于值向量，同样的过程也适用。</p>
<p>能够使用键和值向量的缓存，是因为这些向量在迭代之间保持不变。例如，如果我们首先处理四个词元，然后处理五个词元，而最初的四个词元没有变化，那么前四个键和值向量在第一次和第二次迭代中将保持相同。因此，在第二次迭代中不需要重新计算前四个词元的键和值向量。</p>
<p>这一原则在Transformer的所有层中都成立，而不仅仅是在第一层。在所有层中，每个词元的键和值向量仅依赖于先前的词元。因此，当在后续迭代中添加新词元时，现有词元的键和值向量保持不变。</p>
<p>对于第一层，这一概念相对容易验证：词元的键向量是通过将词元的固定嵌入向量与固定的<code>wk</code>参数矩阵相乘确定的。因此，无论引入了多少新词元，在后续迭代中，它都保持不变。同样的道理也适用于值向量。</p>
<p>对于第二层及后续层，这一原则虽然不那么显而易见，但仍然成立。为了理解其原因，我们可以考虑第一层自注意力阶段的KQV矩阵的输出。KQV矩阵中的每一行是一个加权和，取决于：</p>
<ul>
<li>
<p>前面词元的值向量。</p>
</li>
<li>
<p>由前面词元的键向量计算的得分。</p>
</li>
</ul>
<p>因此，KQV矩阵中的每一行仅依赖于之前的词元。经过一些基于行的操作后，这个矩阵作为第二层的输入。这意味着，除了新增的行外，第二层的输入在未来的迭代中将保持不变。通过归纳法，这一逻辑可以延伸到剩余的各层。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171150127.png" alt="image.png" /></p>
<p>再来看看KQV矩阵的计算方式。突出显示的第三行仅由第三个查询向量以及前三个键和值向量（同样突出显示）决定。后续的词元不会对其产生影响。因此，在未来的迭代中，它将保持不变。</p>
<h3 id="进一步优化后续迭代"><a class="markdownIt-Anchor" href="#进一步优化后续迭代"></a> 进一步优化后续迭代</h3>
<p>你可能会疑惑，既然我们缓存了键和值向量，为什么不缓存查询向量呢？答案是，实际上，除了当前词元的查询向量外，后续迭代中不再需要之前词元的查询向量。有了<strong>kv缓存</strong>后，我们实际上只需要将最新词元的查询向量传入自注意力机制即可。这个查询向量将与缓存的K矩阵相乘，计算最后一个词元与所有之前词元的联合得分。然后，它与缓存的V矩阵相乘，只计算KQV矩阵的最新一行。</p>
<p>事实上，在所有层中，我们现在传递的是大小为<code>1 x n_embd</code>的向量，而不是在第一次迭代中计算的<code>n_token x n_embd</code>矩阵。为了说明这一点，可以对比下图中显示的后续迭代与之前的图示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126171202988.png" alt="image.png" /></p>
<p>后续迭代中的自注意力机制。在这个示例中，第一次迭代中有四个词元，第二次迭代中添加了第五个词元‘▁is’。最新词元的键、查询和值向量与缓存的键和值向量一起，用于计算KQV矩阵的最后一行，这也是预测下一个词元所需的全部内容。</p>
<p>这个过程在所有层中重复，利用每一层的<strong>kv缓存</strong>。因此，在这种情况下，Transformer的输出是一个包含<code>n_vocab</code>个logit的向量，用于预测下一个词元。</p>
<p>通过这种优化，我们节省了在KQ和KQV矩阵中计算不必要行的浮点运算，这种节省在词元列表增大时尤为显著。</p>
<h3 id="kv缓存的实际应用"><a class="markdownIt-Anchor" href="#kv缓存的实际应用"></a> <strong>KV缓存的实际应用</strong></h3>
<p>我们可以深入研究<code>llama.cpp</code>的代码，了解KV缓存是如何在实践中实现的。不出意外，它是使用张量构建的，一个用于键向量，一个用于值向量：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">llama_kv_cache</span> &#123;</span><br><span class="line">    <span class="comment">// cache of key vectors</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * k = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cache of value vectors</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * v = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n; <span class="comment">// number of tokens currently in the cache</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>在初始化缓存时，为每一层分配足够的空间，以容纳512个键和值向量：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified)</span></span><br><span class="line"><span class="comment">// n_ctx = 512 by default</span></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">bool</span> <span class="title">llama_kv_cache_init</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">struct</span> llama_kv_cache &amp; cache,</span></span></span><br><span class="line"><span class="params"><span class="function">    ggml_type   wtype,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span>   n_ctx)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Allocate enough elements to hold n_ctx vectors for each layer.</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int64_t</span> n_elements = n_embd*n_layer*n_ctx;</span><br><span class="line"></span><br><span class="line">    cache.k = <span class="built_in">ggml_new_tensor_1d</span>(cache.ctx, wtype, n_elements);</span><br><span class="line">    cache.v = <span class="built_in">ggml_new_tensor_1d</span>(cache.ctx, wtype, n_elements);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>回顾一下，在推理过程中，计算图是通过<code>llm_build_llama()</code>函数构建的。这个函数有一个之前忽略的参数<code>n_past</code>。在第一次迭代中，<code>n_tokens</code>参数包含词元的数量，而<code>n_past</code>被设置为0。在后续迭代中，<code>n_tokens</code>被设置为1，因为只处理最新的词元，而<code>n_past</code>包含之前的词元数量。<code>n_past</code>用于从kv缓存中提取正确数量的键和值向量。</p>
<p>以下是该函数的相关部分，展示了如何使用缓存来计算K矩阵。我稍微简化了一下，忽略了多头注意力机制，并为每一步添加了注释：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// llama.cpp (simplified and commented)</span></span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">struct</span> <span class="title class_">ggml_cgraph</span> * <span class="built_in">llm_build_llama</span>(</span><br><span class="line">    llama_context &amp; lctx,</span><br><span class="line">    <span class="type">const</span> llama_token * tokens,</span><br><span class="line">    <span class="type">int</span>   n_tokens,</span><br><span class="line">    <span class="type">int</span>   n_past) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iteratively apply all layers.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> il = <span class="number">0</span>; il &lt; n_layer; ++il) &#123;</span><br><span class="line">         <span class="comment">// Compute the key vector of the latest token.</span></span><br><span class="line">         <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * Kcur = <span class="built_in">ggml_mul_mat</span>(ctx0, model.layers[il].wk, cur);</span><br><span class="line">         <span class="comment">// Build a view of size n_embd into an empty slot in the cache.</span></span><br><span class="line">         <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * k = <span class="built_in">ggml_view_1d</span>(</span><br><span class="line">            ctx0,</span><br><span class="line">            kv_cache.k,</span><br><span class="line">            <span class="comment">// size</span></span><br><span class="line">            n_tokens*n_embd,</span><br><span class="line">            <span class="comment">// offset</span></span><br><span class="line">            (<span class="built_in">ggml_element_size</span>(kv_cache.k)*n_embd) * (il*n_ctx + n_past)</span><br><span class="line">         );</span><br><span class="line"></span><br><span class="line">         <span class="comment">// Copy latest token&#x27;s k vector into the empty cache slot.</span></span><br><span class="line">         <span class="built_in">ggml_cpy</span>(ctx0, Kcur, k);</span><br><span class="line"></span><br><span class="line">         <span class="comment">// Form the K matrix by taking a view of the cache.</span></span><br><span class="line">         <span class="keyword">struct</span> <span class="title class_">ggml_tensor</span> * K =</span><br><span class="line">             <span class="built_in">ggml_view_2d</span>(ctx0,</span><br><span class="line">                 kv_self.k,</span><br><span class="line">                 <span class="comment">// row size</span></span><br><span class="line">                 n_embd,</span><br><span class="line">                 <span class="comment">// number of rows</span></span><br><span class="line">                 n_past + n_tokens,</span><br><span class="line">                 <span class="comment">// stride</span></span><br><span class="line">                 <span class="built_in">ggml_element_size</span>(kv_self.k) * n_embd,</span><br><span class="line">                 <span class="comment">// cache offset</span></span><br><span class="line">                 <span class="built_in">ggml_element_size</span>(kv_self.k) * n_embd * n_ctx * il);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先，计算新的键向量。接着，使用<code>n_past</code>找到缓存中的下一个空位，并将新的键向量复制到那里。最后，通过在缓存中取一个包含正确数量词元（<code>n_past + n_tokens</code>）的视图来构建K矩阵。</p>
<p><strong>KV缓存</strong>是LLM推理优化的基础。值得注意的是，目前在<code>llama.cpp</code>中实现的版本（截至撰写本文时）并不是最优化的。例如，它提前分配了大量内存来容纳支持的最大数量的键和值向量（此处为512个）。一些更高级的实现，如vLLM，旨在提高内存使用效率，并可能提供进一步的性能提升。这些高级技术将留待后续讨论。此外，随着该领域的快速发展，未来可能会出现新的、更优的优化技术。</p>
<p><strong>总结</strong></p>
<p>本文涵盖了大量内容，旨在为你提供对LLM推理过程的基本理解。掌握这些知识后，你可以进一步探索更高级的资源：</p>
<ul>
<li>
<p><strong>LLM参数计数</strong>和<strong>Transformer推理算术</strong>深入分析LLM的性能。</p>
</li>
<li>
<p><strong>vLLM</strong>是一个更高效管理kv缓存内存的库。</p>
</li>
<li>
<p><strong>持续批处理</strong>是一种优化技术，用于将多个LLM提示批量处理。</p>
</li>
</ul>
<p>我也希望在未来的文章中探讨更多高级主题的内部原理。以下是一些可能的选项：</p>
<ul>
<li>
<p>量化模型</p>
</li>
<li>
<p>使用<strong>LoRA</strong>微调的LLM</p>
</li>
<li>
<p>各种注意力机制（多头注意力机制、分组查询注意力机制和滑动窗口注意力机制）</p>
</li>
<li>
<p>LLM请求批处理</p>
</li>
<li>
<p>语法采样</p>
</li>
</ul>
<p>敬请期待！</p>
<p><strong>脚注</strong></p>
<ol>
<li>
<p><code>ggml</code> 还提供了 <code>ggml_build_backward()</code>，它通过从输出到输入的反向方式计算梯度。此函数仅在模型训练期间用于反向传播，而在推理中从未使用。</p>
</li>
<li>
<p>这篇文章描述的是编码器-解码器模型。LLaMA 是一个仅解码器模型，因为它一次只预测一个词元。但核心概念是相同的。</p>
</li>
<li>
<p>为简化起见，我在此描述了单头自注意力机制。LLaMA 使用的是多头自注意力机制。除了使张量运算稍微复杂一些外，这并不影响本节中的核心思想。</p>
</li>
<li>
<p>更准确地说，嵌入向量首先经过一个归一化操作，缩放其值。我们忽略了这个步骤，因为它不影响核心思想的表达。</p>
</li>
<li>
<p>得分还会经过<strong>softmax</strong>操作，缩放后每一行得分的总和为1。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
        <tag>工程能力提升</tag>
      </tags>
  </entry>
  <entry>
    <title>量化基础 &amp; 主流量化技术 (QLoRA, LLM.int8, GPTQ, AWQ, SmoothQuant, FP8)</title>
    <url>/2025/01/26/%E9%87%8F%E5%8C%96%E5%9F%BA%E7%A1%80-%E4%B8%BB%E6%B5%81%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF-QLoRA-LLM-int8-GPTQ-AWQ-SmoothQuant-FP8/</url>
    <content><![CDATA[<p>课程链接: <a href="https://www.bilibili.com/video/BV1kw4m1X7Bi">吴恩达《深入模型量化|Quantization in Depth》中英字幕_哔哩哔哩_bilibili</a></p>
<p>参考资料:</p>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/646210009">https://zhuanlan.zhihu.com/p/646210009</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/704280420">https://zhuanlan.zhihu.com/p/704280420</a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/675701614">https://zhuanlan.zhihu.com/p/675701614</a></p>
</li>
</ul>
<h1 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h1>
<h2 id="模型量化是做什么的"><a class="markdownIt-Anchor" href="#模型量化是做什么的"></a> <strong>模型量化是做什么的</strong></h2>
<p><strong>模型量化是将浮点数值转化为定点数值，同时尽可能减少计算精度损失的方法。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126142805147.png" alt="image.png" /></p>
<p>具体而言，模型量化是一种压缩网络参数的方式，它将神经网络的参数（weight）、激活（activation）等原本用浮点表示的量值换用定点（整型）表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126142813497.png" alt="image.png" /></p>
<span id="more"></span>
<p><strong>模型量化实现建立在深度网络对噪声具有一定的容忍性上</strong>，模型量化相当于对深度网络增加了一定的噪声（量化误差），如果量化位数合适，模型量化基本不会造成较大的精度损失。</p>
<h2 id="什么可以量化"><a class="markdownIt-Anchor" href="#什么可以量化"></a> <strong>什么可以量化</strong></h2>
<ul>
<li>
<p><strong>权重</strong>: 神经网络的参数</p>
</li>
<li>
<p><strong>激活</strong>: 在神经网络层与层之间传播的值</p>
</li>
</ul>
<h2 id="量化的优势"><a class="markdownIt-Anchor" href="#量化的优势"></a> <strong>量化的优势</strong></h2>
<p><strong>模型量化既能减少资源消耗，也能提高运行速度，使大规模推理服务的性能提升。</strong></p>
<p>模型量化的好处主要有：</p>
<ol>
<li>可以减少内存和显存占用，给模型瘦身，降低大模型的使用门槛和资源消耗；</li>
</ol>
<p>如果将<strong>16B</strong>参数的 MOSS 模型做<strong>int4</strong>量化，加载模型所需显存就可以从 32GB 降低到<strong>10GB</strong>，使得 MOSS 能在普通的消费级显卡上跑推理。</p>
<ol start="2">
<li>能够提高运行速度，这可以从两方面理解：</li>
</ol>
<ul>
<li>
<p>在适配低精度的硬件下，量化模型的运算能直接用 int8 GEMM kernel 计算；</p>
</li>
<li>
<p>量化减少了单位数据的 bit 数，因而可以减少计算过程中的 IO 通信量。</p>
</li>
</ul>
<ol start="3">
<li>由于以上两点，我们做模型推理时，可以增加更多的 batch size，同时也能加快计算速度，因此规模化的模型推理就能既快速又高效。</li>
</ol>
<h2 id="量化的挑战"><a class="markdownIt-Anchor" href="#量化的挑战"></a> <strong>量化的挑战</strong></h2>
<ul>
<li>
<p>量化误差</p>
</li>
<li>
<p>重新训练（量化感知训练，Quantization Aware Training）</p>
</li>
<li>
<p>硬件支持有限</p>
</li>
<li>
<p>需要校准数据集</p>
</li>
<li>
<p>打包/解包的开销</p>
</li>
</ul>
<h2 id="线性量化"><a class="markdownIt-Anchor" href="#线性量化"></a> <strong>线性量化</strong></h2>
<p>我们这里主要讨论线性量化。</p>
<p><strong>根据量化方案的不同，可以分为量化感知训练（QAT）和后训练量化（PTQ）。</strong></p>
<ul>
<li>
<p>QAT（Quant-Aware Training） 也可以称为<strong>在线量化（On Quantization）</strong>。它需要利用额外的训练数据，在量化的同时结合反向传播对模型权重进行调整，意在确保量化模型的精度不掉点。</p>
</li>
<li>
<p>PTQ （Post Training Quantization）也可以称为<strong>离线量化（Off Quantization）</strong>。它是在已训练的模型上，使用少量或不使用额外数据，对模型量化过程进行校准，可能伴有模型权重的缩放。其中：</p>
<ul>
<li>
<p><strong>训练后动态量化（PostDynamic Quantization）</strong> 不使用校准数据集，直接对每一层 layer 通过量化公式进行转换。<strong>QLoRA 就是采用这种方法。</strong></p>
</li>
<li>
<p><strong>训练后校正量化（Post Calibration Quantization）</strong> 需要输入<strong>有代表性</strong>的数据集，根据模型每一层 layer 的输入输出调整量化权重。<strong>GPTQ 就是采用这种方法。</strong></p>
</li>
</ul>
</li>
</ul>
<p><strong>根据量化公式的不同，可以分为对称量化和非对称量化</strong>。**`</p>
<ol>
<li><strong>非对称量化 (Asymmetric)</strong>：我们将区间<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>r</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>r</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[r_{\text{min}}, r_{\text{max}}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>q</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>q</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[q_{\text{min}}, q_{\text{max}}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126143810731.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126143824208.png" alt="image.png" /></p>
<ul>
<li>
<ul>
<li>
<p><strong>量化公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>int</mtext><mrow><mo fence="true">(</mo><mtext>round</mtext><mrow><mo fence="true">(</mo><mfrac><mi>r</mi><mi>s</mi></mfrac><mo>+</mo><mi>z</mi><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">q = \text{int} \left( \text{round} \left( \frac{r}{s} + z \right) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord text"><span class="mord">int</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord text"><span class="mord">round</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></p>
<ul>
<li>
<p><strong>反量化公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≈</mo><mo stretchy="false">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">r \approx (q - z) \cdot s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span></p>
</li>
<li>
<p><strong>缩放因子</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span><strong>和零点</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span><strong>计算为</strong>：</p>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>r</mi><mtext>min</mtext></msub></mrow><mrow><msub><mi>q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>q</mi><mtext>min</mtext></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">s = \frac{r_{\text{max}} - r_{\text{min}}}{q_{\text{max}} - q_{\text{min}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2995389999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8184309999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428572em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428572em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mtext>int</mtext><mrow><mo fence="true">(</mo><mtext>round</mtext><mrow><mo fence="true">(</mo><msub><mi>q</mi><mtext>min</mtext></msub><mo>−</mo><mfrac><msub><mi>r</mi><mtext>min</mtext></msub><mi>s</mi></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">z = \text{int} \left( \text{round} \left( q_{\text{min}} - \frac{r_{\text{min}}}{s} \right) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord text"><span class="mord">int</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord text"><span class="mord">round</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7114919999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428572em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></p>
</li>
</ul>
</li>
<li>
<p><strong>为什么z是整数?</strong></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>将零值（在原始“r”范围内）表示为在量化的“q”范围内的一个整数。</p>
<p>例如，卷积神经网络中的零填充（zero-padding）使用完全等于零的张量。</p>
<ul>
<li>
<ul>
<li><strong>zero point超出</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>q</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>q</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[q_{min}, q_{max}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span><strong>表示的范围的情况</strong></li>
</ul>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126143907400.png" alt="image.png" /></p>
<ol>
<li><strong>对称量化 (Symmetric)</strong>：我们将区间 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><msub><mi>r</mi><mtext>max</mtext></msub><mo separator="true">,</mo><msub><mi>r</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-r_{\text{max}}, r_{\text{max}}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>映射到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><msub><mi>q</mi><mtext>max</mtext></msub><mo separator="true">,</mo><msub><mi>q</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-q_{\text{max}}, q_{\text{max}}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>。其中，我们可以将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">r_{\text{max}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设置为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi>r</mi><mtext>tensor</mtext></msub><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\max(|r_{\text{tensor}}|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tensor</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose">)</span></span></span></span>。</li>
</ol>
<ul>
<li>
<ul>
<li>
<p><strong>量化公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>int</mtext><mrow><mo fence="true">(</mo><mtext>round</mtext><mrow><mo fence="true">(</mo><mfrac><mi>r</mi><mi>s</mi></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">q = \text{int} \left( \text{round} \left( \frac{r}{s} \right) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord text"><span class="mord">int</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord text"><span class="mord">round</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></p>
<ul>
<li>
<p><strong>反量化公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≈</mo><mi>q</mi><mo>⋅</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">r \approx q \cdot s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.63889em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span></p>
</li>
<li>
<p><strong>缩放因子</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span><strong>计算为</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><msub><mi>r</mi><mtext>max</mtext></msub><msub><mi>q</mi><mtext>max</mtext></msub></mfrac></mrow><annotation encoding="application/x-tex">s = \frac{r_{\text{max}}}{q_{\text{max}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1925999999999999em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7114919999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>权衡（Trade-off）</strong></p>
<ul>
<li>
<p><strong>量化范围的利用率（Utilization of quantized range）：</strong></p>
<ul>
<li>
<p>当使用非对称量化（asymmetric quantization）时，量化范围可以被完全利用。</p>
</li>
<li>
<p>当使用对称量化模式（symmetric mode）时，如果浮点数的范围偏向于某一侧，这将导致部分量化范围被分配给不会出现的值。例如在 ReLU 操作中，输出总是正值。</p>
</li>
</ul>
</li>
<li>
<p><strong>简单性（Simplicity）：</strong></p>
<ul>
<li>对称量化模式相比于非对称模式更简单。</li>
</ul>
</li>
<li>
<p><strong>内存（Memory）：</strong></p>
<ul>
<li>对于对称量化，我们无需存储零点（zero-point）。</li>
</ul>
</li>
</ul>
<h2 id="量化精细度-granularity"><a class="markdownIt-Anchor" href="#量化精细度-granularity"></a> <strong>量化精细度 (granularity)</strong></h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126143916406.png" alt="image.png" /></p>
<ul>
<li>
<p><strong>Per tensor</strong></p>
</li>
<li>
<p><strong>Per channel(along an axis)</strong></p>
</li>
<li>
<p><strong>Per group(group n elements together)</strong></p>
</li>
</ul>
<h2 id="推理线性量化"><a class="markdownIt-Anchor" href="#推理线性量化"></a> <strong>推理：线性量化</strong></h2>
<p>在神经网络中，我们不仅可以量化<strong>权重（weights）</strong>，还可以量化<strong>激活值（activation）</strong>。根据我们量化的对象，**存储（storage）**和 **计算（computation）**会有所不同</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>存储（Storage）</strong></td>
<td><strong>量化权重 +</strong> 激活值（例如 W8A32）</td>
<td><strong>量化权重 + 量化激活值</strong>（例如 W8A8）</td>
</tr>
<tr>
<td><strong>计算（Computation）</strong></td>
<td>浮点运算（FP32，FP16，BF16…）<br><br><strong>注意：我们需要对权重进行反量化（dequantize）以执行浮点运算</strong></td>
<td>基于整数的运算（INT8，INT4…）<br><br><strong>注意：并非所有硬件都支持</strong></td>
</tr>
</tbody>
</table>
<h2 id="低比特量化-2-bit-4-bit-中的的-weight-packing"><a class="markdownIt-Anchor" href="#低比特量化-2-bit-4-bit-中的的-weight-packing"></a> <strong>低比特量化 (2 bit, 4 bit) 中的的 Weight Packing</strong></h2>
<p>框架往往不对低比特数据类型做原生支持</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">tensor = torch.tensor([0, 1], dtype=torch.int4) # is not supported!</span><br></pre></td></tr></table></figure>
<h3 id="packing是如何工作的"><a class="markdownIt-Anchor" href="#packing是如何工作的"></a> <strong>Packing是如何工作的？</strong></h3>
<ul>
<li>考虑以下张量，它存储了 4 个值，这些值可以用 2-bit 精度表示，但存储在 8-bit 中：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">tensor = torch.tensor([1, 0, 3, 2], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
当前编码形式为：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">00000001 00000000 00000011 00000010</span><br></pre></td></tr></table></figure>
我们可以将所有这些数据“打包”到一个单独的 8-bit 值中：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">10110001</span><br></pre></td></tr></table></figure>
代码示例：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">packed_tensor = torch.Tensor([177], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>它反映了量化权重的“真实”内存占用。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>
<p>解包后的张量形状需要是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mi mathvariant="normal">/</mi><mi mathvariant="normal">/</mi><mtext>nbits</mtext></mrow><annotation encoding="application/x-tex">8 // \text{nbits}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">8</span><span class="mord">/</span><span class="mord">/</span><span class="mord text"><span class="mord">nbits</span></span></span></span></span> 的倍数。</p>
</li>
<li>
<p>在执行操作之前需要先解包。</p>
</li>
</ul>
<h1 id="主流量化方法"><a class="markdownIt-Anchor" href="#主流量化方法"></a> 主流量化方法</h1>
<h2 id="qlora"><a class="markdownIt-Anchor" href="#qlora"></a> QLoRA</h2>
<p><strong>QLoRA 同时结合了模型量化 Quant 和 LoRA 参数微调两种方法</strong>，因此可以在单张<strong>48GB</strong>的 GPU 上对一个<strong>65B</strong> 的大模型做 finetune。QLoRA 的量化方法（由 bitsandbytes 库提供 backend）也是 Transformers 官方的模型量化实现。</p>
<p>运用 QLoRA 的微调方法训练的模型 Guanaco 在多项任务上表现强劲，<strong>截止2023.07.14，Guanaco-65B模型在 Open LLM Leaderboard 排名第二</strong>，大幅超越了原始的 llama-65B。正因为 QLoRA 的高效训练方法和在下游任务的优秀表现，自公开 Guanaco 模型后，QLoRA 的这套方法也开始得到许多人的关注。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144209069.png" alt="image.png" /></p>
<p>排行榜的指标仅是一种参考。当然，现在的 LeaderBoard 已经被 Llama 2 等一众模型超越了</p>
<p>QLoRA 针对模型权重（weight）做量化，采用的是对称量化算法，量化过程基本同上面讲述的方法一致。我们主要来看它的量化创新点。</p>
<p>量化部分的创新点：</p>
<ul>
<li>
<p>采用新的 NF（NormalFloat） 数据类型，它是对于正态分布权重而言信息理论上最优的数据类型，同时，NF 类型有助于缓解异常值的影响；</p>
</li>
<li>
<p>Double Quant，对于量化后的 scale 数据做进一步的量化；</p>
</li>
</ul>
<h3 id="nf4-数据类型"><a class="markdownIt-Anchor" href="#nf4-数据类型"></a> NF4 数据类型</h3>
<p>新的数据类型，可以看成<strong>新的</strong>格点分配策略。我们用一张图说明 int4 数据类型和 NF4 数据类型的区别。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144226363.png" alt="" /></p>
<ul>
<li>
<p>int4 的格点分布是<strong>均匀的</strong>，然而模型的权重通常服从均值为 0 的正态分布，因此格点的分布和数据的分布不一致。这会导致格点“供需”的不匹配。</p>
<ul>
<li>
<p>靠近 0 点的数据很多，但可用的格点数就相对较少，这样大量参数 round 的粒度较粗，会导致模型量化的精度受损；</p>
</li>
<li>
<p>远离 0 点的数据较少，而可用的格点数相对多，这部分的少量数据不需要太高的量化精度，因此部分格点就被浪费了。</p>
</li>
</ul>
</li>
<li>
<p><strong>NF4 的格点按照正态分布的分位数截取</strong>，格点分布两端稀疏，中间密集，格点分布与数据分布一致。这样格点分配的效率就大大增加了，同时精度受损也不会太大。</p>
</li>
</ul>
<h3 id="double-quant"><a class="markdownIt-Anchor" href="#double-quant"></a> Double Quant</h3>
<p>QLoRA 将每 64 个参数为做一个 block，即 block_size = 64，每个 block 计算一个 Scale。由于量化后的 Scale 通常以 FP32 存储，在 block 数众多的情况下，Scale 占用的显存也不可忽视。因此，QLoRA 对 Scale 进一步量化成 FP8，取 Double Quant 的 block size = 256，因而进一步降低了显存消耗。</p>
<ul>
<li>
<p>Double Quant 前，每个参数做量化会需要额外的 32/64 = <strong>0.5 bits</strong> 显存；</p>
</li>
<li>
<p>Double Quant 后，每个参数做量化只需要额外的 8/64 + 32 / (64*256) = <strong>0.127 bits</strong> 显存。</p>
</li>
</ul>
<h3 id="实验效果"><a class="markdownIt-Anchor" href="#实验效果"></a> 实验效果</h3>
<ul>
<li>相比于其他数据格式，NF4 + DQ 训练后的模型 PPL 最低，且在精度（rouge-score、MMLU acc 等）上没有明显损失。</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144244892.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144250427.png" alt="image.png" /></p>
<h2 id="llmint8"><a class="markdownIt-Anchor" href="#llmint8"></a> LLM.int8()</h2>
<p><strong>核心思想</strong></p>
<p>激活中存在离群值(Emergent Features),这些离群值对量化精度取决定性作用。分解权重和激活矩阵，对绝大部分权重和激活用8bit量化，对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。主要操作概括为：矩阵分解、量化矩阵乘&amp;高精度矩阵乘、矩阵相加，下图解释的非常清楚：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144259759.png" alt="image.png" /></p>
<p><strong>优点</strong></p>
<p>能保持与FP16相当的精度，同时，有效降低显存占用</p>
<p><strong>缺点</strong></p>
<p>计算比FP16还慢</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144306095.png" alt="image.png" /></p>
<h2 id="gptq"><a class="markdownIt-Anchor" href="#gptq"></a> GPTQ</h2>
<p>简单来说，<strong>GPTQ 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。</strong> GPTQ 量化需要准备校准数据集。</p>
<p>GPTQ 的思想最初来源于 Yann LeCun 在 1990 年提出的 OBD 算法，随后 OBS、OBC（OBQ） 等方法不断进行改进，而 GPTQ 是 OBQ 方法的加速版。GPTQ 的量化有严谨的数学理论推导，所有的算法步骤都有理论支撑。</p>
<h3 id="核心思想"><a class="markdownIt-Anchor" href="#核心思想"></a> <strong>核心思想</strong></h3>
<p>按行迭代进行权重量化，部分权重量化后，利用近似二阶导数信息，更新未量化的权重，从而弥补已量化权重带来的精度损失。</p>
<h3 id="优点"><a class="markdownIt-Anchor" href="#优点"></a> <strong>优点</strong></h3>
<p>小批量推理加速明显，4bit量化精度损失可忽略，可扩展至2bit，甚至三元量化</p>
<h3 id="缺点"><a class="markdownIt-Anchor" href="#缺点"></a> <strong>缺点</strong></h3>
<p>可能会过拟合于校准数据</p>
<h3 id="原理"><a class="markdownIt-Anchor" href="#原理"></a> <strong>原理</strong></h3>
<p><em><strong>目标优化函数</strong></em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144331146.png" alt="image.png" /></p>
<p><em><strong>优化过程示意图（省去Hessian逆&amp;Cholesky，便于理解）</strong></em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144337796.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144344138.png" alt="image.png" /></p>
<p><em><strong>伪代码（耐心看下，就能懂）</strong></em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144350030.png" alt="image.png" /></p>
<h2 id="awq"><a class="markdownIt-Anchor" href="#awq"></a> AWQ</h2>
<p><strong>AWQ（Activation-aware Weight Quantization ）<strong>方法由 MIT、SJTU、Tsinghua University 联合提出的方法，一种对大模型仅权重量化方法。该方法基于”<strong>权重并不同等重要</strong>“的观察，仅保护1%的显著权重（salient weight）可以大大减少量化误差。AWQ不依赖于任何反向传播或重建，因此可以很好地保持LLM在不同领域和模式上的泛化能力，而不会过拟合到校准集；它也不依赖于任何数据布局重新排序，保持硬件效率。AWQ在多种语言建模、常识问答和领域特定基准测试中优于现有工作。得益于更好的泛化能力，它在指令微调LM和首次实现多模态LM方面取得了出色的量化性能。论文还实现了有效的张量核心内核，以加速AWQ的无重新排序在线反量化，实现速度比</strong>GPTQ快1.45倍</strong>，比<strong>cuBLAS FP16实现快1.85倍</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144356075.png" alt="image.png" /></p>
<p><strong>误区：AWQ量化=W4A16量化</strong></p>
<p>AWQ是一种对模型权重进行低比特量化的方法，使用该方法可以将模型权重(Weight)量化为4bit，并在计算激活值(Activation)时反量化为FP16，即W4A16。也可以基于AWQ方法将权重量化为3bit/8bit，并在计算时是使用4bit/8bit/16bit，由此衍生出W4A4、W4A8等一系列方法。</p>
<p>作者在原文中指出，W4A16可以在精度损失较小的情况下，大幅降低内存占用，且提升模型推理速度，是最常用的方法，因此AWQ和W4A16同镜率较高。</p>
<h3 id="通过保留1的显著权重来改进llm量化"><a class="markdownIt-Anchor" href="#通过保留1的显著权重来改进llm量化"></a> 通过保留1%的显著权重来改进LLM量化</h3>
<p>核心思路：<strong>不是所有的权重都同等重要</strong>。1%的权重参数，可能会主导模型量化过程中的损失。保留这些参数的精度（FP16），会极大程度保护模型的性能。</p>
<p>salient weight 如何选择？</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144402474.png" alt="image.png" /></p>
<p>通常的做法是基于权重 W 选择，实验发现这种方式并不能提升模型量化效果，和随机选择效果差不多。论文中发现<strong>根据activation magnitude选择权重能显著提升模型效果。</strong></p>
<p>**局限性：**如果权重矩阵中有的元素用FP16格式存储，有的用INT4格式存储，不仅存储时很麻烦，计算时取数也很麻烦，kernel函数写起来会很抽象。于是，作者想了一个变通的方法——Scaling。</p>
<h3 id="通过激活感知缩放保护显著权重"><a class="markdownIt-Anchor" href="#通过激活感知缩放保护显著权重"></a> 通过激活感知缩放保护显著权重</h3>
<p>考虑一个权重矩阵 w ，线性运算可以写作 y=wx 。对权重矩阵进行量化后，可以写作 y=Q(w)x ，量化函数 Q(⋅) 定义为公式1：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144409513.png" alt="image.png" /></p>
<p>其中 N 是量化后的比特数， Δ 是量化因子(scaler)。 w′=Round(wΔ) 是量化过程， Δ⋅w′ 是反量化过程。原始的 w 、 Δ 和输入 x 都是FP16格式，不会带来精度损失。整个过程的精度损失全部来源于量化过程中的 Round 取整函数，其误差近似成[0, 0.5]的均匀分布，期望为0.25，可以写作 RoundErr(⋅)∼0.25 。</p>
<p>考虑对于权重矩阵 w 中的单个元素 w ，引入一个缩放因子 s&gt;1 ，量化过程将 w 与该因子相乘，写作 w′=Round(wsΔ′) ，相应地将反量化过程写作 Δ′⋅w′s，这样在计算过程上是“等价”的，如公式2：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144427459.png" alt="image.png" /></p>
<p>虽然公式1和公式2在计算过程上是“等价”的，但是带来的精度损失是不一样的。两种计算方法的误差可以写作公式3：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144433283.png" alt="image.png" /></p>
<p>RoundErr(⋅) 视作常数0.25，公式2的误差与公式1的误差的比值可以表示为 Δ′Δ⋅1s 。</p>
<p>通常情况下，我们对权重矩阵 w 中的某几个元素 w ，乘以缩放因子 s 后，大概率是不会影响权重矩阵 w 的元素最大值的，除非乘的 w 本身就是最大值，但这种概率很小。进而，根据公式1中 Δ 的计算方法，我们认为 Δ′≈Δ。加上 s&gt;1 ，公式2与公式1的误差比值&lt;1，于是作者提出一个观点：<strong>量化时对显著权重进行放大，可以降低量化误差。</strong></p>
<p>这种理论是否正确？作者进行了实验，如下表，随着 s 的增大， Δ′≠Δ 的概率由0不断升高，但在 s&lt;2 之前，概率还是很低的(&lt;5%)；同时，<strong>在一定范围内，随着</strong> s <strong>的增大，误差比值越来越小，这是完全支持作者观点的。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144441368.png" alt="image.png" /></p>
<p>因此，作者改变了思路：为了更加hardware-friendly，<strong>我们对所有权重均进行低比特量化</strong>，但是，在量化时，对于显著权重乘以较大的 s ，相当于降低其量化误差；同时，对于非显著权重，乘以较小的 s ，相当于给予更少的关注。这便是上一节提到的缩放(Scaling)方法。</p>
<p>按照作者的观点，激活值越大，对应通道越显著，就应该分配更大的缩放系数降低其量化误差。因此，作者统计了各通道的平均激活值（计算输入矩阵各列绝对值的平均值） sx ，并直接将此作为各通道的缩放系数。同时引入一个变量 α 用于平衡显著通道和非显著通道的系数</p>
<p>作者在论文中提到，采用一种称为“Fast Grid Search”的方法在[0, 1]区间快速找出最合适的 α ，</p>
<h3 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h3>
<p>在 MMLU、Common Sense 数据集上，Llama 模型使用AWQ方法量化后效果要优于 RTN 和 GPTQ 量化方法。</p>
<p>AWQ 在所有模型尺寸和不同的比特精度上都优于四舍五入到最近量化（RTN）。在较小尺寸的OPT模型上，它比GPTQ在WikiText-2困惑度上取得更好的结果，在较大尺寸的模型上结果相当，这表明了它对不同模型尺寸和家族的普遍适用性。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144448068.png" alt="image.png" /></p>
<p>在指令微调模型 Vicuna 7B、13B 模型进行测试，Awq方法也是优于 RTN、GPTQ 方法。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144453615.png" alt="image.png" /></p>
<p>在模型推理性能上，使用 4-bit AWQ 方法量化速度是 3-bit GPTQ 方法量化的 1.45 倍，是 GPTQ-R 的 2 倍，是使用 Triton 实现的 GPTQ 方法的 2.4倍。</p>
<h2 id="smoothquant"><a class="markdownIt-Anchor" href="#smoothquant"></a> SmoothQuant</h2>
<p>LLM.int8()采用一种混合精度分解（离群值使用FP16计算，其它使用 FP8 计算）的方法，虽然降低了模型显存占用，但很难在硬件加速器上高效地实现分解，导致推理效率不佳。</p>
<p>SmoothQuant 方法是一种不需要训练，能保持模型精度的训练后量化（PTQ）方法，<strong>它对权重 Weight和激活值 Activation都采用了 int8 量化</strong>，显存节省了一半，推理速度提升了1.56 倍。</p>
<h3 id="核心思想-2"><a class="markdownIt-Anchor" href="#核心思想-2"></a> <strong>核心思想</strong></h3>
<p>weights容易量化，activations不易量化，通过一个数学等价的转换形式，迁移激活的outliers到weights，均衡一下，使得weights和activations都相对好量化。</p>
<p>关键点：<strong>将量化难度从激活值转移到权重值。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144501403.png" alt="image.png" /></p>
<h3 id="量化难点"><a class="markdownIt-Anchor" href="#量化难点"></a> 量化难点</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144506969.png" alt="image.png" /></p>
<ol>
<li>
<p><strong>激活值比权重更难量化:</strong> 之前的工作LLM.int8()表明，使用 INT8 甚至 INT4 量化 LLM 的权重不会降低准确性；</p>
</li>
<li>
<p><strong>异常值让激活值量化变得更艰难:</strong> 激活异常值比大多数激活值大约 100 倍。 如果我们使用 INT8 量化，大多数值将被清零；</p>
</li>
<li>
<p>**异常值只存在少数通道（Channel）内。**单一token 方差很大（异常值会存在于每一个 token 中），单一 channel 方差会小很多。</p>
</li>
</ol>
<p>需要在权重值和激活值中分离量化难度，让彼此均容易被量化。</p>
<p>提出加入一个超参 α （迁移强度），来控制从激活值迁移多少难度到权重值。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144514597.png" alt="image.png" /></p>
<h3 id="smoothquant-方法应用于-transformer块"><a class="markdownIt-Anchor" href="#smoothquant-方法应用于-transformer块"></a> <strong>SmoothQuant 方法应用于 Transformer块</strong></h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144520635.png" alt="image.png" /></p>
<p>如上图，通过将线性层和 batched matmul (BMMs) 使用 INT8 计算，将 element-wise 算子，譬如 LayerNorm、Softmax 使用 FP16 计算，这样可以均衡精度和推理效率。</p>
<h3 id="实验-2"><a class="markdownIt-Anchor" href="#实验-2"></a> 实验</h3>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144526413.png" alt="image.png" /></p>
<p>量化 baselines 和 SmoothQuant</p>
<p>其中，SmoothQuant 有三种方式 O1|O2|O3，推理效率会依次提升。</p>
<p><strong>模型效果</strong></p>
<p>以 OPT-175B 模型为例，8bit 量化后精度和原模型相当。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144532158.png" alt="image.png" /></p>
<p>SmoothQuant 方法应用于多个模型，在多个数据集的平均精度也与原模型相当。</p>
<p>SmoothQuant方法在推理速度和显存占用方面相比原模型也都有不少的提升。其中<strong>速度提升 1.51 倍，显存占用节省 1.96 倍。</strong></p>
<h2 id="fp8"><a class="markdownIt-Anchor" href="#fp8"></a> FP8</h2>
<h3 id="background"><a class="markdownIt-Anchor" href="#background"></a> Background</h3>
<h4 id="硬件的支持"><a class="markdownIt-Anchor" href="#硬件的支持"></a> 硬件的支持</h4>
<p>许多硬件厂商的芯片开始支持 FP8 的计算，如英伟达最新的两种架构 Ada (4090) 和 Hopper (H100)。它们的 Tensor Core 计算单元都开始支持 FP8 的计算，如图所示：</p>
<p>在 H100 的第四代 Tensor Core 中，支持任意的 FP8 格式矩阵的乘法 （E4M3xE4M3, E5M2xE5M2, E4M3xE5M2, E5M2xE4M3）</p>
<p>然后会进行累加到 FP32 和 FP16 的数据格式之中</p>
<p>同时也支持浮点格式之间的互相转换，如下图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144539470.png" alt="image.png" /></p>
<h4 id="fp8-好处"><a class="markdownIt-Anchor" href="#fp8-好处"></a> FP8 好处</h4>
<ol>
<li>
<p>FP8 Tensor Cores 比 16-bit Tensor Cores 快</p>
</li>
<li>
<p>减少 memory movement</p>
</li>
<li>
<p>如果模型已经在 FP8 中进行，部署更加方便</p>
</li>
<li>
<p>FP8 拥有更宽的动态范围</p>
</li>
<li>
<p>FP8 到 FP16/FP32/BF16 之间的转换电路，可以设计得更简单直接，而不需要像INT8/UINT8到FP的转化需要乘法和加法的开销。</p>
</li>
</ol>
<h3 id="2-fp8-types"><a class="markdownIt-Anchor" href="#2-fp8-types"></a> 2. FP8 TYPES</h3>
<p>先简单回顾一下浮点数的表示方式：</p>
<h4 id="ieee-754"><a class="markdownIt-Anchor" href="#ieee-754"></a> IEEE 754</h4>
<ul>
<li>浮点数会分为符号位（sign）, 指数位 （exponent）, 和小数位 （Mantissa）</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144547807.png" alt="image.png" /></p>
<p>float32 的表示方法</p>
<ul>
<li>
<p>浮点数根据 Exponent 的值会分为规格化，非规格化和特殊值 （无穷和NAN）。</p>
</li>
<li>
<p>规格化的值计算公式如下：</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144555839.png" alt="image.png" /></p>
<ul>
<li>指数部分为了表示负指数，会减去一个 bias, bias 的值为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mi>e</mi><mtext>−</mtext><mn>1</mn></mrow></msup><mtext>−</mtext><mn>1</mn><mo>=</mo><mn>127</mn></mrow><annotation encoding="application/x-tex">2^{e−1}−1= 127</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord">−</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">7</span></span></span></span></li>
</ul>
<hr />
<p>在 <strong>Tensor Cores</strong> 中，根据指数位和小数位的不同，支持 E5M2 和 E4M3</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144601136.png" alt="image.png" /></p>
<p>下面分别来看看它们的浮点数表示方法:</p>
<h4 id="e5m2"><a class="markdownIt-Anchor" href="#e5m2"></a> E5M2</h4>
<p>E5M2 遵循上面的 IEEE 754 的浮点数格式，其中 <code>bias = 15</code></p>
<ul>
<li>
<p>规格化的值 （指数位不全为 0 /不全为 1）<br />
eg: 0 | 01101 | 01 ，如下图</p>
<p>易得最大值和最小值</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144609843.png" alt="image.png" /></p>
</li>
<li>
<p>非规格化的值 （指数位全为 0）<br />
注意此时计算方法发生了改变，指数位变为 21−b ，小数位不需要 <strong>+1</strong><br />
eg: 0 | 00000 | 11 ，如下图</p>
<p>如果指数位和小数位都为 0，则表示 0</p>
<p>同时，易得非规格化的最大值最小值</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144619245.png" alt="image.png" /></p>
</li>
<li>
<p>特殊值 （指数位全为 1）<br />
特殊值根据小数位的不同分为 无穷 （Infinites） 和 NaN (Not a number)，表示如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144627138.png" alt="image.png" /></p>
</li>
</ul>
<h4 id="e4m3"><a class="markdownIt-Anchor" href="#e4m3"></a> E4M3</h4>
<p>E4M3 不完全遵循 IEEE 754 的数据格式，主要不同在于当指数位全为1时，一样可以用来表示规格化的值（当小数位不为1），而且不能用来表示 Infinites。其中， <code>bias = 7</code></p>
<p>计算方式还是一样的，综合一下，可参考下图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144859989.png" alt="image.png" /></p>
<p>根据表示的方式，可以把浮点数看成 2 的幂之间的 2E 个样本的精度，比如在 E5M2 中，2 和 4 之间会有 4 个样本，4 和 8 之间也会有 4 个样本；在 E4M3 中，2 和 4 之间有 16 个样本。通过这一特性，可以容易得出浮点量化的误差会随着数值变化的增大而增大。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144905905.png" alt="image.png" /></p>
<h3 id="3-convert-to-fp8"><a class="markdownIt-Anchor" href="#3-convert-to-fp8"></a> 3. Convert to FP8</h3>
<p>先试想一下，模型的权重如果都是 [-2, 2]，它们原本的数据格式都是 FP32，如果你想把它转换成 FP8，其实只需要一个 round 函数。</p>
<p>eg: 易得 FP8 E5M2 在 [1,2] 之间的数为 [1, 1.25, 1.5, 1.75]，fp32 的值为 1.4445，那么 fp8 = round(1.4445) = 1.5</p>
<p>不过，FP8 E4M3 的表示范围为 [-448, 448]，明显远远小于正常的 FP32 表示的范围。在一些应用上肯定还是无法表示的，办法就是引入一个缩放因子 scale，使得原 Tensor 的值可以在 FP8 的表示范围下表示，如图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144913253.png" alt="image.png" /></p>
<p>所以对于一个 FP32 的数，要想转换成 FP8, 只需要两步：</p>
<ol>
<li>
<p>Unscaled FP32 = FP32 / scale</p>
</li>
<li>
<p>FP8 = Convert(Unscaled FP32)</p>
</li>
</ol>
<p>第一步，就是尽可能地把 FP32 的数据放到 FP8 的表示范围内。</p>
<p>eg: 假设有 fp32 数组 [1000.0, 23.0, 123.123]，最简单的操作类似于 “minmax”，所有数除以 1000.0/448, 得到 Unscaled FP32 数组 [448, 10.304, 55.104]。</p>
<p>第二步，其实就是上面所说的 Rounding 了，FP8 表示不了精确的 10.304 和 55.104。具体方法可见<a href="https://zhuanlan.zhihu.com/p/574825662">FP8 量化-原理、实现与误差分析</a></p>
<h4 id="寻找-scaling-factor"><a class="markdownIt-Anchor" href="#寻找-scaling-factor"></a> 寻找 scaling factor</h4>
<p>这里提供两种较为容易理解的方法：</p>
<p>第一种 （from Nvidia）:</p>
<p>假设是 Per-Tensor 的量化，就是找到整个 Tensor 中的最大值，而为了减少找全局最大值带来的 memory consumption, 这里提出了一个基于一个 window, 即找到一个局部的最大值，然后再在这个局部最大值添加一个 margin 在作为 scaling factor</p>
<p>第二种 （from PPQ）:</p>
<p>根据 <a href="https://zhuanlan.zhihu.com/p/574825662">志佬的文章</a>，他发现 FP8 对 scale 的选择并不太敏感，所以在 ppq 中，预设了一些 scale 的候选 [.0078125, .03125, .125, 1.0, 4.0, 16.0, 64.0]， 然后通过计算 MSE 损失来选择对应的 scale。</p>
<h3 id="4-example-of-fp8-gemm"><a class="markdownIt-Anchor" href="#4-example-of-fp8-gemm"></a> 4. Example of FP8 GEMM</h3>
<p>在实际的硬件上，FP8 矩阵运算的大体计算过程如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144920847.png" alt="image.png" /></p>
<h3 id="5-fp8-performance"><a class="markdownIt-Anchor" href="#5-fp8-performance"></a> 5. FP8 Performance</h3>
<h4 id="gemm-performance"><a class="markdownIt-Anchor" href="#gemm-performance"></a> GEMM Performance</h4>
<p>在 H100 中，矩阵乘法的速度提升如下图：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144934316.png" alt="image.png" /></p>
<h4 id="dl-training-performance"><a class="markdownIt-Anchor" href="#dl-training-performance"></a> DL TRAINING PERFORMANCE</h4>
<p>对于 GPT 模型训练速度的提升</p>
<p>IMAGE CLASSIFICATION</p>
<p>对于大部分图像分类的模型 FP8 的精度如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144940435.png" alt="image.png" /></p>
<p>可以看到，经过FP8训练后，精度几乎很少下降，而且如下图所示，训练的过程与 FP16 十分相似</p>
<h2 id="开源工具"><a class="markdownIt-Anchor" href="#开源工具"></a> 开源工具</h2>
<h3 id="autoawq"><a class="markdownIt-Anchor" href="#autoawq"></a> AutoAWQ</h3>
<p><a href="https://github.com/casper-hansen/AutoAWQ">https://github.com/casper-hansen/AutoAWQ</a></p>
<h3 id="autogptq"><a class="markdownIt-Anchor" href="#autogptq"></a> AutoGPTQ</h3>
<p><a href="https://github.com/AutoGPTQ/AutoGPTQ">https://github.com/AutoGPTQ/AutoGPTQ</a></p>
<h3 id="auto-fp8"><a class="markdownIt-Anchor" href="#auto-fp8"></a> Auto-FP8</h3>
<p><a href="https://github.com/neuralmagic/AutoFP8">https://github.com/neuralmagic/AutoFP8</a></p>
<h3 id="tensorrt-llm"><a class="markdownIt-Anchor" href="#tensorrt-llm"></a> TensorRT-LLM</h3>
<p><a href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/quantize.py">https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/quantize.py</a></p>
<h2 id="nvidia-benchmark-参考数据"><a class="markdownIt-Anchor" href="#nvidia-benchmark-参考数据"></a> Nvidia Benchmark 参考数据</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144947414.png" alt="image.png" /></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144953513.png" alt="image.png" /></p>
<p><em><strong>Performance &amp; Accuracy Tradeoff</strong></em></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126144959259.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>推理技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>大模型推理</tag>
      </tags>
  </entry>
  <entry>
    <title>HuggingFace的Transformers库学习</title>
    <url>/2025/01/26/HuggingFace%E7%9A%84Transformers%E5%BA%93%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
<h1 id="llama模型架构"><a class="markdownIt-Anchor" href="#llama模型架构"></a> LLAMA模型架构</h1>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126173626337.png" alt="image.png" /></p>
<h1 id="trainer解读"><a class="markdownIt-Anchor" href="#trainer解读"></a> Trainer解读</h1>
<h2 id="trainer构建流程"><a class="markdownIt-Anchor" href="#trainer构建流程"></a> Trainer构建流程</h2>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250126173633263.png" alt="image.png" /></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>工程能力提升</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>工程能力提升</tag>
      </tags>
  </entry>
</search>
