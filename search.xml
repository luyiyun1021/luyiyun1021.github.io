<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Bert</title>
    <url>/2025/01/09/Bert/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/pdf/1810.04805">论文链接</a>﻿</p>
<h2 id="BERT-是什么"><a href="#BERT-是什么" class="headerlink" title="BERT 是什么"></a><strong>BERT 是什么</strong></h2><p><strong>BERT</strong> (<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers) 是一种基于Transformer架构的<strong>预训练语言模型,</strong> 它的主要模型结构是trasnformer的encoder堆叠而成。</p>
<p>它通过在大规模文本数据上的预训练来捕捉语言的深层双向表征，然后再针对不同的自然语言处理（NLP）任务进行微调（fine-tuning）。BERT的出现标志着NLP领域的一个重要进步，因为它能够更好地理解语言的上下文和语义关系。Bert 训练阶段具体如下：</p>
<p><strong>1）预训练阶段</strong>：BERT通过预训练任务来学习语言的深层表示。这些任务通常包括“遮蔽语言模型”（Masked Language Model，MLM）（类似于完形填空）和“下一句预测”（Next Sentence Prediction，NSP）。在MLM任务中，模型被训练来预测输入句子中被遮蔽的词；而在NSP任务中，模型需要判断两个句子是否是连续的文本序列。</p>
<p><strong>2）微调阶段</strong>：预训练完成后，BERT模型可以通过添加任务特定的输出层来进行微调，以适应不同的NLP任务，如情感分析、问答、命名实体识别等。微调过程利用了预训练阶段学到的语言表征，使得模型能够快速适应新的任务并取得优异的性能。</p>
<span id="more"></span>

<p><strong>Q1: 什么是预训练语言模型？</strong></p>
<p>预训练：预训练是一种迁移学习的概念。所谓预训练模型，举个例子，假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型（一个知识渊博的人，见多识广），当我们需要在特定场景使用时，例如做医学命名实体识别，那么，只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整即可（增加行业知识后，这个知识渊博的人就是行业专家）。预训练语言模型有很多，典型的如ELMO、GPT、BERT等。</p>
<p><strong>Q2: 什么是双向（Bidirectional）？</strong></p>
<p>因为BERT之前的预训练语言模型如ELMO和GPT都是单向的（ELMO可以说是双向的，但其实是两个方向相反的单向语言模型的拼接），而结合上下文信息对自然语言处理是非常重要的。Bidirectional也是Bert的主要创新点。</p>
<p><strong>ELMo和OpenAI GPT的问题</strong></p>
<p>ELMo和GPT最大的问题就是传统的语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子”The animal didn’t cross the street because it was too tired”。我们在编码it的语义的时候需要同时利用前后的信息，因为在这个句子中，it可能指代animal也可能指代street。根据tired，我们推断它指代的是animal，因为street是不能tired。但是如果把tired改成wide，那么it就是指代street了。传统的语言模型，不管是RNN还是Transformer，它都只能利用单方向的信息。比如前向的RNN，在编码it的时候它看到了animal和street，但是它还没有看到tired，因此它不能确定it到底指代什么。如果是后向的RNN，在编码的时候它看到了tired，但是它还根本没看到animal，因此它也不能知道指代的是animal。Transformer的Self-Attention理论上是可以同时attend to到这两个词的，但是根据前面的介绍，由于我们需要用Transformer来学习语言模型，因此必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的。</p>
<p>注意：即使ELMo训练了双向的两个RNN，但是一个RNN只能看一个方向，因此也是无法”同时”利用前后两个方向的信息的。也许有的读者会问，我的RNN有很多层，比如第一层的正向RNN在编码it的时候编码了animal和street的语义，反向RNN编码了tired的语义，然后第二层的RNN就能同时看到这两个语义，然后判断出it指代animal。理论上是有这种可能，但是实际上很难。举个反例，理论上一个三层(一个隐层)的全连接网络能够拟合任何函数，那我们还需要更多层词的全连接网络或者CNN、RNN干什么呢？如果数据不是足够足够多，如果不对网络结构做任何约束，那么它有很多中拟合的方法，其中很多是过拟合的。但是通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得它能够更好的收敛到最佳的参数。我们研究不同的网络结构(包括resnet、dropout、batchnorm等等)都是为了对网络增加额外的(先验的)约束。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109154955337.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155030883.png" alt="image.png"></p>
<h2 id="BERT-的预训练"><a href="#BERT-的预训练" class="headerlink" title="BERT 的预训练"></a>BERT 的预训练</h2><p>BERT的预训练阶段包括两个任务，一个是Masked LM ，还有一个是下句预测（Next Sentence Prediction，NSP）。</p>
<p><strong>Task #1：Masked LM</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155111095.png" alt="image.png"></p>
<p>Masked LM 可以形象地称为完形填空问题，随机掩盖掉每一个句子中15%的词，用其上下文来去判断被盖住的词原本应该是什么。举例来说，有这样一个未标注句子 <code>my dog is hairy</code> ，我们可能随机选择了hairy进行遮掩，就变成 <code>my dog is [mask]</code> ，训练模型去预测 [mask] 位置的词，使预测出 hairy的可能性最大，在这个过程中就将上下文的语义信息学习并体现到模型参数中去了。</p>
<p>这里需要说明，GPT使用统计语言模型，这限制了它只能是单向的，而BERT通过Masked LM能够提取上下文信息。更一般地：</p>
<p><strong>AR模型，auto regressive，自回归模型。</strong> 自回归模型可以类比为早期的统计语言模型（Statistical Language Model），也就是根据上文预测下一个单词，或者根据下文预测前面的单词，只能考虑单侧信息，典型的如GPT，而ELMo 是将两个方向（从左至右和从右至左）的自回归模型进行了拼接，实现了双向语言模型，但本质上仍然属于自回归模型</p>
<p><strong>AE模型，auto encoding，自编码模型。</strong> 从损坏的输入数据（相当于加入噪声）中预测重建原始数据，可以使用上下文的信息。BERT使用的就是AE。劣势是在下游的微调阶段不会出现掩码词，因此[MASK] 标记会导致<strong>预训练和微调阶段不一致</strong>的问题。</p>
<p>所以该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：</p>
<p>80%的时间是采用[mask]，<code>my dog is hairy</code> → <code>my dog is [MASK]</code>﻿</p>
<p>10%的时间是随机取一个词来代替mask的词，<code>my dog is hairy</code> -&gt; <code>my dog is apple</code>﻿</p>
<p>10%的时间保持不变，<code>my dog is hairy</code> -&gt; <code>my dog is hairy</code>﻿</p>
<p><strong>为什么使用这个策略？</strong></p>
<p>（其实我目前还不能很好的理解，先将其他地方看到的说法放在这里）</p>
<p>这是因为transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是”hairy”（这个地方的理解，强行记住了位置和masked的分布，而没有真正理解上下文），从而导致若训练样本和微调的样本mask不一致的情况下，模型预测出现很大的偏差。</p>
<p>如果仅使用[MASK]或者随机的词，那么模型可能学习到的信息都是错误的单词（认为这个地方的单词就是不正确的）；</p>
<p>若仅使用正确的单词，那么模型学到的方法就是直接copy（根据学到的上下文，直接断定），从而学不到完整的上下文信息。</p>
<p>综上三个特点，必须在正确的信息（10%）、未知的信息（80% MASK，使模型具有预测能力）、错误的信息（加入噪声10%，使模型具有纠错能力）都有的情况下，模型才能获取全局全量的信息。</p>
<p><strong>Task #2：Next Sentence Prediction</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155131598.png" alt="image.png"></p>
<p>很多下游任务（QA和natural language inference）都是基于两个句子之间关系的理解，基于此项任务，为了增强模型对句子之间关系的理解能力。训练数据选择两个句子（50%情况下是真正相连的两个句子，50%是随机拼接的两个句子），判断第二个句子是不是真正的第一个句子的下文。</p>
<p>其输入形式是，开头是一个特殊符号<code>[CLS]</code>，然后两个句子之间用<code>[SEP]</code>隔断：</p>
<p>Input &#x3D; <code>[CLS] the man went to [MASK] store [SEP]he bought a gallon [MASK] milk [SEP]</code>﻿</p>
<p>Label &#x3D; IsNext</p>
<p>Input &#x3D; <code>[CLS] the man [MASK] to the store [SEP]penguin [MASK] are flight ##less birds[SEP]</code>﻿</p>
<p>Label &#x3D; NotNext</p>
<p>实际构建预训练任务时，是首选设计好 “下句预测” 任务，生成该任务的标注信息，在此基础上构建 “Masked LM” 任务，生成掩码语言模型的标注信息。考虑到预训练涉及两个句子，BERT 采用如下的输入信息表征方案：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155143822.png" alt="image.png"></p>
<p><strong>token embedding</strong> ：将各个词转换成固定维度的向量。在BERT中，每个词会被转换成<strong>768维</strong>的向量表示。在实际代码实现中，输入文本在送入token embeddings 层之前要先进行tokenization处理。此外，两个特殊的token会被插入到tokenization的结果的开头 ([CLS])和结尾 ([SEP])</p>
<p><strong>segment embedding：</strong> 用于区分一个token属于句子对中的哪个句子。Segment Embeddings 层只有两种向量表示。前一个向量是把0赋给第一个句子中的各个token, 后一个向量是把1赋给第二个句子中的各个token。如果输入仅仅只有一个句子，那么它的segment embedding就是全0</p>
<p><strong>position embedding</strong>：Transformers无法编码输入的序列的顺序性，所以要在各个位置上学习一个向量表示来将序列顺序的信息编码进来。加入position embeddings会让BERT理解下面下面这种情况，“ I think, therefore I am ”，第一个 “I” 和第二个 “I”应该有着不同的向量表示。</p>
<p>这3种embedding都是768维的，最后要将其按元素相加，得到每一个token最终的768维的向量表示。</p>
<h2 id="BERT-能干什么"><a href="#BERT-能干什么" class="headerlink" title="BERT 能干什么"></a>BERT 能干什么</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155153118.png" alt="image.png"></p>
<p>首先我们可以看到BERT 具有两种输出，一个是pooler output，对应的[CLS]的输出，以及sequence output，对应的是序列中的所有字的最后一层hidden输出。所以BERT主要可以处理两种，一种任务是分类&#x2F;回归任务（使用的是pooler output），一种是序列任务（sequence output）。</p>
<ul>
<li><p>分类任务</p>
<ul>
<li>Single Sentence Classification tasks</li>
</ul>
</li>
</ul>
<blockquote>
<p>例如：文本分类，我想听音乐，分到音乐这个domain</p>
</blockquote>
<ul>
<li><p>Sentence Pair Classification tasks 例如：自然语言推断任务(NLI)，给定前提，推断假设是否成立</p>
</li>
<li><p>回归任务 回归任务其实是分类任务的一种特殊形式，最后的输出是一个数值而不是具体的某个类别的概率。</p>
</li>
<li><p>具体任务例如：文本相似度，可以判断两个句子是不是类似的，得到具体的分数。  </p>
</li>
<li><p>序列任务  </p>
</li>
<li><p>命名实体识别（NER）</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155227065.png" alt="image.png"></p>
<ul>
<li><p>Cloze task（完形填空）其实这就是bert预训练的一种任务。</p>
</li>
<li><p>SQuAD(Standford Question Answering Dataset) task</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155235236.png" alt="image.png"></p>
<p>SQuAD任务传入的是D,Q，其实D是该篇文章,Q是问题，返回的结果是答案开始的位置s以及答案结束的位置e。例如上图第一个问题的答案是gravity, 它的位置是文章中第17个单词，即s&#x3D;17,e&#x3D;17</p>
<p>具体做法是：我们学习两个向量，分别是Vs,Ve他们分别和document的sequence output做dot product，然后经过softmax，得到对应的s,e位置。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109155245956.png" alt="image.png"></p>
<h2 id="为什么BERT仅用编码器，而GPT仅用解码器？"><a href="#为什么BERT仅用编码器，而GPT仅用解码器？" class="headerlink" title="为什么BERT仅用编码器，而GPT仅用解码器？"></a>为什么BERT仅用编码器，而GPT仅用解码器？</h2><p>BERT 和 GPT 是两种经典的自然语言处理（NLP）模型，它们分别选择了编码器（Encoder）和解码器（Decoder）架构的不同部分进行专注，主要是因为它们的设计目标和任务类型不同。</p>
<ul>
<li><p><strong>任务需求</strong>：BERT专注于理解任务，需要对文本进行全局双向理解，因此使用了编码器。而GPT专注于生成任务，需要逐词预测文本，因此使用了解码器。</p>
</li>
<li><p><strong>架构特性</strong>：编码器擅长捕捉全局上下文信息，适合BERT的双向理解任务。解码器擅长逐步生成序列，适合GPT的文本生成任务。</p>
</li>
<li><p><strong>训练目标</strong>：BERT的预训练任务（如MLM）需要双向上下文理解，而GPT的语言模型任务需要单向文本生成。</p>
</li>
</ul>
<h3 id="BERT：仅使用编码器"><a href="#BERT：仅使用编码器" class="headerlink" title="BERT：仅使用编码器"></a>BERT：仅使用编码器</h3><p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong> 选择了仅使用Transformer的编码器部分，这是因为BERT的设计目的是为了解决需要全面理解句子语义的任务，比如：</p>
<ul>
<li><p><strong>句子分类</strong>（例如情感分析）。</p>
</li>
<li><p><strong>问答系统</strong>（识别文本中的答案）。</p>
</li>
<li><p><strong>命名实体识别</strong>（标记文本中的特定信息，如人名、地点名等）。</p>
</li>
</ul>
<h4 id="1-双向性理解"><a href="#1-双向性理解" class="headerlink" title="1. 双向性理解"></a>1. <strong>双向性理解</strong></h4><p>BERT 的核心特点是双向性（Bidirectional），即它在处理文本时，同时考虑了每个词左右两侧的上下文信息。为了实现这种双向性，BERT需要依赖编码器架构的自注意力机制（Self-Attention）来捕捉整个输入序列中所有词之间的关系。</p>
<ul>
<li>在BERT中，编码器不仅看一个词的前面的词（如在传统语言模型中那样），还看后面的词。这样，模型在每个位置上都能获得全局的上下文信息，从而更好地理解句子的语义。</li>
</ul>
<h4 id="2-预训练任务"><a href="#2-预训练任务" class="headerlink" title="2. 预训练任务"></a>2. <strong>预训练任务</strong></h4><p>BERT的预训练任务包括：</p>
<ul>
<li><p><strong>掩码语言模型（Masked Language Model, MLM）</strong>：BERT在预训练时，会随机地掩盖输入序列中的一些词，然后训练模型根据上下文预测这些被掩盖的词。由于BERT需要同时考虑词的前后信息，这一任务非常适合编码器架构。</p>
</li>
<li><p><strong>下一句预测（Next Sentence Prediction, NSP）</strong>：BERT还通过训练模型预测两个句子是否连续出现，以学习句子间的关系。这也是一种全局理解任务，需要编码器的支持。</p>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>BERT 仅使用编码器，因为它的目标是生成丰富的、上下文感知的词向量表示，适合各种需要深度理解文本语义的NLP任务。</p>
<h3 id="GPT：仅使用解码器"><a href="#GPT：仅使用解码器" class="headerlink" title="GPT：仅使用解码器"></a>GPT：仅使用解码器</h3><p><strong>GPT（Generative Pre-trained Transformer）</strong> 选择了仅使用Transformer的解码器部分，这是因为GPT的设计目的是生成文本或预测序列中的下一个词，例如：</p>
<ul>
<li><p><strong>文本生成</strong>（如文章写作、对话生成）。</p>
</li>
<li><p><strong>语言建模</strong>（预测下一个词）。</p>
</li>
<li><p><strong>自动补全</strong>（如智能回复）。</p>
</li>
</ul>
<h4 id="1-自回归模型"><a href="#1-自回归模型" class="headerlink" title="1. 自回归模型"></a>1. <strong>自回归模型</strong></h4><p>GPT 是一种自回归模型（Autoregressive Model），这意味着它在生成每个词时，只依赖于之前生成的词，而不考虑后续词。解码器架构非常适合这种自回归的任务。</p>
<ul>
<li>在解码器中，使用了掩码自注意力机制（Masked Self-Attention），即在预测下一个词时，只允许模型看到当前词之前的词。这样，模型可以按照顺序生成文本。</li>
</ul>
<h4 id="2-单向性生成"><a href="#2-单向性生成" class="headerlink" title="2. 单向性生成"></a>2. <strong>单向性生成</strong></h4><p>GPT 的生成过程是单向的，即从左到右逐词生成。解码器的架构可以通过掩码注意力机制确保每个生成的词只基于其前面的词，而不是后面的词，这与语言建模的目标一致。</p>
<ul>
<li>每个生成的词都是基于之前所有词的条件下生成的，因此GPT非常适合需要连续生成文本的任务。</li>
</ul>
<h4 id="3-预训练任务"><a href="#3-预训练任务" class="headerlink" title="3. 预训练任务"></a>3. <strong>预训练任务</strong></h4><p>GPT 的预训练任务是典型的语言建模任务，即给定一个词序列，预测下一个词。这个任务直接利用了解码器架构的优势。</p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>GPT 仅使用解码器，因为它的目标是生成连贯的、上下文相关的文本，适合各种文本生成任务。解码器架构的单向性和自回归特性非常适合这类任务。</p>
<p>参考链接：</p>
<p>﻿<a href="https://fancyerii.github.io/2019/03/09/bert-theory/#bert%E7%AE%80%E4%BB%8B">https://fancyerii.github.io/2019/03/09/bert-theory/#bert%E7%AE%80%E4%BB%8B</a>﻿</p>
<p>﻿<a href="https://zhuanlan.zhihu.com/p/103226488">https://zhuanlan.zhihu.com/p/103226488</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>C++常见编程错误总结</title>
    <url>/2024/12/16/C-%E5%B8%B8%E8%A7%81%E7%BC%96%E7%A8%8B%E9%94%99%E8%AF%AF%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="i-More-Efficient-Than-i"><a href="#i-More-Efficient-Than-i" class="headerlink" title="++i More Efficient Than i++"></a>++i More Efficient Than i++</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193001450.png" alt="image.png"></p>
<span id="more"></span>

<h1 id="shared-ptr-Construction"><a href="#shared-ptr-Construction" class="headerlink" title="shared_ptr Construction"></a>shared_ptr Construction</h1><p><strong>What’s wrong with this code?</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void f(const shared_ptr&lt;X&gt;&amp; sp, const vector&lt;int&gt;&amp; v);</span><br><span class="line">2. f(shared_ptr&lt;X&gt;(new X(args), &#123;11, 22, 33 &#125;);</span><br></pre></td></tr></table></figure>
<p>The order of the evaluation of function arguments is unspecified in C++. So the compiler can execute “new X(args)” first, create the vector second, and create shared_ptr last. If the vector creation throws, the memory of X is leaked (shared_ptr has not been created yet).<br><strong>Fix:</strong><br><code>f(make_shared&lt;X&gt;(args), &#123; 11, 22, 33 &#125;);</code></p>
<h1 id="Function-Argument-Evaluation-Order-Undefined"><a href="#Function-Argument-Evaluation-Order-Undefined" class="headerlink" title="Function Argument Evaluation Order Undefined"></a>Function Argument Evaluation Order Undefined</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. f(v[i], i+)      \\Result is undefined.</span><br><span class="line">2. f(out1(), out2() \\You don&#x27;t know which &quot;out&quot; is executed first.</span><br></pre></td></tr></table></figure>
<h1 id="Resource-Management"><a href="#Resource-Management" class="headerlink" title="Resource Management"></a>Resource Management</h1><p>程序里不应该有和内存分配释放相关的裸指针<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193108850.png" alt="image.png"></p>
<h1 id="Multiple-Resources"><a href="#Multiple-Resources" class="headerlink" title="Multiple Resources"></a>Multiple Resources</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193121639.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193132627.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216193153724.png" alt="image.png"></p>
<ul>
<li>C++11 15.2: “if the non-delegating constructor for an object has completed execution and a delegating constructor for that object exists with an exception, the object’s destructor will be invoked.”</li>
<li>Just because you can take advantage of this rule doesn’t mean that you should!</li>
</ul>
<h2 id="Recommendations"><a href="#Recommendations" class="headerlink" title="Recommendations"></a>Recommendations</h2><ul>
<li>new and delete are dangerous. They are easy to cause memory leak.<ul>
<li>Use make_shared and make_unique.</li>
</ul>
</li>
<li>new[] and delete[] are also dangerous.<ul>
<li>Use vector and string.</li>
</ul>
</li>
<li>Manual resource management is dangerous.<ul>
<li>Write a wrapper class for automatic resource management.</li>
</ul>
</li>
<li>Each automatic resource manager..<ul>
<li>..should acquire&#x2F;release exactly one resource, or </li>
<li>..should acquire&#x2F;release multiple resources very carefully.</li>
</ul>
</li>
</ul>
<h1 id="Returning-Value"><a href="#Returning-Value" class="headerlink" title="Returning Value"></a>Returning Value</h1><h2 id="Recommendations-1"><a href="#Recommendations-1" class="headerlink" title="Recommendations"></a>Recommendations</h2><ul>
<li>Don’t return by const value<ul>
<li>Inhibits move semantics, doesn’t achieve anything useful.</li>
</ul>
</li>
<li>Don’t move() when returning local X by value X (returned object and function return type are same).<ul>
<li>The NRVO and move semantics are designed to work together.</li>
<li>NRVO applicable -&gt; direct construction is optimal.</li>
<li>NRVO inapplicable -&gt; move semantics is efficient.</li>
</ul>
</li>
<li>Function return type should not be rvalue reference (X&amp;&amp; foo(..)<ul>
<li>For experts only, extremely rare.</li>
<li>Even the Standardization Committee got burned.</li>
<li>Valid examples: forward, move, declval, get(tuple&amp;&amp;)</li>
</ul>
</li>
<li>Function return type can be lvalue reference (X&amp; foo(..)), but must carefully  check the lifetime of the referenced object:<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">()</span> </span>&#123; string s; <span class="keyword">return</span> s; &#125;  <span class="comment">//Wrong</span></span><br><span class="line"><span class="number">2.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string s)</span></span>&#123; <span class="keyword">return</span> s; &#125;     <span class="comment">//Wrong</span></span><br><span class="line"><span class="number">3.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string&amp; s)</span> </span>&#123;<span class="keyword">return</span> s; &#125;     <span class="comment">//Possible wrong, if s is short-lived.</span></span><br><span class="line"><span class="number">4.</span> <span class="function">string&amp; <span class="title">get_string</span><span class="params">(string&amp;&amp; s)</span> </span>&#123; <span class="keyword">return</span> s; &#125;    <span class="comment">//Possible wrong, if s is short-lived.</span></span><br><span class="line"><span class="number">5.</span> <span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="number">6.</span>   <span class="function">string&amp; <span class="title">get_string</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> m_str; &#125;     <span class="comment">//Ok, but make sure caller does</span></span><br><span class="line"><span class="number">7.</span>   string m_str;                              <span class="comment">//not hold the reference after A</span></span><br><span class="line"><span class="number">8.</span> &#125;;                                           <span class="comment">//is destroyed.</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>CLIP</title>
    <url>/2025/01/10/CLIP/</url>
    <content><![CDATA[<p><strong>CLIP打通了文本和图像之间的联系，是多模态方面的经典之作。</strong></p>
<span id="more"></span>
<h1 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h1><h2 id="对比学习背后的直觉"><a href="#对比学习背后的直觉" class="headerlink" title="对比学习背后的直觉"></a>对比学习背后的直觉</h2><p>首先，让我们谈谈对比学习背后的直觉。 下面，我们可以看到一个很多孩子玩的传统游戏：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183045154.png" alt="image.png"></p>
<p>这个游戏的目标是在右侧寻找与左侧的图片相似的动物。 在我们的例子中，孩子必须在右边的四张图片中搜索一张狗的图片。 首先，孩子必须将四种动物中的每一种与一只狗进行比较，然后得出结论，左下角的图像描绘的是一只狗。</p>
<p>根据许多调查，孩子们通过这种方式比阅读有关动物的书更容易学习新概念。 但是为什么这种方法效果更好呢？</p>
<p>事实证明，对于像孩子这样没有先验知识的人来说，通过对比相似和不同的事物来学习新事物比一个一个地学习识别它们更容易。 起初，孩子可能无法识别狗。 但过了一段时间，孩子学会了区分狗的共同特征，比如鼻子的形状和身体姿势。</p>
<h2 id="对比学习的训练目标"><a href="#对比学习的训练目标" class="headerlink" title="对比学习的训练目标"></a>对比学习的训练目标</h2><p><strong>受先前观察的启发，对比学习旨在通过对比相似和不相似的样本来学习数据的低维表示。</strong> 具体来说，它试图在表示空间中使相似的样本彼此靠近，并使用欧氏距离将不同的样本推得更远。</p>
<p>假设我们有三张图片 I_1、I_2 和 I_3。 前两幅图像描绘了一只狗，第三幅图像描绘了一只猫，我们想要为每幅图像（x_1、x_2 和 x_3）学习低维表示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183054187.png" alt="image.png"></p>
<p>在对比学习中，我们希望最小化相似样本之间的距离，最大化不同样本之间的距离。 在我们的示例中，**我们希望最小化距离 d(x_1, x_2) 并最大化距离 d(x_1, x_3) 和 d(x_2, x_3)**，其中 d() 是像欧几里得这样的度量函数。</p>
<p>与锚样本（I_1）相似的样本定义为正样本（I_2），与锚样本（I_3）不相似的样本定义为负样本。</p>
<h2 id="无监督训练"><a href="#无监督训练" class="headerlink" title="无监督训练"></a>无监督训练</h2><p>当我们没有标注样本时，可以使用自监督学习，利用数据中的某些特性来生成伪标签。</p>
<p>一个用于无监督对比学习的著名自监督框架是 SimCLR。其主要思想是通过对原始图像应用随机变换（如裁剪、翻转和颜色抖动）来生成正样本对，因为这些变换不会改变图像的标签。</p>
<h1 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h1><p>对比开放的计算机视觉应用，目前的所有的视觉公开数据集（例如ImageNet等）的应用场景都是非常有限的，为了学习到通用的图像-文本多模态通用特征，首先要做的便是采集足够覆盖开放计算机视觉领域的数据集。这里<strong>OpenAI采集了一个总量超过4亿图像-文本对的数据集</strong>WIT（WebImage Text）。为了尽可能的提高数据集在不同场景下的覆盖度，WIT的首先使用在英文维基百科中出现了超过100次的单词构建了50万个查询，并且使用WordNet进行了近义词的替换。为了实现数据集的平衡，每个查询最多取2万个查询结果。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>CLIP的核心思想是将图像和文本映射到同一个特征空间。这个特征空间是一个抽象的概念，例如当我们看到一条狗的图片的时候，我们心中想的是狗，当我们读到狗的时候我们想的也是狗，那么我们心中想象的狗，便是“特征空间”。</p>
<p>所以CLIP也是由两个编码器组成，如图所示：</p>
<ul>
<li><p>图像编码器（VIT和ResNet，都用了）：用于将图像映射到特征空间；</p>
</li>
<li><p>文本编码器（Transformer）：用于将文本映射到相同的特征空间。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183110519.png" alt="image.png"></p>
<p>在模型训练过程中，我们取到的每个batch由N个图像-文本对组成。这N个图像送入到图像编码器中会得到N个图像特征向量(I1,I2,⋯,IN)，同理将这N个文本送入到文本编码器中我们可以得到N个文本特征向量(T1,T2,⋯,TN)。因为只有在对角线上的图像和文本是一对，所以<strong>CLIP的训练目标是让是一个图像-文本对的特征向量相似度尽可能高，而不是一对的相似度尽可能低</strong>，这里相似度的计算使用的是向量内积。通过这个方式，CLIP构建了一个由N个正样本和N^2−N个负样本组成的损失函数。另外，因为不同编码器的输出的特征向量长度不一样，CLIP使用了一个线性映射将两个编码器生成的特征向量映射到统一长度，CLIP的计算过程伪代码如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># image_encoder - 残差网络 或者 ViT</span><br><span class="line"># text_encoder - CBOW 或者 文本Transformer</span><br><span class="line"># I[n, h, w, c] - 训练图像</span><br><span class="line"># T[n, l] - 训练文本</span><br><span class="line"># W_i[d_i, d_e] - 训练图像生成的特征向量</span><br><span class="line"># W_t[d_t, d_e]  - 训练文本生成的特征向量</span><br><span class="line"># t - softmax的温度（temperature）参数</span><br><span class="line"></span><br><span class="line"># 提取多模态的特征</span><br><span class="line">I_f = image_encoder(I) #[n, d_i]</span><br><span class="line">T_f = text_encoder(T) #[n, d_t]</span><br><span class="line"></span><br><span class="line"># 多模态特征向特征空间的映射</span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=1)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=1)</span><br><span class="line"></span><br><span class="line"># 计算余弦相似度</span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"></span><br><span class="line"># 构建损失函数</span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=0)</span><br><span class="line">loss_t  = cross_entropy_loss(logits, labels, axis=1)</span><br><span class="line">loss = (loss_i + loss_t)/2</span><br></pre></td></tr></table></figure>

<h2 id="Zero-shot-推理"><a href="#Zero-shot-推理" class="headerlink" title="Zero-shot 推理"></a>Zero-shot 推理</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183141161.png" alt="image.png"></p>
<ul>
<li><p>考虑到大部分的数据集的标签都是以单词的形式存在的，比如“bird”，“cat”等等，然而在预训练阶段的文本描述大多都是某个短句，为了填补这种数据分布上的差别，作者考虑用指示上下文（guide context） 对标签进行扩展。可以<strong>用a photo of a label作为推理时文本端的输入</strong>，其中的 label 恰恰是需要预测的zero-shot标签。</p>
</li>
<li><p>CLIP模型的效果实现了图像和文本向同一个特征空间映射的能力。当进行图像识别时，我们将待识别的图像映射成一个特征向量。同时我们将所有的类别文本转换成一个句子（指示上下文），然后将这个句子映射成另外一组特征向量。<strong>文本特征向量和图像特征向量最相近的那一个便是我们要识别的目标图像的类</strong>。</p>
</li>
<li><p>考虑到以单词作为标签存在多义的情况，比如boxer表示斗牛犬、拳击运动；crane同时表示了起重机和鹤。这种词语的多义显然对是因为缺少对标签的上下文描述导致的。为了解决这种问题，<strong>作者在指示上下文中添加了一些提示标签类型的词语</strong>，比如A photo of a label, a type of pet.。作者将这个方法<strong>称之为“prompt engineering”</strong>。在合适地选取了不同的指示上下文，并且将其打分进行ensemble之后。作者发现这些Tricks竟能在zero-shot实验上提高5个绝对百分位。</p>
</li>
<li><p><strong>全局学习</strong>：CLIP学习的不再是图像中的一个物体，而是整个图像中的所有信息，不仅包含图像中的目标，还包含这些目标之间的位置，语义等逻辑关系。这便于将CLIP迁移到任何计算机视觉模型上。这也就是为什么CLIP可以在很多看似不相关的下游任务上（OC<strong>R等）取得令人意外的效果。</strong></p>
</li>
<li><p><strong>在实际应用中，这个类别的标签也是可以改的，不必非得是 ImageNet 中的1000个类，可以换成任何的单词；这个图片也不需要是 ImageNet 的图片，也可以是任何的图片</strong>，依旧可以通过算相似度来判断这图中含有哪些物体。即使这个类别标签是没有经过训练的，只要图片中有某个物体也是有很大概率判断出来的，这就是 zero-shot。但如果像之前的那些方法，严格按照1000个类去训练分类头，那么模型就只能判断出这1000个类，这1000个类之外的所有内容都将判断不出来。</p>
</li>
</ul>
<h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><p>CLIP 彻底摆脱了 categorical label 的限制，无论在训练时，还是在推理时，都不需要有这么一个提前定好的标签列表，任意给出一张图片，都可以通过给模型不同的文本句子，从而知道这张图片里有没有我想要的物体。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183153436.png" alt="image.png"></p>
<p>CLIP 把视觉的语义和文字的语义联系到了一起，学到的特征语义性非常强，迁移的效果也非常好。如图左侧部分是在 ImageNet 上训练好的 ResNet101，右侧是 CLIP 训练出的 ViT-L，在 ImageNet 上 ResNet 和 CLIP 效果相同，但在 ImageNetV2、ImageNet-R、ObjectNet、ImageNet Sketch、ImageNet-A上，ResNet 的性能明显就不行了，迁移的效果惨目忍睹，但对于 CLIP 来说，它的效果始终都非常好。这也说明了 CLIP 因为和自然语言处理的结合，导致 <strong>CLIP 学出来的视觉特征和我们用语言所描述的某个物体产生了强烈的联系</strong>。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>﻿<a href="https://www.baeldung.com/cs/contrastive-learning">https://www.baeldung.com/cs/contrastive-learning</a>﻿</p>
<p>﻿<a href="https://zhuanlan.zhihu.com/p/477760524">https://zhuanlan.zhihu.com/p/477760524</a>﻿</p>
<p>﻿<a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a>﻿</p>
<p>﻿<a href="https://blog.csdn.net/h661975/article/details/135116957">【CLIP】多模态预训练模型CLIP论文详解-CSDN博客</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>DALL·E 2</title>
    <url>/2025/01/10/DALL%C2%B7E-2/</url>
    <content><![CDATA[<p><a href="https://cdn.openai.com/papers/dall-e-2.pdf">《Hierarchical Text-Conditional Image Generation with CLIP Latents》</a>﻿</p>
<p>Paper: <a href="https://cdn.openai.com/papers/dall-e-2.pdf">https://cdn.openai.com/papers/dall-e-2.pdf</a>﻿</p>
<p>Project: <a href="https://openai.com/product/dall-e-2">https://openai.com/product/dall-e-2</a>﻿</p>
<p>Author: OpenAI</p>
<span id="more"></span>
<p><strong>DALL·E 2能做什么</strong></p>
<ul>
<li><p><strong>DALL·E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.</strong></p>
<ul>
<li><p><strong>生成原创性的图片</strong></p>
</li>
<li><p><strong>组合concepts、attributes，and styles</strong></p>
</li>
</ul>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183739514.png" alt="image.png"></p>
<ul>
<li><strong>DALL·E 2 can expand images beyond what’s in the original canvas, creating expansive new compositions.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183747747.png" alt="image.png"></p>
<ul>
<li><strong>DALL·E 2 can make realistic edits to existing images from a natural language caption. It can add and remove elements while taking shadows, reflections, and textures into account.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183754130.png" alt="image.png"></p>
<ul>
<li><strong>DALL·E 2 can take an image and create different variations of it inspired by the original.</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183800592.png" alt="image.png"></p>
<p><strong>DALLE 2 的hierarchical：</strong>先生成64<em>64小分辨率图片，再利用一个模型上采样到256</em>256，再利用一个模型上采样到1024*1024。</p>
<p>DALLE 2就是： CLIP模型+GLIDE模型（一个基于diffusion model的文本图像生成方法）；</p>
<h1 id="前置"><a href="#前置" class="headerlink" title="前置"></a>前置</h1><h2 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h2><p>CLIP通过对比学习将图像和文本映射到同一特征空间，实现了图像与文本之间的多模态理解。</p>
<h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>GAN的思想其实就是左右手互博，同时训练两个网络：生成器G(generator) 和判别器D(discriminator)。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183810503.png" alt="image.png"></p>
<ul>
<li><p><strong>生成器</strong>: 给定一个随机噪声Z, 生成一个比较真实的图片X’</p>
</li>
<li><p><strong>判别器</strong>: 把生成的图片X’给判别器. 同时再给真实的图片X给这个判别器, 然后让这个判别器去看, 到底哪个是真图片, 哪个是假图片.</p>
</li>
</ul>
<p>其实就是一个0-1的这个二分类问题.</p>
<p>通过generator和discriminator这两个网络之间互相较量, 然后判别器不停地提高自己, 生成器也不停地提高自己, 所以说最后能生成比较真实的图片.</p>
<p><strong>优点：</strong>因为GAN的目标就是以假乱真，所以其生成图像的保真度比较高</p>
<p><strong>缺点：</strong></p>
<ol>
<li><p>训练不够稳定，因为要同时训练两个网络；</p>
</li>
<li><p>因为是以保真度为目标，所以生成图像的多样性较差；</p>
</li>
<li><p>不是概率模型，它的生成都是隐式完成的。你不知道它做了什么，不知道遵循了什么分布，因此GAN在数学上不如后续的VAE、扩散模型优美。</p>
</li>
</ol>
<h2 id="AE（Auto-encoder）"><a href="#AE（Auto-encoder）" class="headerlink" title="AE（Auto-encoder）"></a>AE（Auto-encoder）</h2><p>自编码器是很早的技术，目的是将高维的信息通过encoder压缩到一个低维的code内，然后再使用decoder对其进行重建，关于它的细节可参阅文章：<a href="https://blog.csdn.net/DUDUDUTU/article/details/129286398?spm=1001.2014.3001.5502">Auto-encoder系列</a>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183819046.png" alt="image.png"></p>
<p>给定一个输入X, 通过一个编码器, 然后就能得到一个特征Z, 这个特征的维度一般都会小很多, 所以也叫bottleneck. 然后再从bottleneck开始, 通过一个解码器, 最后得到一个图像X’.</p>
<p>训练的目标函数: 我们希望这个图像X’能尽可能的重建之前的这个X.</p>
<p>因为是自己重建自己, 所以说这也就是为什么叫auto-encoder.</p>
<h2 id="DAE（Denoising-Auto-encoder）"><a href="#DAE（Denoising-Auto-encoder）" class="headerlink" title="DAE（Denoising Auto-encoder）"></a>DAE（Denoising Auto-encoder）</h2><p>先把原图进行了一定程度的打乱变成了一个Xc, 也就是corrupted x, 然后把这个经过扰乱过后的输入传给编码器, 后续都是一样的, 我们仍然希望这个图像X’能尽可能的重建之前的这个X, 而不是扰动后的Xc.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183829472.png" alt="image.png"></p>
<p>这个改进证明非常的有用, 会让这个训练出来的模型非常的稳健, 也不容易过拟合. 主要原因是图像这边像素的冗余性太高了, 即使把原来的这个图片做一些污染, 其实模型还是能抓住它的本质, 然后去把它重建出来的. 这个其实也就有点何恺明MAE (Masked AutoEncoders)的意思也就是masked auto-encoder, 这个掩码自编码器在训练的时候能够mask掉75%, 还能把这个图像很好的重建出来, 也就说明了图像的冗余性确实是高, 也就从侧面证明了这种denoising auto-encoder或者masked auto-encoder的有效性</p>
<h2 id="VAE（Variational-Auto-Encoder）"><a href="#VAE（Variational-Auto-Encoder）" class="headerlink" title="VAE（Variational Auto-Encoder）"></a>VAE（Variational Auto-Encoder）</h2><p>无论是AE，DAE，还是MAE，核心上都是学习bottleneck处这个特征的，然后去做目标检测、分割、分类这些下游任务，并不是去做生成的。原因就是中间特征Z并不是一个概率分布，我们没法对它采样，它是一个用于重建的特征。因此就有了VAE，中间不再生成一个特征，而是生成一个分布（高斯）。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183837941.png" alt="image.png"></p>
<p>因为VAE学到的是一个概率分布，从分布里去抽样生成图像的多样性就会好很多。</p>
<p>VAE的本质思想是学习一个分布，而不是一个特征。后续的诸多工作也是基于这一本质思想进行展开的。</p>
<p>关于VAE的细节也可参阅文章：<a href="https://blog.csdn.net/DUDUDUTU/article/details/129286398?spm=1001.2014.3001.5502">Auto-encoder系列</a>。</p>
<h2 id="VQ-VAE（Vector-Quantized-Variational-Auto-Encoder）"><a href="#VQ-VAE（Vector-Quantized-Variational-Auto-Encoder）" class="headerlink" title="VQ-VAE（Vector Quantized Variational Auto-Encoder）"></a>VQ-VAE（Vector Quantized Variational Auto-Encoder）</h2><p>VQ-VAE 的核心思想是将连续的高维向量编码（例如语音信号、图像、视频等）离散化，从而减少模型的复杂度和存储需求。具体来说，VQ-VAE 通过将连续的编码向量映射到一组离散的“码本”（codebook）中的最近邻，从而将高维连续编码转换为低维的离散编码。在解码器中，这些离散编码被解码回原始的高维向量。</p>
<p>VQ-VAE就是将特征进行量化的VAE。相对于VAE，优化起来相对容易。codebook可以理解为聚类中心，有K*D个聚类中心。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183912860.png" alt="image.png"></p>
<p>但是VQ-VAE学习的是一个固定的codebook，因此无法像VAE一样做随机采样，因此VQ-VAE更像是VQ-AE。若想像VAE那样，就需要有一个prior网络。在VQ-VAE中，prior网络是一个用于生成离散码本（codebook）的神经网络。离散码本是一组预定义的离散向量，用于将连续的向量空间映射到一个离散的向量空间。这种离散化的表示方式使得VQ-VAE可以对输入数据进行高效地编码和解码。</p>
<p>具体来说，prior网络的输入是由编码器生成的连续向量，输出是一个由离散向量组成的码本。prior网络的训练目标是最小化输入向量和最近的码本向量之间的欧几里得距离，从而实现向量量化。在训练过程中，prior网络不仅学习生成码本，还学习将连续向量映射到码本中最近的向量。通过将编码器生成的连续向量量化为码本中最近的向量，VQ-VAE可以保留输入数据的局部结构信息，并且可以在编码和解码过程中实现高效的计算。因此，prior网络在VQ-VAE中起着非常重要的作用。</p>
<p>VQ-VAE 已经在许多领域得到了应用，包括图像生成、音频压缩、语音识别、自然语言处理等。DALL·E 第一版就是基于VQ-VAE做的。</p>
<h2 id="DALL-E"><a href="#DALL-E" class="headerlink" title="DALL-E"></a>DALL-E</h2><p>对于一个图像文本对，文本用BPE编码生成256维的特征，图像用现成的VQ-VAE（codebook）编码为32*32维的特征，然后将这两个特征拼接成一个1280维的序列，再输入至GPT中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183921032.png" alt="image.png"></p>
<p>训练时，将1280维序列的部分遮住，让GPT进行预测。</p>
<p>推理时，只需要输入文本的特征（256维），让GPT自回归的做预测就行。</p>
<h2 id="Diffusion-model-扩散模型"><a href="#Diffusion-model-扩散模型" class="headerlink" title="Diffusion model (扩散模型)"></a>Diffusion model (扩散模型)</h2><h3 id="扩散模型的方法"><a href="#扩散模型的方法" class="headerlink" title="扩散模型的方法"></a>扩散模型的方法</h3><p>扩散模型分为 forward diffusion 和 reverse diffusion 两个阶段。对于一个图像，forward diffusion 做的就是往$X_{0}$中不断添加噪声，一共添加T次，最后得到$X_{t} - N(0,z)$。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183928002.png" alt="image.png"></p>
<p>为什么叫“扩散”，这启发于热力学。热力学中有一个名词叫“diffusion”，如果有两个物质分别是高密度和低密度的，那高密度的物质会慢慢地向低密度的做扩散，最后达到一种平衡。在Diffusion model 中，这种“平衡”的体现就是 forward diffusion 过程最后得到的趋近于各向同性的正态分布。</p>
<p>reverse diffusion过程要做的就是训练一个模型，使得从$X_{t}$恢复至$X_{t-1}$，然后再训练一个模型，使得从$X_{t-1}$恢复至$X_{t-2}$，以此类推直至恢复至$X_{0}$。在reverse diffusion 过程中所有使用到的模型都是共享参数的，也就是说整体其实只有一个模型，只是需要抽样生成很多次。 因此扩散模型目前最大的不足就是：相比于其他生成模型，训练很慢，且推理是最慢的。因为对于一个随机采样的噪声，要往前推T次才能生成Image。此外，目前reverse diffusion 过程中的网络大部分选用的都是U-Net。</p>
<h3 id="扩散模型的发展历程"><a href="#扩散模型的发展历程" class="headerlink" title="扩散模型的发展历程"></a>扩散模型的发展历程</h3><p>扩散模型这个想法在2015年就有人提出来了，但是并不能训练很好，生成图像的效果并不如其他的生成模型，比如GAN等。直至2020年有一篇论文——<strong>DDPM对扩散模型的思想进行了改进，使得训练更方便，其核心思想是：在 reverse diffusion 过程中，给定，不去直接预测（训练起来很难），而是预测使变为的噪声。</strong>这将扩散模型的问题简化了很多，直接让网络去预测一个噪声即可（类似于ResNet中去预测一个残差），比如对于正态分布而言，就是预测均值和方差，DDPM作者还发现，将方差设为一个常数，只去预测均值的话，就已经可以使扩散模型生成很好的图像。</p>
<p><strong>给U-Net中加入temporal embedding，目的是使 reverse diffusion 是一个 coarse-to-fine 的过程，让模型知道目前处于哪一步</strong>，希望在最开始先生成一些粗糙的信息，最后快结束时再生成一些细致的信息，即高频的信息（比如物体的边边角角）。损失函数就是：，就是U-Net网络，t是temporal embedding，是前向过程中添加的噪声，是已知的，因此可以拿来当作Ground truth，目的是希望U-Net在每一步预测的噪声与前向过程中的相同。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183935982.png" alt="image.png"></p>
<p>DDPM与VAE其实是有相似之处，比如DDPM 也可以看做的一个 encoder-decoder 结构，分别对应forward、reverse的过程。但他们也有不同，如下：</p>
<ul>
<li><p>DDPM的前向过程是固定的，而VAE的encoder是学习的；</p>
</li>
<li><p>DDPM的每一步的特征维度都是相同的，而VAE的bottle neck的维度是比输入小很多的。</p>
</li>
<li><p>扩散模型有 step 的概念，有很多步；VAE没有；</p>
</li>
</ul>
<p>在DDPM 证明了扩散模型可以work很好之后，后续出现了很多工作，比如2021年OpenAI的这篇论文<strong>《Diffusion Models Beat GANs on Image Synthesis》</strong>。在这篇论文出现之前扩散模型生成的图像已经很逼真了，但是在各项指标上还比不过GAN，然后这篇论文提出了一个Classifier guidance方法来引导模型生成图片，使得生成的效果更好，并且只做25次采样即可实现。</p>
<p>Classifier guided diffusion的意思是：<strong>在训练diffusion的同时，再去训练一个图片分类器</strong>(classifier，一般是在ImageNet的加了noise的图片上训练)。这个classifier的作用是：有了后，就可以丢给classifier计算一个类别损失，也就是计算出了一个梯度来辅助U-Net的训练过程。这个梯度中暗含了中是否有这个物体，或者说当前的这个物体真不真实的信息，以此来告诉U-Net要在生成的图片中在颜色、纹理等要跟真实的物体匹配上。</p>
<p>这个操作的核心思想是利用梯度来引导diffusion的生成，它牺牲了一定的diversity，来换取生成图片的逼真性。这种思想提出来之后，后续有人思考不用classifier来当作引导信号，而是用CLIP来进行引导，这样文本和图像就可以联系起来了，不再用梯度，而是用文本来引导引导diffusion的采样和生成。所有的引导都是目标函数中的y，也就是输入不只是和t，同时还有一个condition（y），至于是什么就看你往里加入什么。</p>
<p>但是Classifier guided diffusion这类方法也有一个缺陷，就是：必须用另外一个模型来进行guidance，要么是pre-trained，要么是再训练一个，这样的话成本较高且不可控。因此就出现了后续的Classifier free guidance方法，GLIDE、DALLE·2、Imagen都是基于这类方法的思想。Classifier free guidance方法不想用之前的那种guidance，而是希望找到另外一种指导信号。它的思想就是在训练时同时做两个输出，一个有条件的，一个没条件的，最后就会知道二者的差距是多少，这样在测试时，在没有条件引导的情况下，也可以推测出有条件引导的输出是多少。但是这种训练成本也很高，同时要有两个输出。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183944101.png" alt="image.png"></p>
<p>GLIDE采用了classifier free guidance的扩散模型，可以实现很好的图像生成。OpenAI也因此摒弃了DALL·E用VQ-VAE的思路，在DALL·E 2中转而使用扩散模型来进行图像生成，也就是基于GLIDE，并在其之前加入了prior网络，以及一些层级生成的技巧等。</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><strong>像CLIP这些对比学习的模型已经可以学习到很稳健的图像特征，既能捕捉语义信息，又能捕捉风格信息</strong>。这种良好特征只用来做分类就很可惜，因此为了借助于这种良好特征来完成图像生成任务，我们提出了一个<strong>两阶段的模型：a prior （给定一个文本，先用现成的CLIP模型生成对应的textual embedding，然后接下来用这个textual embedding生成image embedding的过程就叫做prior），and a decoder（基于image embedding生成图像）</strong>。此外，由于是text-to-image任务，因此可以基于CLIP，可以实现zero-shot。</p>
<p>作者发现： 显式地生成图像特征的这种方式（就是先从文本生成image embedding，再将其解码为Image），可以很显著地提升生成图像的diversity（多样性）。</p>
<h1 id="DALL·E-2-模型解读"><a href="#DALL·E-2-模型解读" class="headerlink" title="DALL·E 2 模型解读"></a>DALL·E 2 模型解读</h1><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>对于一段话，DALLE 2是能够捕捉其中的层次关系，并在画图时也考虑进来的。比如给出下面这句话，DALLE 2 生成的图片，是真的把熊画在滑板之上了，也就是说捕捉到并解码出目标之间的关联关系的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110183953965.png" alt="image.png"></p>
<p>在DALLE 2这篇论文里，CLIP这个模型是锁住的，只是被用，没有被训练。DALLE 2模型的整体框架如下，虚线之上是CLIP 模型，虚线之下才是DALLE 2模型。unCLIP是指：CLIP的目的是从文本、图像中获取特征，本文的方法是为了从特征中还原出图像，因此叫unCLIP。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184000723.png" alt="image.png"></p>
<p>DALLE 2的训练数据是图像文本对(x, y)，过程分以下两个阶段：</p>
<ul>
<li><p><strong>Prior阶段</strong>：<strong>对于一个文本x，经由fixed CLIP可以产生一个texual embedding（蓝色）</strong>和Image embedding（红色），产生的texual embedding经过 autoregressive或difussion的prior模型生成Image embedding，这一个阶段<strong>用刚才CLIP中生成的Image embedding对其做监督。</strong>这样做的目的是：等在做推理的时候，即使只有文本特征，也可以生成比较好的图像特征。</p>
</li>
<li><p><strong>Decoder阶段</strong>：用于从Image embedding中解码出图像。</p>
</li>
</ul>
<p>如果DALLE 2的输入是图像的话，会通过CLIP生成文本特征，然后再经过prior、decoder生成图像。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>DALL·E 2是将其子模块分开训练的，最后将这些训练好的子模块拼接在一起，最后实现由文本生成图像的功能。</p>
<p><strong>1. 训练CLIP，使其能够编码文本和对应图像</strong></p>
<p>这一步是与CLIP模型的训练方式完全一样的，目的是能够得到训练好的text encoder和img encoder。这么一来，文本和图像都可以被编码到相应的<a href="https://zhida.zhihu.com/search?content_id=205119088&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&zhida_source=entity">特征空间</a>。对应上图中的虚线以上部分。</p>
<p><strong>2. 训练prior，使文本编码可以转换为图像编码</strong></p>
<p>论文中对于该步骤作用的解释为：</p>
<blockquote>
<p>A prior $P(z_i|y)$that produces CLIP image embeddings$z_i$conditioned on captions$y$.</p>
</blockquote>
<p>实际的训练过程为：将CLIP中训练好的text encoder拿出来，输入文本$y$，得到文本编码$z_t$。同样的，将CLIP中训练好的img encoder拿出来，输入图像$x$得到图像编码$z_i$。我们希望prior能从$z_t$获取相对应的$z_i$。假设$z_t$经过prior输出的特征为$z_i’$，那么我们自然希望$z_i’$与$z_i$越接近越好，这样来更新我们的prior模块。最终训练好的prior，将与CLIP的text encoder串联起来，它们可以根据我们的输入文本$y$生成对应的图像编码特征$z_i$了。关于具体如何训练prior，可以精度一下原文，作者使用了<a href="https://zhida.zhihu.com/search?content_id=205119088&content_type=Article&match_order=1&q=%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95&zhida_source=entity">主成分分析法</a>PCA来提升训练的稳定性。</p>
<p>在DALL·E 2 模型中，作者团队尝试了两种先验模型：自回归式Autoregressive (AR) prior 和扩散模型Diffusion prior。实验效果上发现两种模型的性能相似，而因为扩散模型效率较高，因此最终选择了扩散模型作为prior模块。</p>
<p><strong>3. 训练decoder生成最终的图像</strong></p>
<p>论文中对于该步骤作用的解释为：</p>
<blockquote>
<p>A decoder$P(x|z_i,y)$ that produces images $x$ conditioned on CLIP image embeddings $z_i$ (and optionally text captions $y$ ).</p>
</blockquote>
<p>也就是说我们要训练decoder模块，从<a href="https://zhida.zhihu.com/search?content_id=205119088&content_type=Article&match_order=1&q=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&zhida_source=entity">图像特征</a>﻿$z_i$还原出真实的图像$x$，如下图左边所示。这个过程与<a href="https://zhida.zhihu.com/search?content_id=205119088&content_type=Article&match_order=1&q=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&zhida_source=entity">自编码器</a>类似，从中间特征层还原出输入图像，但又不完全一样。我们需要生成出的图像，只需要保持原始图像的显著特征就可以了，这样以便于多样化生成，例如下图右边的示例。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184009679.png" alt="image.png"></p>
<p>DALL-E 2使用的是改进的GLIDE模型 [2]。这个模型可以根据CLIP图像编码的$z_i$，还原出具有相同与$x$有相同语义，而又不是与$x$完全一致的图像。</p>
<h2 id="推理过程（由文本生成图像过程）"><a href="#推理过程（由文本生成图像过程）" class="headerlink" title="推理过程（由文本生成图像过程）"></a>推理过程（由文本生成图像过程）</h2><p>经过以上三个步骤的训练，已经可以完成DALL·E 2预训练模型的搭建了。我们这事丢掉CLIP中的img encoder，留下CLIP中的text encoder，以及新训练好的prior和decoder。这么一来流程自然很清晰了：由text encoder将文本进行编码，再由prior将文本<a href="https://zhida.zhihu.com/search?content_id=205119088&content_type=Article&match_order=1&q=%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2&zhida_source=entity">编码转换</a>为图像编码，最后由decoder进行解码生成图像。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184021236.png" alt="image.png"></p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>DALL·E 2的几种应用场景：</p>
<ul>
<li><strong>text-to-image</strong></li>
</ul>
<p>DALL·E 2可以生成各式各样的沙发，变换颜色、样式、位置</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184027705.png" alt="image.png"></p>
<ul>
<li><strong>image-to-image</strong></li>
</ul>
<p>给定一个图像，可以生成很多风格类似的图像（整体语义信息不变）：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184033514.png" alt="image.png"></p>
<ul>
<li><strong>通过对两个图像的特征做内插，生成新图像</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184039857.png" alt="image.png"></p>
<ul>
<li><strong>通过对两个文本的特征做内插，生成新图像</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184046392.png" alt="image.png"></p>
<h1 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h1><ul>
<li><strong>DALL-E 2不能很好的将attribute绑定到 object 上。</strong></li>
</ul>
<p>比如下面这个图，object就是方块，属性就是颜色。给一句话：“a red cube on top of a blue cube”，GLIDE的效果要比DALL-E 2好很多。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184054203.png" alt="image.png"></p>
<p>作者解释很可能是使用CLIP的原因。虽然使用CLIP后可以是图像和文本的联系更紧密，这使得更容易去做文本生成图像的任务。但是CLIP模型学习的时候，只是考虑相似性，它只是去找红方块、蓝方块来把相似性提升到最高就行了，但是CLIP模型不了解”on top of”这种东西，不了解什么叫做“上下左右”，它从头到尾都在找物体间的相似性。</p>
<p>因此在做CLIP特征做下游任务的时候，就不能很好的区分object和其对应的attribute。</p>
<p>此外，在对图片进行重建时，decoder会将object的attribute混淆。</p>
<ul>
<li><strong>直接生成文字，效果很差</strong></li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110184100962.png" alt="image.png"></p>
<p>作者解释可能是使用BPE编码的原因，这种是词根词缀编码。但可能还有其他原因。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p>﻿<a href="https://blog.csdn.net/DUDUDUTU/article/details/129438597">DALL·E 2 论文阅读笔记_dalle论文-CSDN博客</a>﻿</p>
</li>
<li><p>﻿<a href="https://zhuanlan.zhihu.com/p/526438544">https://zhuanlan.zhihu.com/p/526438544</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>AIGC</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT系列</title>
    <url>/2025/01/09/GPT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p>GPT整体的模型架构都是以Transformer的解码器为模块进行堆叠而成。主要的创新点集中在模型训练策略，还有就是讲究一个大力出奇迹。</p>
<h2 id="论文时间轴"><a href="#论文时间轴" class="headerlink" title="论文时间轴"></a>论文时间轴</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170449688.png" alt="image.png"></p>
<span id="more"></span>

<p><strong>GPT</strong>：Transformer解码器，在没有标号的大量的文本数据上，训练一个语言模型，来获得预训练模型，后续在子任务上做微调，得到每一个任务所用的分类器。</p>
<p><strong>BERT</strong>：Transformer编码器，收集了一个更大的数据集，用来做预训练，效果比GPT好。BERT有两个模型，BERT-base模型与GPT模型参数相差不大，BERT-Large比BERT-base模型大。</p>
<blockquote>
<p>Transformer和BERT来自Google，想要解决的问题都比较小；Transformer就想解决机器翻译这样的问题，从一个序列到另外一个序列；BERT想把计算机视觉中成熟的那一套预训练模型应用到NLP中。在同样大小的模型上，BERT的效果是要好于GPT的。所以，后续的工作，非常愿意使用BERT，而不是GPT。</p>
</blockquote>
<p><strong>GPT-2</strong>：原作者吸取教训，收集更大的数据集，训练了一个更大的模型（15亿参数），GPT-2的模型比BERT-large要大。继续使用Transformer的解码器，发现非常适合做Zero Shot，步子跨的太大，效果上不是那么好。</p>
<p><strong>GPT-3</strong>：GPT-3对GPT-2的改进就是数据和模型都大了100倍（1750亿参数）。大力出奇迹，效果非常惊艳。几乎没有什么团队去复现结果。</p>
<p><strong>InstructGPT</strong> ：使用了指示学习（Instruction Learning）和人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）来指导模型的训练。<strong>ChatGPT</strong> (也被称为<strong>GPT3.5</strong>)就是使用了InstructGPT的模型结构和训练方法。</p>
<p><strong>GPT-4</strong>：改进了 GPT-3.5 的架构，参数数量显著增加（具体参数量未公开，传言有1.8万亿参数），并引入了更多的训练数据和新的优化方法。这使得 GPT-4.0 在处理复杂任务和理解更细微的上下文时表现更好。</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>论文标题：“Improving Language Understanding by Generative Pre-Training”，2018.6.</p>
<p>在自然语言处理领域，有很多各式各样的的任务，如问答，文本分类等。然而，现有的无标签文本非常多，而有标签的文本很少，这使得在这些有标签文本训练一个好的分辨模型很难，因为数据集太少。因此GPT第一个版本主要就是为了解决这个问题而提出的一套针对语言模型的预训练方法，使得大量的无标签数据能够被使用，并且对预训练好的模型加以微调使其适用于许多的下游任务。在微调时，构造与子任务相关的输入，从而之只需要很少改变模型架构。</p>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>如何利用无标签的数据，当时最成功的还是词嵌入模型（Word Embeddings）。</p>
<p>使用无标记数据时，需要的两个困难：</p>
<ul>
<li><p>损失函数设计困难：不清楚什么样的优化目标对文本有效</p>
</li>
<li><p>特征迁移困难：怎么样把学习到的问题表示，传递到下游的子任务上；没有一种表示，能够迁移到所有子任务上</p>
</li>
</ul>
<p>提出一个半监督的方法（预训练+微调）：在没有标号的数据上，训练一个比较大的语言模型，然后再再子任务上微调。</p>
<ul>
<li>半监督学习：有一些标记的数据，但还有大量没有标记的相似数据，怎么把这些没有标记的数据用过来。</li>
</ul>
<p>后续的BERT、GPT的方法，预训练 + 微调，不再叫做半监督学习，而叫做自监督学习。</p>
<p>GPT架构使用Transformer块，相比于RNN，在做迁移学习时，Transformer块学到的特征更加稳健。作者猜测Transformer里面有更结构化的记忆，使得能够处理更长的问题信息，从而能偶抽取出句子层面、段落层面的语义信息。在迁移时，使用任务相关的输入表示。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="无监督预训练：预训练"><a href="#无监督预训练：预训练" class="headerlink" title="无监督预训练：预训练"></a>无监督预训练：预训练</h4><ul>
<li><p>什么最大似然函数<br>  GPT 的目标是最大化以下联合概率：</p>
<p>  ﻿$P(X)&#x3D;P(x1,x2,…,xT)&#x3D;P(x1)⋅P(x2∣x1)⋅P(x3∣x1,x2)⋯P(xT∣x1,x2,…,xT−1)$﻿</p>
<p>  最大似然估计的目标是通过选择模型参数 $\theta$ 来最大化训练数据的似然函数 $L(\theta)$：</p>
<p>  ﻿$L(\theta) &#x3D; \prod_{t&#x3D;1}^{T} P(x_t | x_1, x_2, \dots, x_{t-1}; \theta)$﻿</p>
<p>  为了方便计算，通常将这个似然函数取对数，转化为对数似然（Log-Likelihood）：</p>
<p>  ﻿$\log L(\theta) &#x3D; \sum_{t&#x3D;1}^{T} \log P(x_t | x_1, x_2, \dots, x_{t-1}; \theta)$</p>
</li>
</ul>
<p>﻿</p>
<p><strong>GPT的目标函数</strong></p>
<p>给定一个未监督的语料信息$\mathcal{U}&#x3D;\left{u_{1}, \ldots, u_{n}\right}$，使用标准的语言模型，使下面这个似然函数最大化：</p>
<p>﻿$L_{1}(\mathcal{U})&#x3D;\sum_i \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)$﻿</p>
<p>其中，_k_为上下文窗口大小，Θ为模型参数。</p>
<p>简单来说就是根据上文的k个单词，预测下一个最大概率的单词$u_i$﻿</p>
<p>使用多层Transformer decoder块作为语言模型（标准的语言模型，根据已有的词进行下一个词的预测，不能使它看到所有的词，所以只有解码器）。</p>
<p>模型输入输出如下所示</p>
<p>﻿$\begin{aligned} h_{0} &amp; &#x3D;U W_{e}+W_{p} \ h_{l} &amp; &#x3D;\operatorname{transformer_ block}\left(h_{l-1}\right) \forall i \in[1, n] \ P(u) &amp; &#x3D;\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)\end{aligned}$﻿</p>
<p>其中$U&#x3D;\left(u_{-k}, \ldots, u_{-1}\right)$为上下文tokens向量，𝑛为transformer的层数，𝑊为词嵌入矩阵维度，𝑊𝑝为位置编码矩阵。</p>
<h4 id="有监督微调"><a href="#有监督微调" class="headerlink" title="有监督微调"></a>有监督微调</h4><p>在得到预训练模型后，就使用有标签的数据进行微调。具体来说每一次我给你一个长为m的一个词的序列，然后告诉你这个序列它对应的标号是y，也就是每次给定一个序列预测他这个y。微调优化目标是最小化分类目标函数。</p>
<p>NLP领域四大常见的应用：</p>
<ul>
<li><p>分类：给定一段话或文本，判断所属标号；如用户的评价是正面还是负面的。一段文本，在前面加入开始词元[Start]，在后面加入抽取词元[Extract]，做成一个序列，放入Transformer解码器中，模型对最后一个词抽取的特征[Extract]放入线性层进行分类。</p>
</li>
<li><p>蕴含：给一段话，再给一个假设，判断前面一段话是否蕴含提出的假设。如文本：a给b送一朵玫瑰；假设：a喜欢b。判断前面文本是否支持假设。即两端文本做三分类问题，支持，不支持，既不支持也不反对。将两端问题串成一个序列，使用开始符，分隔符，抽取符。</p>
</li>
<li><p>相似：判断两段文字是不是相似。如一个搜索词与文档是不是相似，或两个文档相似去重。相似是对称的，a和b相似，则b和a也相似。所以，做了两个序列，两个序列的文字顺序不同，再使用开始符，分隔符，抽取符。得到两段输出后，经过Transformer，在做加号，最后经过线性层输出，得到是相似还是不是相似的二分类问题。</p>
</li>
<li><p>多选题：问一个问题，给几个答案，选择最可能的几个答案。如果有n个答案，构造n个序列，前面是问题，每一个答案作为第二个序列；每个序列进入Transformer块，最后经过线性层，输出答案的置信度；最后做一个softmax。</p>
</li>
</ul>
<p>复用预训练的Transformer的结构，加一个线性层，不同的任务需要不同的输入。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170604083.png" alt="image.png"></p>
<h2 id="GPT2"><a href="#GPT2" class="headerlink" title="GPT2"></a>GPT2</h2><p>论文标题：Language Models are Unsupervised Multitask Learners，2019</p>
<p>自从GPT提出后，Google紧随其后提出了BERT预训练模型，采用更大的模型和更大的数据，在各方面效果都要优于GPT。作为竞争对手，GPT当然是要反击的，那怎么做呢？如果单纯加大模型于增加数据量，或许能击败BERT，但是却少了些新意，因此GPT2从另外一个角度，除了加大模型和数据量，还引入了zero-shot设定，就是在做下游任务是，不需要下游任务的任何标签信息，也不需要重新训练一个模型，即在更难的一个设定上体现他的一个新意度。</p>
<h3 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h3><p>现在主流的机器学习系统训练方式为：一个任务收集一个数据集，然后再上面做模型训练和预测，因现在模型泛化性能不是很好。</p>
<p>多任务学习：训练一个模型时，同时看多个数据集，通过多个损失函数，来达到一个模型能够在多个任务上都能用。但是NLP中使用的不多，主要使用的还是预训练+微调方式。但还是有两个问题：第一个是对每一个下游任务，需要重新训练模型；二是需要收集有标号的数据才行。这样导致，拓展到新的任务上，还是有成本的。</p>
<p><strong>GPT-2还是在做语言模型，在到下游任务时，会使用zero-shot的设定（不需要下游任务的标注信息，不引入模型没有见过的特殊符号），这样训练一个模型，任何地方都可以用。</strong></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="GPT-1和GPT-2区别"><a href="#GPT-1和GPT-2区别" class="headerlink" title="GPT-1和GPT-2区别"></a>GPT-1和GPT-2区别</h4><ul>
<li>GPT-1时，根据不同的下游任务，会调整输入信息，会加入开始符、分隔符、结束符等信息，然后在使用有标记的数据进行微调，让模型去认识这些符号。而GPT-2做下游任务时，不再加入开始符、分隔符、结束符等模型未见过的信息，而是采用zero-shot的设定。GPT-2下游任务模型的输入，和预训练时，模型看到的输入是一样的。</li>
</ul>
<p>例如：</p>
<ul>
<li><p>英语翻译为法语：translate to french, english text, french text</p>
</li>
<li><p>做阅读理解：answer the question, document, question, answer</p>
</li>
</ul>
<p>这个提示符后面叫做Prompt。</p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>BERT训练数据：Wikipedia</p>
<p>Common Crawl项目：一个公开的爬虫项目，不断抓取网页放在AWS上，是能下载到最大的文本数据集，TB级别的数据量。但作者认为这个不好用，因为信噪比比较低，抓取的网页，较多是没有信息的网页。GPT-2使用Reddit里面的数据，选取最近3词评论以上的数据，得到4500万分链接，将数据抽取出来，得到一个数据集，约800万文本，40GB的文字。</p>
<h4 id="GPT-2模型大小"><a href="#GPT-2模型大小" class="headerlink" title="GPT-2模型大小"></a>GPT-2模型大小</h4><p>GPT2也是基于Transformer解码器的架构，作者设计了4种大小的模型，参数结构如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170614852.png" alt="image.png"></p>
<p>可以看到，最大模型的参数量已经去到了15个亿。还有一个细节就是GPT2调整了transformer解码器结构：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。</p>
<h4 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h4><p>采用预训练+zero-shot的训练范式。为实现zero-shot，GPT2在做下游任务时，输入就不能像GPT那样在构造输入时加入开始、中间和结束的特殊字符，因为这些特殊字符是模型在预训练时没有见过的。正确的输入应该和预训练模型看到的文本一样，更像一个自然语言。比如在做机器翻译时，直接可以输入“请将下面一段英文翻译成法语，英文文本”，由于在训练时可能已经存在很多这样的翻译文本样例，因此模型就可以实现输出一段法语。</p>
<h2 id="GPT3"><a href="#GPT3" class="headerlink" title="GPT3"></a>GPT3</h2><p>论文题目：Language Models are Few-Shot Learners，2020</p>
<p>GPT-3：自回归语言模型，有1750亿个可学习参数，比之前非稀疏模型在可学习参数上要大10倍。GPT-3作用到子任务上，不做任何的梯度更新或是微调；GPT-3可以生成一些人类难以区分的文章。</p>
<h3 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h3><p>GPT2实验采用了zero-shot设定，在新意度上很高，但是有效性却比较低。而GPT3则是尝试解决GPT2的有效性，因此回到了GPT提到的Few-shot设置，即模型在做下游任务时，可以看到一些任务的样例，而不是像GPT2那样啥样例都不给。此外，GPT3还是只采用无监督预训练方式，那么传统的二阶段训练方式（预训练+微调）有什么问题？二阶段训练方式在预训练好一个模型后，还需要一个与任务相关的数据集，以及跟任务相关的微调方式。去除这种方式是可取的，有以下几个原因：</p>
<ul>
<li><p>微调需要一个较大的有标签数据，对于一些如问答型任务，做标签是很困难的；</p>
</li>
<li><p>当一个样本没有出现在数据分布里是，微调模型的泛化能力不一定好，即尽管微调效果好，也不一定说明预训练的模型泛化能力好，因为极有可能微调是过拟合了预训练的训练数据，或者说预训练训练数据和下游任务数据有一定重合性，所以效果会好一点；</p>
</li>
<li><p>以人类角度来阐述为什么不用微调，就是说人类做任务不需要一个很大的数据集进行微调，比如一个人有一定的语言功底，让你去做一个别的事情，可能就是告诉你怎么做并提供几个样例就可以了，GPT3就是采用一样的思想。</p>
</li>
</ul>
<p>总的来说，GPT3就是一个参数特别大，效果也很好的一个模型。</p>
<p><strong>多任务预训练（上下文训练</strong>，类似于 meta-learning 元学习的想法，元学习的核心思想在于通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果<strong>）</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170624378.png" alt="image.png"></p>
<p>模型评估方式（3种）：</p>
<ul>
<li><p>few-shot learning：语境学习允许尽可能多的示例活动将适合模型的上下文窗口(通常10 - 100)</p>
</li>
<li><p>one-shot learning：只提供一个示例，如英语翻译德语时，只提供第一个词怎么翻译，后续让模型自己翻译</p>
</li>
<li><p>zero-shot：一个示例样本都没有</p>
</li>
</ul>
<p>在三个设定下，模型的学习区别，x轴为语言模型的大小，其中虚线是每个子任务，做平均变成了实线。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170634563.png" alt="image.png"></p>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><h4 id="训练方式-1"><a href="#训练方式-1" class="headerlink" title="训练方式"></a>训练方式</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170649230.png" alt="image.png"></p>
<ul>
<li><p>Fine-tuning：训练完成预训练模型后，在每一个子任务上提供训练样本；微调对数据量的要求少于从0开始训练；</p>
</li>
<li><p>Zero-shot：任务描述和prompt之间没有任何样本</p>
</li>
<li><p>One-shot：任务描述和prompt之前，插入一个样本进来。样本只做预测，不做训练，模型在前向推理时，使用注意力机制，处理比较长的信息，从中间抽取出有用的信息。</p>
</li>
<li><p>Few-shot：任务描述和prompt之前，插入多个样本进来。多个不一定有用，可能模型不能处理很长的数据。</p>
</li>
</ul>
<p>几种训练方式简单概括如下：</p>
<ul>
<li><p>fine-tuning：预训练 + 微调计算loss更新梯度，然后预测。会更新模型参数</p>
</li>
<li><p>zero-shot：预训练 + task description + prompt，直接预测。不更新模型参数</p>
</li>
<li><p>one-shot：预训练 + task description + example + prompt，预测。不更新模型参数</p>
</li>
<li><p>few-shot：预训练 + task description + examples + prompt，预测。不更新模型参数</p>
</li>
</ul>
<h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>GPT-3的模型和GPT-2的模型是一样的，稍微有点改动，把transformer换成了Sparse Transformer中的结构，并设计8个不同大小的模型。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170702577.png" alt="image.png"></p>
<ul>
<li><p>整体：GPT3模型偏扁</p>
</li>
<li><p>Batch Size：使用相对比较大的批量大小，计算性能更好，每台机器的并行度更高，通讯量变低，降低批量里的噪音分布式比较好，小的模型批量大小更容易过拟合—些。</p>
</li>
<li><p>过拟合：模型越来越大的时候过拟合没有那么的严重，搜索范围更广，可能存在一个比较简单的模型架构，SDG可以帮助找到那个模型，使泛化精度更好一些。</p>
</li>
<li><p>学习率模型批量大小增大学习率下降。</p>
</li>
</ul>
<p><strong>Sparse Transformer</strong></p>
<ul>
<li><p>Self-Attention：每个 token 之间两两计算 attention，复杂度$O(n²)$﻿</p>
</li>
<li><p>Sparse Attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 $O(n∗logn)$﻿</p>
</li>
<li><p>Sparse Attention 除了相对距离不超过k以及相对距离为$k，2k，3k，…$的 token，其他所有token的注意力都设为0：</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170712746.png" alt="image.png"></p>
<h2 id="InstructGPT-ChatGPT-GPT3-5"><a href="#InstructGPT-ChatGPT-GPT3-5" class="headerlink" title="InstructGPT&#x2F;ChatGPT&#x2F;GPT3.5"></a>InstructGPT&#x2F;ChatGPT&#x2F;GPT3.5</h2><p>InstructGPT 使用了指示学习（Instruction Learning）和人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）来指导模型的训练。ChatGPT (也被称为GPT3.5) 和InstructGPT在模型结构，训练方式上都完全一致，它们不同的仅仅是采集数据的方式上有所差异。</p>
<p>预训练模型自诞生之始，一个备受诟病的问题就是预训练模型的偏见性。因为预训练模型都是通过海量数据在超大参数量级的模型上训练出来的，对比完全由人工规则控制的专家系统来说，预训练模型就像一个黑盒子。没有人能够保证预训练模型不会生成一些包含种族歧视，性别歧视等危险内容，因为它的几十GB甚至几十TB的训练数据里几乎肯定包含类似的训练样本。这也就是InstructGPT和ChatGPT的提出动机，论文中用3H概括了它们的优化目标：</p>
<ul>
<li><p>有用的（Helpful）;</p>
</li>
<li><p>可信的（Honest）;</p>
</li>
<li><p>无害的（Harmless）。</p>
</li>
</ul>
<h3 id="指示学习（Instruct-Learning）和提示（Prompt-Learning）学习"><a href="#指示学习（Instruct-Learning）和提示（Prompt-Learning）学习" class="headerlink" title="指示学习（Instruct Learning）和提示（Prompt Learning）学习"></a>指示学习（Instruct Learning）和提示（Prompt Learning）学习</h3><p>指示学习是谷歌Deepmind的Quoc V.Le团队在2021年的一篇名为《Finetuned Language Models Are Zero-Shot Learners》文章中提出的思想。指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。不同的是Prompt是激发语言模型的<strong>补全能力</strong>，例如根据上半句生成下半句，或是完形填空等。Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。我们可以通过下面的例子来理解这两个不同的学习方式：</p>
<ol>
<li><p>提示学习：给女朋友买了这个项链，她很喜欢，这个项链太____了。</p>
</li>
<li><p>指示学习：这句话的情感是非常正向的：给女朋友买了这个项链，她很喜欢。</p>
</li>
</ol>
<p>指示学习的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示学习都是针对一个任务的。泛化能力不如指示学习。我们可以通过下图来理解微调，提示学习和指示学习。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170721839.png" alt="image.png"></p>
<h3 id="人类反馈的强化学习-（RLHF）"><a href="#人类反馈的强化学习-（RLHF）" class="headerlink" title="人类反馈的强化学习 （RLHF）"></a>人类反馈的强化学习 （RLHF）</h3><p>因为训练得到的模型并不是非常可控的，模型可以看做对训练集分布的一个拟合。那么反馈到生成模型中，训练数据的分布便是影响生成内容的质量最重要的一个因素。有时候我们希望模型并不仅仅只受训练数据的影响，而是人为可控的，从而保证生成数据的有用性，真实性和无害性。论文中多次提到了对齐（Alignment）问题，我们可以理解为模型的输出内容和人类喜欢的输出内容的对齐，人类喜欢的不止包括生成内容的流畅性和语法的正确性，还包括生成内容的有用性、真实性和无害性。</p>
<p>我们知道强化学习通过奖励（Reward）机制来指导模型训练，奖励机制可以看做传统模型训练机制的损失函数。奖励的计算要比损失函数更灵活和多样（AlphaGO的奖励是对局的胜负），这带来的代价是奖励的计算是不可导的，因此不能直接拿来做反向传播。强化学习的思路是通过对奖励的大量采样来拟合损失函数，从而实现模型的训练。同样人类反馈也是不可导的，那么我们也可以将人工反馈作为强化学习的奖励，基于人类反馈的强化学习便应运而生。</p>
<p>RLHF最早可以追溯到Google在2017年发表的《Deep Reinforcement Learning from Human Preferences》，它通过人工标注作为反馈，提升了强化学习在模拟机器人以及雅达利游戏上的表现效果。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170730546.png" alt="image.png"></p>
<p>InstructGPT&#x2F;ChatGPT中还用到了强化学习中一个经典的算法：OpenAI提出的最近策略优化（Proximal Policy Optimization，PPO）<a href="https://zhuanlan.zhihu.com/p/590311003#ref_7">[7]</a>。PPO算法是一种新型的Policy Gradient算法，Policy Gradient算法对步长十分敏感，但是又难以选择合适的步长，在训练过程中新旧策略的的变化差异如果过大则不利于学习。PPO提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。其实TRPO也是为了解决这个思想但是相比于TRPO算法PPO算法更容易求解。</p>
<h3 id="InstructGPT-ChatGPT原理解读"><a href="#InstructGPT-ChatGPT原理解读" class="headerlink" title="InstructGPT&#x2F;ChatGPT原理解读"></a>InstructGPT&#x2F;ChatGPT原理解读</h3><p>有了上面这些基础知识，我们再去了解InstructGPT和ChatGPT就会简单很多。简单来说，InstructGPT&#x2F;ChatGPT都是采用了<strong>GPT-3</strong>的网络结构，通过<strong>指示学习</strong>构建训练样本来训练一个反应预测内容效果的奖励模型（RM），最后通过这个奖励模型的打分来指导强化学习模型的训练。InstructGPT&#x2F;ChatGPT的训练流程如图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170739026.png" alt="image.png"></p>
<p>InstructGPT的计算流程：（1）有监督微调（SFT）；（2）奖励模型（RM）训练；（3）通过PPO根据奖励模型进行强化学习。</p>
<p>从上图中我们可以看出，InstructGPT&#x2F;ChatGPT的训练可以分成3步，其中第2步和第3步是的奖励模型和强化学习的SFT模型可以反复迭代优化。</p>
<ol>
<li><p>根据采集的SFT数据集对GPT-3进行有监督的微调（Supervised FineTune，SFT）；</p>
</li>
<li><p>收集人工标注的对比数据，训练奖励模型（Reword Model，RM）；</p>
</li>
<li><p>使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型。</p>
</li>
</ol>
<p>我们将分别介绍InstructGPT&#x2F;ChatGPT的数据集采集和模型训练两个方面的内容。</p>
<h4 id="数据集采集"><a href="#数据集采集" class="headerlink" title="数据集采集"></a>数据集采集</h4><p>InstructGPT&#x2F;ChatGPT的训练分成3步，每一步需要的数据也有些许差异，下面我们分别介绍它们。</p>
<h5 id="SFT数据集"><a href="#SFT数据集" class="headerlink" title="SFT数据集"></a>SFT数据集</h5><p>SFT数据集是用来训练第1步有监督的模型，即使用采集的新数据，按照GPT-3的训练方式对GPT-3进行微调。因为GPT-3是一个基于提示学习的生成模型，因此SFT数据集也是由提示-答复对组成的样本。SFT数据一部分来自使用OpenAI的PlayGround的用户，另一部分来自OpenAI雇佣的40名标注工（labeler）。并且他们对labeler进行了培训。在这个数据集中，标注工的工作是根据内容自己编写指示，并且要求编写的指示满足下面三点：</p>
<ul>
<li><p><strong>简单任务</strong>：labeler给出任意一个简单的任务，同时要确保任务的多样性；</p>
</li>
<li><p><strong>Few-shot任务</strong>：labeler给出一个指示，以及该指示的多个查询-响应对；</p>
</li>
<li><p><strong>用户相关的</strong>：从接口中获取用例，然后让labeler根据这些用例编写指示。</p>
</li>
</ul>
<h5 id="RM数据集"><a href="#RM数据集" class="headerlink" title="RM数据集"></a>RM数据集</h5><p>RM数据集用来训练第2步的奖励模型，我们也需要为InstructGPT&#x2F;ChatGPT的训练设置一个奖励目标，要尽可能全面且真实的对齐我们需要模型生成的内容。很自然的，我们可以通过人工标注的方式来提供这个奖励，通过人工对可以给那些涉及偏见的生成内容更低的分从而鼓励模型不去生成这些人类不喜欢的内容。InstructGPT&#x2F;ChatGPT的做法是先让模型生成一批候选文本，让后通过labeler根据生成数据的质量对这些生成内容进行排序。</p>
<h5 id="PPO数据集"><a href="#PPO数据集" class="headerlink" title="PPO数据集"></a>PPO数据集</h5><p>InstructGPT的PPO数据没有进行标注，它均来自GPT-3的API的用户。既又不同用户提供的不同种类的生成任务，其中占比最高的包括生成任务（45.6%），QA（12.4%），头脑风暴（11.2%），对话（8.4%）等。</p>
<h5 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h5><p>因为InstructGPT&#x2F;ChatGPT是在GPT-3基础上做的微调，而且因为涉及了人工标注，它们数据总量并不大。</p>
<p>论文的附录A对数据的分布进行了更详细的讨论，这里我列出几个可能影响模型效果的几项：</p>
<ul>
<li><p>数据中96%以上是英文，其它20个语种例如中文，法语，西班牙语等加起来不到4%，这可能导致InstructGPT&#x2F;ChatGPT能进行其它语种的生成，但效果应该远不如英文；</p>
</li>
<li><p>提示种类共有9种，而且绝大多数是生成类任务，可能会导致模型有覆盖不到的任务类型；</p>
</li>
<li><p>40名外包员工来自美国和东南亚，分布比较集中且人数较少， InstructGPT&#x2F;ChatGPT的目标是训练一个价值观正确的预训练模型，它的价值观是由这40个外包员工的价值观组合而成。而这个比较窄的分布可能会生成一些其他地区比较在意的歧视，偏见问题。</p>
</li>
</ul>
<p>此外，ChatGPT的博客中讲到ChatGPT和InstructGPT的训练方式相同，不同点仅仅是它们采集数据上有所不同，但是并没有更多的资料来讲数据采集上有哪些细节上的不同。考虑到ChatGPT仅仅被用在对话领域，这里我猜测ChatGPT在数据采集上有两个不同：1. 提高了对话类任务的占比；2. 将提示的方式转换Q&amp;A的方式。当然这里也仅仅是猜测，更准确的描述要等到ChatGPT的论文、源码等更详细的资料公布我们才能知道。</p>
<h4 id="训练任务"><a href="#训练任务" class="headerlink" title="训练任务"></a>训练任务</h4><p>我们刚介绍到InstructGPT&#x2F;ChatGPT有三步训练方式。这三步训练会涉及三个模型：SFT，RM以及PPO，下面我们详细介绍它们。</p>
<h5 id="有监督微调（SFT）"><a href="#有监督微调（SFT）" class="headerlink" title="有监督微调（SFT）"></a>有监督微调（SFT）</h5><p>这一步的训练和GPT-3一致，而且作者发现让模型适当过拟合有助于后面两步的训练。</p>
<h5 id="奖励模型（RM）"><a href="#奖励模型（RM）" class="headerlink" title="奖励模型（RM）"></a>奖励模型（RM）</h5><p>因为训练RM的数据是一个labeler根据生成结果排序的形式，所以它可以看做一个回归模型。RM结构是将SFT训练后的模型的最后的嵌入层去掉后的模型。它的输入是prompt和Reponse，输出是奖励值。具体的讲，对弈每个prompt，InstructGPT&#x2F;ChatGPT会随机生成 K 个输出（ 4≤K≤9 ），然后它们向每个labeler成对的展示输出结果，也就是每个prompt共展示 CK2 个结果，然后用户从中选择效果更好的输出。在训练时，InstructGPT&#x2F;ChatGPT将每个prompt的 CK2 个响应对作为一个batch，这种按prompt为batch的训练方式要比传统的按样本为batch的方式更不容易过拟合，因为这种方式每个prompt会且仅会输入到模型中一次。</p>
<p>奖励模型的损失函数表示为式(1)。这个损失函数的目标是最大化labeler更喜欢的响应和不喜欢的响应之间的差值。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170755396.png" alt="image.png"></p>
<p>其中 rθ(x,y) 是提示 x 和响应 y 在参数为 θ 的奖励模型下的奖励值， yw 是labeler更喜欢的响应结果， yl 是labeler不喜欢的响应结果。 D 是整个训练数据集。</p>
<h5 id="强化学习模型（PPO）"><a href="#强化学习模型（PPO）" class="headerlink" title="强化学习模型（PPO）"></a>强化学习模型（PPO）</h5><p>强化学习和预训练模型是最近两年最为火热的AI方向之二，之前不少科研工作者说强化学习并不是一个非常适合应用到预训练模型中，因为很难通过模型的输出内容建立奖励机制。而InstructGPT&#x2F;ChatGPT反直觉的做到了这点，它通过结合人工标注，将强化学习引入到预训练语言模型是这个算法最大的创新点。</p>
<p>PPO的训练集完全来自API。它通过第2步得到的奖励模型来指导SFT模型的继续训练。很多时候强化学习是非常难训练的，InstructGPT&#x2F;ChatGPT在训练过程中就遇到了两个问题：</p>
<ol>
<li><p>问题1：随着模型的更新，强化学习模型产生的数据和训练奖励模型的数据的差异会越来越大。作者的解决方案是在损失函数中加入KL惩罚项 βlog⁡(πϕRL(y∣x)&#x2F;πSFT(y∣x)) 来确保PPO模型的输出和SFT的输出差距不会很大。</p>
</li>
<li><p>问题2：只用PPO模型进行训练的话，会导致模型在通用NLP任务上性能的大幅下降，作者的解决方案是在训练目标中加入了通用的语言模型目标 γEx∼Dpretrain [log⁡(πϕRL(x))] ，这个变量在论文中被叫做PPO-ptx。</p>
</li>
</ol>
<p>综上，PPO的训练目标为</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250109170802276.png" alt="image.png"></p>
<h3 id="InstructGPT-ChatGPT的性能分析"><a href="#InstructGPT-ChatGPT的性能分析" class="headerlink" title="InstructGPT&#x2F;ChatGPT的性能分析"></a>InstructGPT&#x2F;ChatGPT的性能分析</h3><p>不可否认的是，InstructGPT&#x2F;ChatGPT的效果是非常棒的，尤其是引入了人工标注之后，让模型的“价值观”和的正确程度和人类行为模式的“真实性”上都大幅的提升。那么，仅仅根据InstructGPT&#x2F;ChatGPT的技术方案和训练方式，我们就可以分析出它可以带来哪些效果提升呢？</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li><p><strong>InstructGPT&#x2F;ChatGPT的效果比GPT-3更加真实</strong>：这个很好理解，因为GPT-3本身就具有非常强的泛化能力和生成能力，再加上InstructGPT&#x2F;ChatGPT引入了不同的labeler进行提示编写和生成结果排序，而且还是在GPT-3之上进行的微调，这使得我们在训练奖励模型时对更加真实的数据会有更高的奖励。作者也在TruthfulQA数据集上对比了它们和GPT-3的效果，实验结果表明甚至13亿小尺寸的PPO-ptx的效果也要比GPT-3要好。</p>
</li>
<li><p><strong>InstructGPT&#x2F;ChatGPT在模型的无害性上比GPT-3效果要有些许提升</strong>：原理同上。但是作者发现InstructGPT在歧视、偏见等数据集上并没有明显的提升。这是因为GPT-3本身就是一个效果非常好的模型，它生成带有有害、歧视、偏见等情况的有问题样本的概率本身就会很低。仅仅通过40个labeler采集和标注的数据很可能无法对模型在这些方面进行充分的优化，所以会带来模型效果的提升很少或者无法察觉。</p>
</li>
<li><p><strong>InstructGPT&#x2F;ChatGPT具有很强的Coding能力</strong>：首先GPT-3就具有很强的Coding能力，基于GPT-3制作的API也积累了大量的Coding代码。而且也有部分OpenAI的内部员工参与了数据采集工作。通过Coding相关的大量数据以及人工标注，训练出来的InstructGPT&#x2F;ChatGPT具有非常强的Coding能力也就不意外了。</p>
</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p><strong>InstructGPT&#x2F;ChatGPT会降低模型在通用NLP任务上的效果</strong>：我们在PPO的训练的时候讨论了这点，虽然修改损失函数可以缓和，但这个问题并没有得到彻底解决。</p>
</li>
<li><p><strong>有时候InstructGPT&#x2F;ChatGPT会给出一些荒谬的输出</strong>：虽然InstructGPT&#x2F;ChatGPT使用了人类反馈，但限于人力资源有限。影响模型效果最大的还是有监督的语言模型任务，人类只是起到了纠正作用。所以很有可能受限于纠正数据的有限，或是有监督任务的误导（只考虑模型的输出，没考虑人类想要什么），导致它生成内容的不真实。就像一个学生，虽然有老师对他指导，但也不能确定学生可以学会所有知识点。</p>
</li>
<li><p><strong>模型对指示非常敏感</strong>：这个也可以归结为labeler标注的数据量不够，因为指示是模型产生输出的唯一线索，如果指示的数量和种类训练的不充分的话，就可能会让模型存在这个问题。</p>
</li>
<li><p><strong>模型对简单概念的过分解读</strong>：这可能是因为labeler在进行生成内容的比较时，倾向于给给长的输出内容更高的奖励。</p>
</li>
<li><p><strong>对有害的指示可能会输出有害的答复</strong>：例如InstructGPT&#x2F;ChatGPT也会对用户提出的“AI毁灭人类计划书”给出行动方案（这个是因为InstructGPT&#x2F;ChatGPT假设labeler编写的指示是合理且价值观正确的，并没有对用户给出的指示做更详细的判断，从而会导致模型会对任意输入都给出答复。虽然后面的奖励模型可能会给这类输出较低的奖励值，但模型在生成文本时，不仅要考虑模型的价值观，也要考虑生成内容和指示的匹配度，有时候生成一些价值观有问题的输出也是可能的。</p>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>﻿<a href="https://dongnian.icu/paper_reading/2.5.GPT_GPT-2_GPT-3/index.html">https://dongnian.icu/paper_reading&#x2F;2.5.GPT_GPT-2_GPT-3&#x2F;index.html</a>﻿</p>
<p>﻿<a href="https://zhuanlan.zhihu.com/p/590311003">https://zhuanlan.zhihu.com/p/590311003</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA系列</title>
    <url>/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165618253.png" alt="image.png"></p>
<span id="more"></span>

<h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><ul>
<li><p><strong>Transformer模型的结构以及注意力机制的工作原理</strong>。</p>
</li>
<li><p><strong>Transformer模型是如何训练和推理的</strong>。</p>
</li>
<li><p><strong>线性代数</strong>：矩阵乘法，点积。</p>
</li>
<li><p><strong>复数</strong>：欧拉公式（非必需，了解更好）。</p>
</li>
</ul>
<h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><ul>
<li><p><strong>原始Transformer与LLaMA的架构差异</strong>。</p>
</li>
<li><p><strong>RMS Normalization</strong></p>
</li>
<li><p><strong>Rotary Positional Embeddings</strong></p>
</li>
<li><p><strong>KV-Cache</strong></p>
</li>
<li><p><strong>Multi-Query Attention</strong></p>
</li>
<li><p><strong>Grouped Multi-Query Attention</strong></p>
</li>
<li><p><strong>SwiGLU Activation Function</strong></p>
</li>
</ul>
<h1 id="Transformer-vs-LLaMA"><a href="#Transformer-vs-LLaMA" class="headerlink" title="Transformer vs LLaMA"></a>Transformer vs LLaMA</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165628374.png" alt="image.png"></p>
<ul>
<li><p><strong>llama只有encoder</strong>, 因为它基于next prediction token task训练, 只需要自注意力机制来预测下一个 token</p>
</li>
<li><p>**所有的归一化被移到了block之前(**Attention, Feed Forward)</p>
</li>
<li><p>llama不再使用Transformer的Postional Encoding, 而是<strong>使用Rotary Positional Embeddings, 并且只应用在Query, Key上</strong></p>
</li>
<li><p><strong>Attention层增加了KV cache, Grouped Multi-Query Attention</strong></p>
</li>
<li><p><strong>Feed Forward block中的激活函数从ReLU变为SwiGLU</strong></p>
</li>
</ul>
<h1 id="LLaMA1-LLaMA2模型参数量"><a href="#LLaMA1-LLaMA2模型参数量" class="headerlink" title="LLaMA1, LLaMA2模型参数量"></a>LLaMA1, LLaMA2模型参数量</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165639343.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165646305.png" alt="image.png"></p>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>句子通过分词器(Tokenizer)变成token(字典中的位置), token通过embedding层被映射到一个向量中(Transformer为512维, 而llama为4096维), 用来表示这个token的特征, 这些embedding是可学习的,是模型参数的一部分</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165655594.png" alt="image.png"></p>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="回顾神经网络的数学概念"><a href="#回顾神经网络的数学概念" class="headerlink" title="回顾神经网络的数学概念"></a><strong>回顾神经网络的数学概念</strong></h4><p>假设我们有一个线性层 nn.Linear(in_features&#x3D;3, out_features&#x3D;5, bias&#x3D;True), 输入为(10, 3). 这意味着我们有10个item, 每个item有3个feature.</p>
<p>对于这个带有bias的线性层, 我们有两个权重矩阵, 一个是大小为(5, 3)的W, 一个是大小为(1, 5)的b</p>
<p>如下图所示, item1的第一个输出feature的计算方法为$a_1 * w_1 + a_2 * w_2 + a_3 * w_3 + b_1$﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165721292.png"></p>
<p>我们有以下结论:</p>
<ul>
<li><p>神经元对数据项的输出取决于输入数据项的特征（以及神经元的参数）。</p>
</li>
<li><p>我们可以将一个神经元的输入视为前一个线性层的输出。</p>
</li>
<li><p><strong>如果前一层在经过梯度下降更新权重后，其输出发生了剧烈变化，那么下一层的输入也会发生剧烈变化，因此在下一步梯度下降时，下一层的权重也会被迫进行大幅调整。</strong></p>
</li>
<li><p><strong>这种神经网络内部节点（神经元）的分布发生变化的现象称为Internal Covariate Shift(内部协变量偏移)。我们希望避免这种情况，因为它会使网络训练变得更慢，迫使神经元因为前一层输出的剧烈变化而大幅调整其权重。</strong></p>
</li>
</ul>
<h4 id="回顾Transformer中的Layer-Normalization"><a href="#回顾Transformer中的Layer-Normalization" class="headerlink" title="回顾Transformer中的Layer Normalization"></a><strong>回顾Transformer中的Layer Normalization</strong></h4><ul>
<li><p>对于每个item,我们独立统计两个统计量,均值和方差</p>
</li>
<li><p>每个item都会用其归一化后的值进行更新，使其变成均值为0、方差为1的正态分布。</p>
</li>
<li><p>参数 gamma 和 beta 是可学习的参数，它们允许模型根据损失函数的需求“放大”每个特征的尺度或对特征进行平移。</p>
</li>
<li><p>和传统的batch norm相比, layer norm按行(即item)进行计算统计, 而batch norm按列(即feature)计算统计, 好处是可以更好的处理语言模型中,每次input的sequence length不同的情况</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165751368.png" alt="image.png"></p>
<h4 id="LLaMA使用的normalization-Root-Mean-Square-Normalization"><a href="#LLaMA使用的normalization-Root-Mean-Square-Normalization" class="headerlink" title="LLaMA使用的normalization: Root Mean Square Normalization"></a><strong>LLaMA使用的normalization: Root Mean Square Normalization</strong></h4><ul>
<li><p>RMS的作者认为, <strong>导致Layer norm起作用的更多的是由于re-scaling(除以方差), 而不是re-centering(减去均值)</strong>. 因此他们想要找到一个不依赖均值的统计量.</p>
</li>
<li><p><strong>RMS提出使用均方根归一化, 即每个item除以均方根,在乘以一个可学习的参数gamma, 如下图所示</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164739603.png" alt="image.png"></p>
<ul>
<li><strong>RMS只需要计算一个统计量, 因此计算量更少, 同时在实践中表现很好</strong></li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>﻿<a href="https://www.bilibili.com/video/BV1xR1RY9ECm">位置编码有什么用？简单讲解位置编码原理 + 源码解读（绝对 &#x2F; 相对 &#x2F; RoPE）_哔哩哔哩_bilibili</a>﻿</p>
<h4 id="回顾Transformer中的Positional-Encoding"><a href="#回顾Transformer中的Positional-Encoding" class="headerlink" title="回顾Transformer中的Positional Encoding"></a><strong>回顾Transformer中的Positional Encoding</strong></h4><ul>
<li><p><strong>我们希望每个词都能携带一些关于其在句子中位置的信息。</strong></p>
</li>
<li><p>我们希望模型将出现在彼此相邻的词视为“接近”，而将距离较远的词视为“远离”。</p>
</li>
<li><p>通过下图的公式, 计算出句子中每个词, 每一维embedding所对应的Positional Encoding,</p>
</li>
<li><p><strong>我们只需要计算一次位置编码，然后在训练或推理过程中都可以重复使用它</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165800937.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165809840.png" alt="image.png"></p>
<h4 id="Absolute-Positional-Encodings-v-s-Relative-Positional-Encodings"><a href="#Absolute-Positional-Encodings-v-s-Relative-Positional-Encodings" class="headerlink" title="Absolute Positional Encodings v.s. Relative Positional Encodings"></a>Absolute Positional Encodings v.s. Relative Positional Encodings</h4><ul>
<li><p><strong>绝对位置编码</strong>是固定的向量，它们被添加到一个token的embedding中，用来表示其在句子中的绝对位置。因此，它<strong>一次只处理一个token</strong>。你可以将其类比为地图上的（纬度，经度）对：地球上的每个点都有一个唯一的坐标对。绝对位置编码的优势在于它不依赖于序列中的其他元素，可以独立地表示每个位置的信息。</p>
</li>
<li><p><strong>相对位置编码</strong>则一次处理两个token，并在我们计算注意力时起作用。因为注意力机制捕捉的是两个词之间相关性的“强度”，相对位置编码则告诉注意力机制这两个词之间的距离。因此，<strong>给定两个token，我们创建一个向量来表示它们之间的距离。</strong>相对于绝对位置编码，相对位置编码更关注序列中位置之间的相对顺序和距离，它可以更好地处理长序列中的位置信息, 也具有更好的外推性。</p>
</li>
</ul>
<blockquote>
<p><strong>备注：什么是大模型外推性？（Length Extrapolation）</strong></p>
<p><strong>外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。</strong>例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。</p>
</blockquote>
<ul>
<li>如下图所示, 相对位置编码没有完整建模整个序列的位置信息，而是在算当前位置的Attention的时候，考虑了当前位置和被Attention位置之间的相对距离（这一操作可以体现<strong>在计算Attention过程中，在计算中引入一个相对位置向量进行学习</strong>)</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165820235.png" alt="image.png"></p>
<h4 id="Rotary-Position-Embeddings"><a href="#Rotary-Position-Embeddings" class="headerlink" title="Rotary Position Embeddings"></a>Rotary Position Embeddings</h4><p><strong>旋转位置编码的出发点: 把绝对位置编码和相对位置编码相结合. 具体来说, 我们希望给每个位置分配一个绝对位置编码, 然后不同位置的qk内积又直接包含位置差的信息(注意是“直接”，显式地包含).</strong></p>
<p><strong>也就是我们需要找到满足这个公式</strong>$\langle f_q(x_m, m), f_k(x_n, n) \rangle &#x3D; g(x_m, x_n, m - n)$<strong>的函数f, 同时希望编码本身有一些其他的性质(如如远程衰减、易于外推等)</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165828163.png" alt="image.png"></p>
<p><strong>回顾transformer用到的三角式位置编码, 虽然是绝对位置编码, 但是旋转矩阵的引入也让它包含了一定的相对位置信息</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165836993.png" alt="image.png"></p>
<p><strong>先研究2D情况</strong></p>
<p>利用三角函数运算, 可以将m-n的旋转矩阵分解为m和n的矩阵乘积, 也就实现了相对位置到绝对位置的转变</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165851458.png" alt="image.png"></p>
<p>如果左右乘上相应位置的q, k, 就得到了最终的内积形式, 这正是我们追求的目标形式$\langle f_q(x_m, m), f_k(x_n, n) \rangle &#x3D; g(x_m, x_n, m - n)$﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165901170.png" alt="image.png"></p>
<p><strong>扩展到全维度</strong></p>
<p>总结起来就是给词向量左乘一个旋转矩阵</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165910905.png" alt="image.png"></p>
<p><strong>优化</strong></p>
<p>由于$R_m$比较稀疏, 直接用矩阵乘法实现效率较低, 论文中提供了一种高效的实现方法, 用矩阵元素相乘替代矩阵乘法, 最后奇偶维度错位相加.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165918431.png" alt="image.png"></p>
<p><strong>总结</strong></p>
<ul>
<li><p>RoPE就像绝对位置编码一样，我们计算一次，然后可以对我们将要训练模型的所有句子重复使用。</p>
</li>
<li><p>使用RoPE编码的两个token之间的关系“强度”会随着它们之间距离的增加而在数值上变小, 称为long-term decay</p>
</li>
<li><p>RoPE只应用在Q, K上</p>
</li>
</ul>
<blockquote>
<p>在自注意力机制中，<strong>Query</strong> 和 <strong>Key</strong> 的内积决定了不同位置的词元之间的相关性（或称为“注意力权重”）。注意力机制的目标是根据这些权重，确定每个词元在多大程度上应该关注序列中的其他词元。因此，位置编码需要引入到 Query 和 Key 中，以影响注意力计算时如何考虑不同词元之间的相对位置。</p>
<p><strong>Value</strong> 并不直接参与注意力权重的计算，它只是在计算出注意力权重之后，根据这些权重聚合输入内容。因此，Value 中不需要加入位置编码。在自注意力机制中，Attention 输出是通过加权求和值得出的，而 Value 是被这些权重加权的部分。因为位置编码的目的是为了告诉模型不同位置之间的关系，而不是影响如何组合内容，所以没有必要将位置编码应用于 Value 上。</p>
</blockquote>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p><strong>自注意力机制允许模型将词与其他词关联起来。</strong></p>
<p>假设我们考虑一个序列长度为6, embedding维度为512的输入, 由于是自注意力, QKV都是相同的</p>
<p>﻿$softmax(\frac{QK^T}{\sqrt{d_k}})$捕捉了两个token之间的相关性</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165928491.png" alt="image.png"></p>
<p>上一步的结果再和V做矩阵乘, 得到与输入维度相同的输出矩阵, <strong>每行(即经过计算后的的新的embedding)不仅捕捉了词语的含义（由嵌入表示）或在句子中的位置（由位置编码表示），还捕捉了每个词与其他词的相互作用</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165937525.png" alt="image.png"></p>
<p>给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165943584.png" alt="image.png"></p>
<p><strong>Multi-Head Attention</strong></p>
<p>沿着d模型维度将QKV分割成较小的矩阵，并在这些较小的矩阵之间计算注意力. 所以每个头都在观察完整的句子，但是是每个embedding的不同方面。这样做的原因是<strong>我们希望每个头观察同一个词的不同方面</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165951462.png" alt="image.png"></p>
<p>再给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170001367.png" alt="image.png"></p>
<h3 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV cache"></a>KV cache</h3><h4 id="Next-Token-Prediction-Task"><a href="#Next-Token-Prediction-Task" class="headerlink" title="Next Token Prediction Task"></a>Next Token Prediction Task</h4><p>首先来了解一下LLama的训练（下词预测任务）：seq2seq的生成，但迭代T次，seq_len逐渐增加。</p>
<p>即输入序列的第一个token将映射到输出序列的第一个token, [SOS] -&gt; Love, 第二次迭代中将用[SOS] Love预测that, 以此类推</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170012232.png" alt="image.png"></p>
<h4 id="Motivation-behind-KV-cache"><a href="#Motivation-behind-KV-cache" class="headerlink" title="Motivation behind KV cache"></a>Motivation behind KV cache</h4><p>在推理的每一步，我们只关心模型输出的最后一个词元，因为我们已经有了之前的词元。然而，模型需要访问所有之前的词元来决定输出哪个词元，因为它们构成了模型的上下文（或称为“提示”）。<br>有没有办法让模型在推理时对已经看到的词元进行更少的计算呢？</p>
<h4 id="KV-cache-1"><a href="#KV-cache-1" class="headerlink" title="KV cache"></a>KV cache</h4><p>下句预测时的Self-Attention：</p>
<ul>
<li>timpstep&#x3D;1时seq_len&#x3D;1，给[SOS]时，预测Love；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170018823.png" alt="image.png"></p>
<ul>
<li>timpstep&#x3D;2时<code>seq_len=2</code>，给[SOS] 和 Love时，预测that</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170024662.png" alt="image.png"></p>
<ul>
<li>timpstep&#x3D;4时<code>seq_len=4</code>，给[SOS] 和 Love 和 can 和 quickly时，预测seize…</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170032874.png" alt="image.png"></p>
<p>再来分析一下，<strong>每次个timestep的self-attention中我们到底需要哪些</strong>：因为我们只关注<strong>最后一个token的</strong><code>**attention_output**</code>，如下图timestep&#x3D;4，我们只需要attention_output的第4个token。</p>
<p>因此我们只需要<strong>Q的最后一个token</strong>和<strong>K的所有token</strong>相乘，得到最后一个token的<code>attention_score</code>，然后用<strong>V的所有token</strong>再与<code>attention_score</code>点积(相乘求和)，得到最后一个token的<code>attention_output</code>：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170040017.png" alt="image.png"></p>
<p>由上分析可知，<strong>每个timestep，我们的Q只需要新增的那个token即可，而K和V要缓存之前timestep的token，保证token是全的</strong>。<strong>每次计算出来的attention_output就是那个新增的token的attention。</strong> 这样就可以节省大量计算开销。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170046580.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170053043.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170058416.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170105884.png" alt="image.png"></p>
<p><strong>参数量的计算方法:</strong> <a href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a>﻿</p>
<h3 id="Grouped-Multi-Query-Attention"><a href="#Grouped-Multi-Query-Attention" class="headerlink" title="Grouped Multi-Query Attention"></a>Grouped Multi-Query Attention</h3><p>回顾原始的<strong>多头注意力Multi-Head Attention</strong>：时间开销的瓶颈在于<strong>矩阵的运算</strong><code>**matrix computation**</code>。</p>
<ul>
<li><p>多头注意力机制如同在原始论文《Attention is all you need》中所展示的。</p>
</li>
<li><p>通过设定 $m &#x3D; n$（查询的序列长度 &#x3D; 键和值的序列长度）。</p>
</li>
<li><p>执行的算术运算数量为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量（包括派生张量）之和给出，为 $O(bnd + bhn^2 + d^2)$。</p>
</li>
<li><p>总内存与算术运算数量之间的比率为 $O\left( \frac{1}{k} + \frac{1}{bn} \right)$。</p>
</li>
<li><p>在这种情况下，该比率远小于 1，这意味着我们进行的内存访问量远少于算术运算的数量，因此内存访问<strong>不是</strong>这里的瓶颈。</p>
</li>
</ul>
<p>当我们<strong>使用KV-Cache</strong>后：时间开销的瓶颈在于<strong>内存的访问</strong><code>**memory access**</code>。</p>
<ul>
<li><p>使用 KV 缓存来减少执行的操作次数。</p>
</li>
<li><p>通过设定 $m &#x3D; n$（查询的序列长度 &#x3D; 键和值的序列长度）。</p>
</li>
<li><p>执行的算术运算次数为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 $O(bn^2d + nd^2)$。</p>
</li>
<li><p>总内存量与算术运算次数之间的比率为 $O(n&#x2F;d + 1&#x2F;b)$。</p>
</li>
<li><p>当 n ≈ d（序列长度接近嵌入向量的维度大小）或 b ≈ 1（批次大小为 1）时，比率变为 1，此时内存访问成为算法的瓶颈。对于批次大小通常没有问题，因为它通常远大于 1，而对于 n&#x2F;d 项，我们需要减少序列长度。但还有更好的方法……</p>
</li>
</ul>
<h5 id="Multi-Query-Attention（MQA）"><a href="#Multi-Query-Attention（MQA）" class="headerlink" title="Multi Query Attention（MQA）"></a>Multi Query Attention（MQA）</h5><p>为了提升attention计算效率，<strong>多查询注意力（</strong><code>**Multi Query Attention，MQA**</code><strong>）</strong> 是多头注意力的一种变体。其主要区别在于，在<strong>多查询注意力中</strong><code>**多个不同的注意力head**</code><strong>共享</strong><code>**一个**</code><strong>k和v的集合，每个head只单独保留了一份q参数。</strong> 具体操作上，<code>**去除 K和V 的head维度，只为Q保留head维度**</code>。因此这就是被叫做Multi Query Attention的原因。</p>
<ul>
<li><p>我们从 K 和 V 中移除了 h 维度，同时保留了 Q 的 h 维度。这意味着所有不同的查询头将共享相同的键和值。</p>
</li>
<li><p>执行的算术运算次数为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 $O(bnd + bn^2k + nd^2)$。</p>
</li>
<li><p>总内存量与算术运算次数之间的比率为 $O(1&#x2F;d + n&#x2F;dh + 1&#x2F;b)$。</p>
</li>
<li><p>与之前的方法相比，我们通过一个 $h$ 因子减少了昂贵的 $n&#x2F;d$ 项。</p>
</li>
<li><p>性能提升显著，而模型的质量只出现了少许下降。</p>
</li>
</ul>
<p>因此<code>K和V的矩阵的数量仅为1个</code>（不分head），大幅度减少了显存占用，使其更高效。<strong>由于多查询注意力改变了注意力机制的结构，因此模型通常需要从训练开始就支持多查询注意力。</strong></p>
<p>研究结果表明，可以通过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可以达到不错的效果。包括Falcon、SantaCoder、StarCoder等在内很多模型都采用了多查询注意力机制。</p>
<h5 id="Grouped-Multi-Query-Attention-GMQA"><a href="#Grouped-Multi-Query-Attention-GMQA" class="headerlink" title="Grouped Multi-Query Attention(GMQA)"></a>Grouped Multi-Query Attention(GMQA)</h5><p>就是在 Multi-Query Attention的基础上，对input进行分组，<strong>如下图2个head分为1组，每组都有自己的K，V，每个组包含2个Q。</strong> （与MQA的区别在于：<strong>MQA的KV只有1份；GQA的KV有</strong><code>**group**</code><strong>份(LLama-70B中是</strong><code>**kv_heads**</code>**&#x3D;8，即每个kv对应8个q)**）</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170116630.png" alt="image.png"></p>
<p><strong>参数量的计算方法:</strong> <a href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a>﻿</p>
<h3 id="SwiGLU-Function"><a href="#SwiGLU-Function" class="headerlink" title="SwiGLU Function"></a>SwiGLU Function</h3><p>SwiGLU 激活函数是Shazeer 在文献中提出，并在PaLM等模中进行了广泛应用，并且取得了不错的效果，<strong>相较于ReLU 函数在大部分评测中都有不少提升</strong>。在LLaMA 中全连接层使用带有SwiGLU 激活函数的FFN（Position-wise Feed-Forward Network）的计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164815049.png" alt="image.png"></p>
<p>其中，<strong>σ(x) 是Sigmoid 函数</strong>。下图给出了Swish 激活函数在参数β 不同取值下的形状。可以看到当β 趋近于0 时，Swish 函数趋近于线性函数y &#x3D; x，当β 趋近于无穷大时，Swish 函数趋近于ReLU 函数，β 取值为1 时，Swish 函数是光滑且非单调。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170126831.png" alt="image.png"></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>SwinTranformer</title>
    <url>/2025/01/10/SwinTranformer/</url>
    <content><![CDATA[<p>《<a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>》作为2021 ICCV最佳论文，屠榜了各大CV任务，性能优于DeiT、ViT和EfficientNet等主干网络，已经替代经典的CNN架构，成为了<strong>计算机视觉领域通用的通用骨干网络</strong>。它基于了ViT模型的思想，创新性的引入了<strong>滑动窗口机制</strong>，让模型能够学习到跨窗口的信息，同时也。同时通过<strong>下采样层</strong>，使得模型能够处理超分辨率的图片，节省计算量以及能够关注全局和局部的信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182640059.png" alt="image.png"></p>
<span id="more"></span>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h5 id="从CNN到Transformer"><a href="#从CNN到Transformer" class="headerlink" title="从CNN到Transformer"></a>从CNN到Transformer</h5><ul>
<li><p><strong>CNN的局限性：</strong> 虽然CNN在图像识别任务中取得了巨大成功，但它们通常局限于局部感受野，这限制了它们捕获长距离依赖的能力。此外，CNN在处理高分辨率图像时面临着效率和性能的挑战。</p>
</li>
<li><p><strong>Transformer的优势：</strong> Transformer结构，最初用于NLP任务，以其自注意力机制著称，可以有效处理长距离的依赖关系。这一特性使得Transformer在理解复杂的、全局性的数据结构方面表现出色。</p>
</li>
</ul>
<h5 id="从Transformer到Swin-Transformer"><a href="#从Transformer到Swin-Transformer" class="headerlink" title="从Transformer到Swin Transformer"></a>从Transformer到Swin Transformer</h5><ul>
<li><p><strong>尺度变化和分辨率问题：</strong> 在计算机视觉领域，尤其是处理高分辨率图像时，面临的主要挑战之一是视觉实体尺度的大变化和像素的高分辨率。这些特点与语言处理中的情况截然不同，因为在文本中，词汇的“分辨率”（即明确性和区分度）相对较低。</p>
</li>
<li><p><strong>传统Transformer结构的局限：</strong> 由于这些差异，传统的Transformer结构（如在自然语言处理中使用的那样）直接应用于视觉任务时会遇到效率和性能的挑战。尤其是在处理需要细致像素级预测的高分辨率图像时，传统Transformer的全局自注意力机制导致计算复杂度过高，不适合直接应用于视觉任务。</p>
</li>
</ul>
<p>Swin Transformer 提出了一种基于<strong>滑动窗口机制，具有层级设计（下采样层）</strong> 的模型结构。其中<strong>滑窗操作</strong>包括<strong>不重叠的 local window，和重叠的 cross-window</strong>。将注意力计算限制在一个窗口（window size固定）中，<strong>一方面能引入 CNN 卷积操作的局部性，另一方面能大幅度节省计算量</strong>，它只和窗口数量成线性关系。通过<strong>下采样</strong>的层级设计，能够逐渐增大感受野，从而使得注意力机制也能够注意到<strong>全局</strong>的特征。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182648835.png" alt="image.png"></p>
<p>整个模型采取层次化的设计，一共包含 4 个 Stage，除第一个 stage 外，每个 stage 都会先通过 <strong>Patch Merging</strong> 层缩小输入特征图的分辨率，进行<strong>下采样操作</strong>，像 CNN 一样逐层扩大感受野，以便获取到全局的信息。</p>
<ul>
<li><p>在输入开始的时候，做了一个<code>Patch Partition</code>，即ViT中<code>Patch Embedding</code>操作，通过 <strong>Patch_size</strong> 为4的卷积层将图片切成一个个 <strong>Patch</strong> ，并嵌入到<code>Embedding</code>，将 <strong>embedding_size</strong> 转变为48（可以将 CV 中图片的<strong>通道数</strong>理解为NLP中token的<strong>词嵌入长度</strong>），以swin-s为例，具体是将图像先分割成4 × 4 的小块，然后将每一个小块通过映射成一个像素点，进行了通道上的扩充，输入的224 × 224 图像经过这一步操作就变成了56 × 56的特征图。</p>
</li>
<li><p>随后在第一个Stage中，通过<code>Linear Embedding</code>调整通道数为C。然后Transformer的输入和输出维度是相同的，所以进入下一个stage的大小是不变的。</p>
</li>
<li><p>在每个 Stage 里（除第一个 Stage ），均由<code>Patch Merging</code>和多个<code>Swin Transformer Block</code>组成。</p>
</li>
<li><p>其中<code>Patch Merging</code>模块主要在每个 Stage 一开始降低图片分辨率，进行下采样的操作。H和W各变为二分之一，总共是缩小4分之一，然后通道数增加了4倍（用空间换深度），经过一个全连接层再调整通道维度为原来的2倍，也就是说每经过一个stage，总的数据量变为原来的1&#x2F;2。可以参考下面的示意图（输入张量N&#x3D;1, H&#x3D;W&#x3D;8, C&#x3D;1，不包含最后的全连接层调整）。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182658508.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182704756.png" alt="image.png"></p>
<ul>
<li>而<code>Swin Transformer Block</code>具体结构如右图所示，主要是<code>LayerNorm</code>，<code>Window Attention</code> ，<code>Shifted Window Attention</code>和<code>MLP</code>组成 。</li>
</ul>
<h3 id="Window-Attention-和-Shift-Window-Attention"><a href="#Window-Attention-和-Shift-Window-Attention" class="headerlink" title="Window Attention 和 Shift Window Attention"></a><strong>Window Attention 和 Shift Window Attention</strong></h3><p>这是这篇文章的关键。传统的Transformer都是基于全局来计算注意力的，因此计算复杂度十分高。而Swin Transformer则将注意力的计算限制在每个窗口内，进而减少了计算量。</p>
<p>window attention就是按照一定的尺寸将图像划分为不同的window，每次transformer的attention只在window内部进行计算。</p>
<p>那么如果只有window attention就会带来每一个像素点的感受野得不到提升的问题，为了更好的和其他 window 进行信息交互，所以它又设计了一个shift window attention（下移两个patchs）的方法，就是换一下window划分的方式，让每一个像素点做attention计算的window块处于变化之中。那么就起到了提升感受野的作用。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182712651.png" alt="image.png"></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><p>﻿<a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a>﻿</p>
</li>
<li><p>﻿<a href="https://zhuanlan.zhihu.com/p/367111046">https://zhuanlan.zhihu.com/p/367111046</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/12/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>参数传递</title>
    <url>/2024/12/16/%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92/</url>
    <content><![CDATA[<h1 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214303103.png" alt="image.png"></p>
<span id="more"></span>

<h2 id="普通参数-可以为nullptr，或者可被修改"><a href="#普通参数-可以为nullptr，或者可被修改" class="headerlink" title="普通参数-可以为nullptr，或者可被修改"></a>普通参数-可以为nullptr，或者可被修改</h2><p>class Widget { .. };</p>
<ol>
<li>可以为nullptr,只读参数<br> void foo (const Widget* const widget);</li>
<li>可以为nullptr,可被修改的参数<br> void foo(Widget* const widget);</li>
<li>不为nullptr,可被修改的参数<br> void foo (Widget&amp; widget);<br> 或者void foo(Widget* const widget);如果编程规范有统一要求</li>
</ol>
<h2 id="普通参数-不为-nullptr，不能修改，不能-move"><a href="#普通参数-不为-nullptr，不能修改，不能-move" class="headerlink" title="普通参数-不为 nullptr，不能修改，不能 move"></a>普通参数-不为 nullptr，不能修改，不能 move</h2><p>class Widget { .. };</p>
<ol>
<li>不能修改的只读参数<br> void foo (const Widget&amp; widget) {<br> std:cout &lt; widget.x &lt;&lt; std:endl;<br> }</li>
</ol>
<h2 id="普通参数-不为-nullptr，不能修改，可以-move"><a href="#普通参数-不为-nullptr，不能修改，可以-move" class="headerlink" title="普通参数-不为 nullptr，不能修改，可以 move"></a>普通参数-不为 nullptr，不能修改，可以 move</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214314697.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214330669.png" alt="image.png"></p>
<h1 id="unique-ptr"><a href="#unique-ptr" class="headerlink" title="unique_ptr&lt;T&gt;"></a>unique_ptr&lt;T&gt;</h1><p>把ownership传递给 foo<br><code>void foo (std:unique_ptr\&lt;T&gt; object);</code><br><strong>例子：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">auto object = std: make_unique\&lt;std:string&gt;();</span><br><span class="line">foo(std:move(object);</span><br><span class="line">assert(object == nullptr);</span><br></pre></td></tr></table></figure>
<h2 id="unique-ptr-转换为普通参数"><a href="#unique-ptr-转换为普通参数" class="headerlink" title="unique_ptr&lt;T&gt; 转换为普通参数"></a>unique_ptr&lt;T&gt; 转换为普通参数</h2><p>如果不需要传递ownership,应该按照普通数据结构来传递，不要<br>直接传递unique_ptr。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. auto object = std: make_unique&lt;std: string&gt;();</span><br><span class="line">2. foo (object.get(); //foo (const std:string* const str);</span><br><span class="line">3.                    //foo(std:string* const str);</span><br><span class="line">4.foo(*object);       //foo (const std:string&amp; str);</span><br><span class="line">5.                    //foo (std: string&amp; str)</span><br></pre></td></tr></table></figure>
<h2 id="什么时候需要传递std-unique-ptr"><a href="#什么时候需要传递std-unique-ptr" class="headerlink" title="什么时候需要传递std:unique_ptr&lt;T&gt;&amp;"></a>什么时候需要传递std:unique_ptr&lt;T&gt;&amp;</h2><p>用于factory类型的场合，foo返回一个unique_ptr给调用者。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo (std:unqieu_ptr&lt;std:string&gt;&amp; str) &#123;</span><br><span class="line">2. 	str = std::make_unique&lt;std:string&gt;();</span><br><span class="line">3. &#125;</span><br></pre></td></tr></table></figure>
<h1 id="shared-ptr"><a href="#shared-ptr" class="headerlink" title="shared_ptr&lt;T&gt;"></a>shared_ptr&lt;T&gt;</h1><p>把ownership分享给 foo<br><code>void foo (std:shared_ptr&lt;T&gt; object);</code><br><strong>例子：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">auto object = std:make_shared&lt;std:string&gt;();</span><br><span class="line">foo (object);</span><br><span class="line">assert(object != nullptr);</span><br></pre></td></tr></table></figure>
<h2 id="shared-ptr-转换为普通参数"><a href="#shared-ptr-转换为普通参数" class="headerlink" title="shared_ptr&lt;T&gt; 转换为普通参数"></a>shared_ptr&lt;T&gt; 转换为普通参数</h2><p>如果不需要分享ownership,应该按照普通数据结构来传递，不要直接传递shared_ptr(直接传递shared_ptr涉及引用计数的增减，是很慢的操作)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. auto object = std: make_shared&lt;std:string&gt;();</span><br><span class="line">2. foo(object.get(); //foo (const std:string* const str);</span><br><span class="line">3.                   //foo(std:string* const str);</span><br><span class="line">4.foo(*object);      //foo(const std:string&amp; str);</span><br><span class="line">5.                   //foo(std:string&amp; str)</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214348374.png" alt="image.png"></p>
<h2 id="什么时候传递-const-std-shared-ptr"><a href="#什么时候传递-const-std-shared-ptr" class="headerlink" title="什么时候传递 const std:shared_ptr&lt;T&gt;&amp;"></a>什么时候传递 const std:shared_ptr&lt;T&gt;&amp;</h2><p>很少见。需要分享ownership,但是又不能 move。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo (const std:shared_ptr&lt;std:string&gt;&amp; str)&#123;</span><br><span class="line">2.   bar(str);</span><br><span class="line">3.   std: cout &lt;&lt;*str &lt;&lt; std:endl;</span><br><span class="line">4. &#125;</span><br><span class="line">5. void bar (std: shared_ptr&lt;std:string&gt; str)&#123;</span><br><span class="line">6.   _str = std: move(str);</span><br><span class="line">7. &#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214357476.png" alt="image.png"></p>
<h1 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214417945.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-std-function"><a href="#Pass-Lambda-By-std-function" class="headerlink" title="Pass Lambda By std::function"></a>Pass Lambda By std::function</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. void foo() &#123;</span><br><span class="line">2.   int x= 0;</span><br><span class="line">3.   auto func = [=](int i)-&gt;bool &#123; return i&gt; x; &#125;;</span><br><span class="line">4.   bar (func);</span><br><span class="line">5. &#125;</span><br><span class="line">6. void bar (std: function&lt;bool(int number)&gt; func) &#123;</span><br><span class="line">7.   bool result = func(100);</span><br><span class="line">8. &#125;</span><br></pre></td></tr></table></figure>
<p><strong>std::function 是什么？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. class function &#123;</span><br><span class="line">2.   lambda_xyz* lambda_ptr;</span><br><span class="line">3.   ...</span><br><span class="line">4. &#125;;</span><br></pre></td></tr></table></figure>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214430960.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214438981.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-std-function-Reference-–-Side-Effect"><a href="#Pass-Lambda-By-std-function-Reference-–-Side-Effect" class="headerlink" title="Pass Lambda By std::function Reference – Side Effect"></a>Pass Lambda By std::function Reference – Side Effect</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214448012.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-std-function-优缺点"><a href="#Pass-Lambda-By-std-function-优缺点" class="headerlink" title="Pass Lambda By std::function 优缺点"></a>Pass Lambda By std::function 优缺点</h2><p>好处：明确的参数类型和返回值类型（编译器强制检查）<br>坏处：有一次内存分配的开销（对特别小的lambda可以优化掉）<br>建议：在性能要求不高的场合使用</p>
<h2 id="Pass-Lambda-By-Template"><a href="#Pass-Lambda-By-Template" class="headerlink" title="Pass Lambda By Template"></a>Pass Lambda By Template</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214508109.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-Perfect-Forwarding"><a href="#Pass-Lambda-By-Perfect-Forwarding" class="headerlink" title="Pass Lambda By Perfect Forwarding"></a>Pass Lambda By Perfect Forwarding</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214533786.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214543503.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-Perfect-Forwarding-–-Side-Effect"><a href="#Pass-Lambda-By-Perfect-Forwarding-–-Side-Effect" class="headerlink" title="Pass Lambda By Perfect Forwarding – Side Effect"></a>Pass Lambda By Perfect Forwarding – Side Effect</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214551580.png" alt="image.png"></p>
<h2 id="Pass-Lambda-By-Perfect-Forwarding-优缺点"><a href="#Pass-Lambda-By-Perfect-Forwarding-优缺点" class="headerlink" title="Pass Lambda By Perfect Forwarding  优缺点"></a>Pass Lambda By Perfect Forwarding  优缺点</h2><p>好处：没有内存分配的开销，也没有lambda 拷贝。<br>坏处：另外失去了强制类型检查，还有副作用。</p>
<h2 id="参考标准库"><a href="#参考标准库" class="headerlink" title="参考标准库"></a>参考标准库</h2><p>Pass lambda by template<br>1. template &lt;.., class Function&gt;<br>2. void for_each(.., Function f)<br>Pass lambda by perfect forwarding<br>1. template &lt;., class URBG&gt;<br>2. void shuffle(.., URBG&amp;&amp; g);</p>
<h1 id="Pass-Arguments-to-Thread"><a href="#Pass-Arguments-to-Thread" class="headerlink" title="Pass Arguments to Thread"></a>Pass Arguments to Thread</h1><h2 id="先拷贝再move"><a href="#先拷贝再move" class="headerlink" title="先拷贝再move"></a>先拷贝再move</h2><p>1. void func (int i, const std: string&amp; s);<br>2. std:thread t (func, 3, “hello”);<br>编译器产生的代码是这样的：</p>
<ol>
<li>thread的构造函数把参数3和“hello”拷贝到一个安全的地方。这里的<br>重点是“拷贝”。”hello”被当作const char * 拷贝。</li>
<li>创建线程。</li>
<li>新线程调用func,并把刚才拷贝的参数，move给func。这里的重点是“move”。”hello”以 const char * 的类型 move给 “const std:string&amp; s”, 这是可以的。</li>
</ol>
<h2 id="先拷贝再move的陷阱1"><a href="#先拷贝再move的陷阱1" class="headerlink" title="先拷贝再move的陷阱1"></a>先拷贝再move的陷阱1</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214605516.png" alt="image.png"></p>
<h2 id="陷阱1的解决方法"><a href="#陷阱1的解决方法" class="headerlink" title="陷阱1的解决方法"></a>陷阱1的解决方法</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214614141.png" alt="image.png"></p>
<h2 id="先拷贝再move的陷阱2"><a href="#先拷贝再move的陷阱2" class="headerlink" title="先拷贝再move的陷阱2"></a>先拷贝再move的陷阱2</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214623154.png" alt="image.png"></p>
<h2 id="陷阱2的解决方法"><a href="#陷阱2的解决方法" class="headerlink" title="陷阱2的解决方法"></a>陷阱2的解决方法</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214633346.png" alt="image.png"></p>
<h2 id="传递类的成员函数给thread"><a href="#传递类的成员函数给thread" class="headerlink" title="传递类的成员函数给thread"></a>传递类的成员函数给thread</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. class Widget &#123;</span><br><span class="line">2. public:</span><br><span class="line">3.   void do_work(int number) &#123;. &#125;</span><br><span class="line">4. &#125;;</span><br><span class="line">5. Widget w;</span><br><span class="line">6. std:thread t(&amp;Widget:do_work, &amp;w, 100);</span><br></pre></td></tr></table></figure>
<h1 id="initializer-list"><a href="#initializer-list" class="headerlink" title="initializer_list"></a>initializer_list</h1><p>foo (std: initializer_list&lt;int&gt; numbers);</p>
<p>调用例子：<br>foo ({1, 2, 3});</p>
<p>initializer_list不需要 const initializer_list&lt;T&gt;&amp;。<br>不需要引用：initializer_list很小，传引用和传值代价相当。<br>不需要const：initializer_list默认是const。</p>
<h2 id="什么是initializer-list"><a href="#什么是initializer-list" class="headerlink" title="什么是initializer_list&lt;int&gt;"></a>什么是initializer_list&lt;int&gt;</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214642741.png" alt="image.png"></p>
<h1 id="move-only-objects"><a href="#move-only-objects" class="headerlink" title="move-only objects"></a>move-only objects</h1><ul>
<li>std::future</li>
<li>std::thread</li>
<li>std::unique_ptr</li>
</ul>
<h1 id="small-object-that-can-pass-by-value"><a href="#small-object-that-can-pass-by-value" class="headerlink" title="small object that can pass by value"></a>small object that can pass by value</h1><ul>
<li>std::initializer_list</li>
</ul>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>MAE (Masked AutoEncoders)</title>
    <url>/2025/01/10/MAE-Masked-AutoEncoders/</url>
    <content><![CDATA[<h2 id="MAE-是什么"><a href="#MAE-是什么" class="headerlink" title="MAE 是什么"></a>MAE 是什么</h2><p>MAE (Masked AutoEncoders) 是可拓展自监督视觉学习器，思想是随机掩盖一些图像块，然后重建丢失的像素。MAE 使用一个非对称的解码编码器对可见的图形块进行处理，同时使用一个轻量级的解码器从潜在表示和掩码块重建原始图像；如果遮挡输入图像的大部分（例如75%），就是变成了一个自监督任务。这两种方法可以有效地训练大规模模型：可以加速训练和提高精度。MAE可用于ViT下游识别任务的fine-tune，还可以用于目标检测、实例分割以及语义分割等任务的迁移学习。</p>
<p><strong>和VIT的区别：</strong></p>
<ol>
<li><p>需要盖住更多的块，使得剩下的块块与块之间的冗余度没那么高</p>
</li>
<li><p>使用Transformer架构的解码器，直接还原原始的像素信息</p>
</li>
</ol>
<span id="more"></span>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182334475.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182342364.png" alt="image.png"></p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><strong>编码器</strong></p>
<p>编码器是 ViT。 它接受张量形状为 (batch_size, RGB_channels, height, width) 的图像。 通过执行线性投影为每个Patch获得嵌入， 这是通过 2D 卷积层来完成。 然后张量在最后一个维度被展平（压扁），变成 (batch_size, encoder_embed_dim, num_visible_patches)，并 转置为形状（batch_size、num_visible_patches、encoder_embed_dim）的张量。正如原始 Transformer 论文中提到的，位置编码添加了有关每个Patch位置的信息。 作者使用“sine-cosine”版本而不是可学习的位置嵌入。 与 Transformer 类似，每个块由norm层、多头注意力模块和前馈层组成。 中间输出形状是（batch_size、num_visible_patches、encoder_embed_dim）。这部分仅用于下游任务的微调。 论文的模型遵循 ViT 架构，该架构具有用于分类的类令牌（patch）。 因此，他们添加了一个虚拟令牌，但是论文中也说到他们的方法在没有它的情况下也可以运行良好，因为对其他令牌执行了平均池化操作。 在这里也包含了实现的平均池化版本。 之后，添加一个线性层作为分类器。 最终的张量形状是 (batch_size, num_classes)。</p>
<p><strong>解码器</strong></p>
<p>与编码器类似，解码器由一系列transformer 块组成。 在解码器的末端，有一个由norm层和前馈层组成的分类器。 输入张量的形状为 batch_size, num_patches,decoder_embed_dim) 而最终输出张量的形状为 (batch_size, num_patches, 3 * patch_size ** 2)。</p>
<p><strong>把所有东西放在一起——MAE架构</strong></p>
<p>MAE 用于对掩码图像进行预训练。首先，屏蔽的输入被发送到编码器。然后，它们被传递到前馈层以更改嵌入维度以匹配解码器。 在传递给解码器之前，被掩码的Patch被输入进去。 位置编码再次应用于完整的图像块集，包括可见的和被掩码遮盖的。</p>
<p>在论文中，作者对包含所有Patch的列表进行了打乱，以便正确插入Patch的掩码。 这部分在本篇文章中没有完成，因为在 PyTorch 上实现并不简单。所以这里使用的是位置编码在被添加到Patch之前被相应地打乱的做法。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>让我们看看原始论文中报道的预训练阶段的重建图像。看起来MAE在重建图像方面做得很好，即使80%的像素被遮蔽了。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110182351798.png" alt="image.png"></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ul>
<li>MAE的Decoder和Bert，Vit有什么区别</li>
</ul>
<p>用于重建的decoder在图像和文本任务发挥的角色有区别，从句子中预测单词属于高语义任务，encoder和decoder的gap小，所以BERT的decoder部分微不足道（只需要一个MLP），而对图像重建像素属于低语义任务（相比图像分类），decoder需要发挥更大作用：将高语义的中间表征恢复成低语义的像素值。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>﻿<a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>右值引用</title>
    <url>/2024/12/16/%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8/</url>
    <content><![CDATA[<h1 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h1><h2 id="表达式（Expression）"><a href="#表达式（Expression）" class="headerlink" title="表达式（Expression）"></a>表达式（Expression）</h2><p>C++表达式是运算符和操作数的组合。运算符包括赋值、数逻比较和函数等，操作数包括变量和literal等。例子：<br>1. <code>foo(x + y)</code><br>2. <code>1 + b * foo()</code><br>3. <code>a = 1 + b * foo()</code><br>4. <code>a == b &amp;&amp; a &lt; c</code><br>5. <code>((i &lt; 3) ? i : j)=7</code> </p>
<span id="more"></span>
<h2 id="中间结果"><a href="#中间结果" class="headerlink" title="中间结果"></a>中间结果</h2><p>表达式可以分解为子表达式。比如：“foo()”是“b * foo()”的子表达式，“b * foo()”是“1+b * foo()”的子表达式。每个表达式和子表达式都产生一个结果。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213404989.png" alt="image.png"></p>
<h2 id="函数非引用返回值"><a href="#函数非引用返回值" class="headerlink" title="函数非引用返回值"></a>函数非引用返回值</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213425072.png" alt="image.png"></p>
<h2 id="无名的临时变量"><a href="#无名的临时变量" class="headerlink" title="无名的临时变量"></a>无名的临时变量</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213434290.png" alt="image.png"></p>
<h2 id="Copy-v-s-Move"><a href="#Copy-v-s-Move" class="headerlink" title="Copy v.s. Move"></a>Copy v.s. Move</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213455621.png" alt="image.png"></p>
<h2 id="利用move实现编译自动优化"><a href="#利用move实现编译自动优化" class="headerlink" title="利用move实现编译自动优化"></a>利用move实现编译自动优化</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213506139.png" alt="image.png"></p>
<h2 id="右值引用（Rvalue-Reference）"><a href="#右值引用（Rvalue-Reference）" class="headerlink" title="右值引用（Rvalue Reference）"></a>右值引用（Rvalue Reference）</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213516271.png" alt="image.png"></p>
<h2 id="值类别（Value-Category）"><a href="#值类别（Value-Category）" class="headerlink" title="值类别（Value Category）"></a>值类别（Value Category）</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213523593.png" alt="image.png"></p>
<h3 id="lvalue"><a href="#lvalue" class="headerlink" title="lvalue"></a>lvalue</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213533694.png" alt="image.png"></p>
<h3 id="prvalue"><a href="#prvalue" class="headerlink" title="prvalue"></a>prvalue</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213540915.png" alt="image.png"></p>
<h3 id="lvalue-和-prvalue-的例子"><a href="#lvalue-和-prvalue-的例子" class="headerlink" title="lvalue 和 prvalue 的例子"></a>lvalue 和 prvalue 的例子</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213551360.png" alt="image.png"></p>
<h3 id="xvalue"><a href="#xvalue" class="headerlink" title="xvalue"></a>xvalue</h3><p>Xvalue是接近生命周期末尾的lvalue,尽管它在内存中，可以被访问，但是程序员主动放弃了对它的访问权。程序员需要显式地进行强制转换std:move(x),告诉编译器x不再访问。std:move(x)表达式的结果是一个xvalue。</p>
<h1 id="std-move"><a href="#std-move" class="headerlink" title="std::move"></a>std::move</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213600749.png" alt="image.png"></p>
<h2 id="实现move操作的完整例子"><a href="#实现move操作的完整例子" class="headerlink" title="实现move操作的完整例子"></a>实现move操作的完整例子</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213609211.png" alt="image.png"></p>
<p><strong>改进后的例子</strong><br>初始化列表是在构造函数的冒号后面使用的，用于直接初始化类的成员变量。在构造函数体内赋值则是首先调用默认构造函数或默认初始化后，再进行赋值操作。这种方式避免了额外的赋值操作，特别是对于类类型的成员变量或容器类型时，初始化列表可以提升性能。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213616937.png" alt="image.png"></p>
<h2 id="Move操作常见错误一：忘了继续-move"><a href="#Move操作常见错误一：忘了继续-move" class="headerlink" title="Move操作常见错误一：忘了继续 move"></a>Move操作常见错误一：忘了继续 move</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213624252.png" alt="image.png"></p>
<h2 id="Move操作常见错误二：-move-const-object"><a href="#Move操作常见错误二：-move-const-object" class="headerlink" title="Move操作常见错误二： move const object"></a>Move操作常见错误二： move const object</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213634397.png" alt="image.png"></p>
<h2 id="Move-操作常见错误三：-move-from-object-invalid"><a href="#Move-操作常见错误三：-move-from-object-invalid" class="headerlink" title="Move 操作常见错误三： move-from object invalid"></a>Move 操作常见错误三： move-from object invalid</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213644265.png" alt="image.png"></p>
<h2 id="Move-构造函数的两段式写法"><a href="#Move-构造函数的两段式写法" class="headerlink" title="Move 构造函数的两段式写法"></a>Move 构造函数的两段式写法</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213651652.png" alt="image.png"></p>
<h2 id="Move-赋值运算符的四段式写法"><a href="#Move-赋值运算符的四段式写法" class="headerlink" title="Move 赋值运算符的四段式写法"></a>Move 赋值运算符的四段式写法</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213701677.png" alt="image.png"></p>
<h2 id="Move-操作常见错误四：move-同类型的本地返回值"><a href="#Move-操作常见错误四：move-同类型的本地返回值" class="headerlink" title="Move 操作常见错误四：move 同类型的本地返回值"></a>Move 操作常见错误四：move 同类型的本地返回值</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213710567.png" alt="image.png"></p>
<h2 id="Move操作常见错误五：忘了-move不同类型的本地返回值"><a href="#Move操作常见错误五：忘了-move不同类型的本地返回值" class="headerlink" title="Move操作常见错误五：忘了 move不同类型的本地返回值"></a>Move操作常见错误五：忘了 move不同类型的本地返回值</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213722724.png" alt="image.png"></p>
<h2 id="Move-操作带来的问题"><a href="#Move-操作带来的问题" class="headerlink" title="Move 操作带来的问题"></a>Move 操作带来的问题</h2><p>假如foo函数想接收一个string类型的参数，它得写两个函数：</p>
<ol>
<li>void foo(const string&amp; str);</li>
<li>void foo(string&amp;&amp; str);<br>假如foo函数想接收两个string类型的参数，它得写四个函数：</li>
<li>void foo(const string&amp; str1, const string&amp; str2);</li>
<li>void foo(const string&amp; str1, string&amp;&amp; str2);</li>
<li>void foo(string&amp;&amp; str1, const string&amp; str2);</li>
<li>void foo(string&amp;&amp; str1, string&amp;&amp; str2);</li>
</ol>
<h1 id="完美转发（Perfect-Forwarding）和转发引用（Forward-Reference）"><a href="#完美转发（Perfect-Forwarding）和转发引用（Forward-Reference）" class="headerlink" title="完美转发（Perfect Forwarding）和转发引用（Forward Reference）"></a>完美转发（Perfect Forwarding）和转发引用（Forward Reference）</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213732468.png" alt="image.png"></p>
<h2 id="引用折叠（Reference-Collapsing）–-lvalue"><a href="#引用折叠（Reference-Collapsing）–-lvalue" class="headerlink" title="引用折叠（Reference Collapsing）– lvalue"></a>引用折叠（Reference Collapsing）– lvalue</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213739335.png" alt="image.png"></p>
<h2 id="引用折叠（Reference-Collapsing）–-rvalue"><a href="#引用折叠（Reference-Collapsing）–-rvalue" class="headerlink" title="引用折叠（Reference Collapsing）– rvalue"></a>引用折叠（Reference Collapsing）– rvalue</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213749826.png" alt="image.png"></p>
<h2 id="转发引用的关键特征：有推理-格式对"><a href="#转发引用的关键特征：有推理-格式对" class="headerlink" title="转发引用的关键特征：有推理+格式对"></a>转发引用的关键特征：有推理+格式对</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213800564.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213808575.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213820439.png" alt="image.png"></p>
<h2 id="完美转发常见问题一：把转发引用当作右值引用"><a href="#完美转发常见问题一：把转发引用当作右值引用" class="headerlink" title="完美转发常见问题一：把转发引用当作右值引用"></a>完美转发常见问题一：把转发引用当作右值引用</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213832161.png" alt="image.png"></p>
<h2 id="完美转发常见问题二：把右值引用当作转发引用"><a href="#完美转发常见问题二：把右值引用当作转发引用" class="headerlink" title="完美转发常见问题二：把右值引用当作转发引用"></a>完美转发常见问题二：把右值引用当作转发引用</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213839849.png" alt="image.png"></p>
<h2 id="完美转发常见问题三：同一变量转发多次"><a href="#完美转发常见问题三：同一变量转发多次" class="headerlink" title="完美转发常见问题三：同一变量转发多次"></a>完美转发常见问题三：同一变量转发多次</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213848714.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213855500.png" alt="image.png"></p>
<h2 id="完美转发常见问题四：不要重载完美转发"><a href="#完美转发常见问题四：不要重载完美转发" class="headerlink" title="完美转发常见问题四：不要重载完美转发"></a>完美转发常见问题四：不要重载完美转发</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213903165.png" alt="image.png"></p>
<h2 id="什么时候用noexcept"><a href="#什么时候用noexcept" class="headerlink" title="什么时候用noexcept"></a>什么时候用noexcept</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216213915030.png" alt="image.png"></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>并行无锁数据结构(上) - 基本概念, 链表</title>
    <url>/2024/12/16/%E5%B9%B6%E8%A1%8C%E6%97%A0%E9%94%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%B8%8A-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="Fine-grained-Lock-vs-Lock-Free"><a href="#Fine-grained-Lock-vs-Lock-Free" class="headerlink" title="Fine-grained Lock vs. Lock Free"></a>Fine-grained Lock vs. Lock Free</h2><ul>
<li><strong>Fine-grained lock</strong><br>  Slow down all the threads if one thread  is stuck inside the critical section.  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">spin_lock.<span class="built_in">lock</span>();</span><br><span class="line">x= new_value; <span class="comment">// </span></span><br><span class="line">spin_lock.<span class="built_in">unlock</span>();</span><br></pre></td></tr></table></figure></li>
<li><strong>Lock Free</strong><ul>
<li>no stuck in critical section (critical section is a single atomic operation)</li>
<li>If one thread fails at CAS, there must be another thread succeeds at the CAS (system-wide progress).  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> expected = x.<span class="built_in">load</span>();</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">	<span class="type">const</span> <span class="type">bool</span> ok = x.<span class="built_in">compare_exchange_strong</span>(expected, new_value);</span><br><span class="line">&#125; <span class="keyword">while</span> (!ok);</span><br><span class="line"><span class="comment">// x.compare_exchange_strong(expected, new_value) 的意思是：</span></span><br><span class="line"><span class="comment">// 如果 x 的当前值等于 expected，就把 new_value 赋值给 x，并且返回 true</span></span><br><span class="line"><span class="comment">// 否则把 x 的当前值传给 expected，返回 false</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
在第一种情况中，如果正好在临界区中触发了一次线程的切换，另一个被切换到的线程得不到这个锁，让跑满了一个时间片，但又做不了任何事情， 10ms后才被强制切换走。在第二种情况中，用了CAS这个原子指令，则不会因为线程切换而产生类似的情况</li>
</ul>
</li>
</ul>
<h2 id="Non-blocking-vs-Lock-free-vs-Wait-free"><a href="#Non-blocking-vs-Lock-free-vs-Wait-free" class="headerlink" title="Non-blocking vs. Lock-free vs. Wait-free"></a>Non-blocking vs. Lock-free vs. Wait-free</h2><p>1. Non-blocking: Failure or suspension of any thread cannot cause failure or suspension of another thread.<br>理解：一个线程在临界区被切换走不会导致另一个线程被卡住<br>2. Lock-free: A non-blocking algorithm is lock-free if there is a guaranteed system-wide progress.<br>理解：还能保证整个系统在宏观层面一直在 make progress<br>3. Wait-free: A non-blocking algorithm is wait-free if there is a guaranteed per-thread progress.<br>理解：还能保证每个线程一直在 make progress，比如保证能在一定次数之后一定能成功一次，但是实现起来非常复杂。</p>
<h2 id="并行数据结构"><a href="#并行数据结构" class="headerlink" title="并行数据结构"></a>并行数据结构</h2><ol>
<li><strong>Concurrent Search Data Structure(CSDS)</strong><ol>
<li>KV容器，三个接口：查找，插入和删除。<ul>
<li>查找操作的核心是搜索数据结构，比如遍历链表。</li>
<li>插入和删除的过程可以分为两阶段：首先也是对数据结构进行搜索，找到正确的位置，然后完成插入或者删除的动作。</li>
</ul>
</li>
<li>例子：链表（linked list）,跳表（skip list）,哈希表（hash table）,查找树（search tree）和缓存（Cache）<br> <strong>高性能 CSDS 的关键</strong><ol>
<li><strong>搜索快</strong>。搜索过程应该尽量避免（最好完全避免）耗时的操作 (写操作，memory barrier，原子操作），等待和重试（retry）。</li>
<li><strong>细粒度</strong>。修改应该尽可能涉及较小范围。如果CSDS采用锁来实现，就意味着细粒度锁。</li>
</ol>
</li>
</ol>
</li>
<li><strong>非查找数据结构</strong><ol>
<li>例子：队列(queue)和栈(stack)</li>
</ol>
</li>
</ol>
<h1 id="Singly-Linked-List-Memory-Management"><a href="#Singly-Linked-List-Memory-Management" class="headerlink" title="Singly-Linked List &amp; Memory Management"></a>Singly-Linked List &amp; Memory Management</h1><h2 id="回顾单链表"><a href="#回顾单链表" class="headerlink" title="回顾单链表"></a>回顾单链表</h2><ul>
<li><p>插入节点<br>  <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214932956.png" alt="image.png"></p>
<ol>
<li>把20的next指针指向30</li>
<li>用cas指令判断10的next指针是否还等于30，如果是的话则改成20，防止在修改的过程中另一个线程也想在10之后插入节点</li>
</ol>
</li>
<li><p>删除节点<br>  <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216214941144.png" alt="image.png"></p>
<ol>
<li>把head的next指向10后面的节点30，同样用cas保护</li>
</ol>
</li>
</ul>
<h2 id="并行单链表的问题"><a href="#并行单链表的问题" class="headerlink" title="并行单链表的问题"></a>并行单链表的问题</h2><h3 id="并行单链表的问题1-–-插入和删除冲突"><a href="#并行单链表的问题1-–-插入和删除冲突" class="headerlink" title="并行单链表的问题1 – 插入和删除冲突"></a>并行单链表的问题1 – 插入和删除冲突</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215112033.png" alt="image.png"></p>
<p>插入新节点(20)的同时，这个新节点的前驱节点(10)正在被删除。虽然新节点的插入动作完成，但是新节点实际没有插入到链表。</p>
<h3 id="并行单链表的问题2-–-删除和删除冲突"><a href="#并行单链表的问题2-–-删除和删除冲突" class="headerlink" title="并行单链表的问题2 – 删除和删除冲突"></a>并行单链表的问题2 – 删除和删除冲突</h3><ol>
<li><p>一个节点(10)被删除的同时，它的后继节点(20)也在被删除。虽然后继节点(20)的删除动作完成，但是后继节点(20)没有真的从链表中删除。<br> <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215000078.png" alt="image.png"></p>
</li>
<li><p>一个节点(20)被删除的同时，它的前驱节点(10)也在被删除。等前驱节点的删除动作完成后，节点20又回到链表中。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215143246.png" alt="image.png"></p>
</li>
</ol>
<h3 id="并行单链表的问题3-–-内存管理问题"><a href="#并行单链表的问题3-–-内存管理问题" class="headerlink" title="并行单链表的问题3 – 内存管理问题"></a>并行单链表的问题3 – 内存管理问题</h3><p>线程A把节点10从链表摘除，但是线程B还在访问节点10。线程A不能马上释放节点10的内存，必须等到线程B不再访问节点10，才能释放节点10的内存。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215211226.png" alt="image.png"></p>
<h3 id="并行单链表的问题4-–-ABA问题"><a href="#并行单链表的问题4-–-ABA问题" class="headerlink" title="并行单链表的问题4 – ABA问题"></a>并行单链表的问题4 – ABA问题</h3><p>新建的节点复用了被删除节点的内存<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215223856.png" alt="image.png"></p>
<h3 id="并行单链表的问题5-–-线性化-Linearizability-（1）"><a href="#并行单链表的问题5-–-线性化-Linearizability-（1）" class="headerlink" title="并行单链表的问题5 – 线性化 (Linearizability)（1）"></a>并行单链表的问题5 – 线性化 (Linearizability)（1）</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215230339.png" alt="image.png"></p>
<ul>
<li>t0：读者停留在Head</li>
<li>t1：读者停留在20</li>
<li>t2：读者停留在20，另一个线程插入10</li>
<li>t3：读者停留在20，另一个线程插入30</li>
<li>t4：读者停留在30<br>这样，读者看到(20,30)没有出现在历史上。<strong>读者看到的是一个不曾存在的链表。</strong></li>
</ul>
<h3 id="并行单链表的问题5-–-线性化-Linearizability-（2）"><a href="#并行单链表的问题5-–-线性化-Linearizability-（2）" class="headerlink" title="并行单链表的问题5 – 线性化 (Linearizability)（2）"></a>并行单链表的问题5 – 线性化 (Linearizability)（2）</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215241469.png" alt="image.png"></p>
<p>节点10的next指针被设置为nullptr，游标可能认为到了链表尾，会错过节点20和30。<br>解决方法：</p>
<ol>
<li>删除节点不要把它的 next 指针清空。</li>
<li>游标遍历要找到 Tail 才能认为到了尾。</li>
</ol>
<h3 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h3><ol>
<li>插入和删除冲突：有解(Harris List)</li>
<li>删除和删除冲突：有解(HarrisList)</li>
<li>内存管理问题：解法不完美(RCU和风险指针：复杂难用；引用计数：性能差)</li>
<li>ABA问题：解法不完美(tagged pointer)</li>
<li>线性化问题：按照CSDS的定义，单链表只提供三个操作：任意位置的插入、任意位置的删除和全链表范围内的查找，并且保证这三个操作是线性化的。<strong>不提供迭代器的功能，无论如何都做不到。</strong></li>
</ol>
<h3 id="观察和思路"><a href="#观察和思路" class="headerlink" title="观察和思路"></a>观察和思路</h3><p><strong>观察</strong>：都是删除惹的祸。<br><strong>思路</strong>：</p>
<ol>
<li>节点不删除（可先标记再集中批量GC,GC加锁）。<ul>
<li>查找、插入和标记都很快，但是在GC时会短暂阻塞，造成长尾。</li>
</ul>
</li>
<li>节点可删除，但是不要从内存释放（可重用，比如放在内存池）。<ul>
<li>查找、插入和删除都很快，但是内存不断增长。</li>
</ul>
</li>
<li>节点可删除，节点内存可释放。<ul>
<li>查找、插入和删除性能降低，或者实现复杂度大（且依赖于sys_membarrier)。</li>
</ul>
</li>
</ol>
<h3 id="回顾并行单链表的问题1和2"><a href="#回顾并行单链表的问题1和2" class="headerlink" title="回顾并行单链表的问题1和2"></a>回顾并行单链表的问题1和2</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215248763.png" alt="image.png"></p>
<p><strong>根本问题</strong>：在决定要删除节点10之后，节点10的next指针发生了变化。如果能够在删除节点10之前把节点10的next指针锁定（不允许其变化），就能避免上述问题。</p>
<h2 id="Harris-List"><a href="#Harris-List" class="headerlink" title="Harris List"></a>Harris List</h2><h3 id="Remove"><a href="#Remove" class="headerlink" title="Remove"></a>Remove</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215254621.png" alt="image.png"></p>
<p>用”两阶段”的方法删除节点（以删除节点20为例）：</p>
<ol>
<li>标记节点20的next指针。</li>
<li>修改前驱节点（节点10）的next指针，指向后继节点30。1比特标志放在next指针中，读写指针和标志可以做到原子。一旦标记，指针不能修改。Harris List解决了问题1和2。</li>
</ol>
<h4 id="Example-High-Level"><a href="#Example-High-Level" class="headerlink" title="Example - High Level"></a>Example - High Level</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215301481.png" alt="image.png"></p>
<ol>
<li>节点20要被删除，20的next被打上标记</li>
<li>节点10要被删除，10的next被打上标记</li>
<li>节点20删除的过程中，要把节点10的next指向30，但是发现10的next被打上标记，失败</li>
<li>节点10删除的过程中，要把head的next指向20，但是发现20的next已经被打上标记，因此顺水推舟，把head的next指向30</li>
</ol>
<h4 id="Example-Low-Level"><a href="#Example-Low-Level" class="headerlink" title="Example - Low Level"></a>Example - Low Level</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215308319.png" alt="image.png"></p>
<p>解释：</p>
<ul>
<li>Remove step 2：给节点20的next打上标记</li>
<li>Remove step 3：给节点20的prev的next赋值为30</li>
<li>Insert step 4：给节点25的prev的next赋值为25</li>
<li>Remove step 2 和 Insert step 4一定有一个全局序<ul>
<li>Remove step 2 先执行：Insert step 4失败，因为20的next已经被打上标记，因此remove成功，insert失败</li>
<li>Insert step 4 先执行：Remove step 2失败，因为20的next已经被修改了，因此insert成功，remove失败</li>
</ul>
</li>
</ul>
<h3 id="RCU-like-Solution-for-Memory-Management"><a href="#RCU-like-Solution-for-Memory-Management" class="headerlink" title="RCU-like Solution for Memory Management"></a>RCU-like Solution for Memory Management</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215314964.png" alt="image.png"></p>
<ul>
<li>每个线程有一个时间戳，在线程访问链表前，把当前时间记录在时间戳，在访问完成后清除时间戳。</li>
<li>每个线程有一个free list,节点10从链表摘除后，被放到当前线程的free list, 并记下当前时间t0。</li>
<li>当其它所有线程的时间戳都大于t0,可以把节点10从内存释放。<br><strong>这个方法好不好？</strong></li>
<li>这个方法很难实现，因为在不同core上读到的时间戳可能会有diff，很难保证时钟是严格递增的</li>
<li>ta, tb是全局的变量，怎么存在数据结构中，如果是个map，那线程id作为key，拿线程id也是额外操作，写map也需要锁，带来开销，因为要保证全局可见</li>
<li>动态库和主程序各自有一套全局变量，可能导致未必能看到这个全局变量</li>
</ul>
<h2 id="Michael-List"><a href="#Michael-List" class="headerlink" title="Michael List"></a>Michael List</h2><p>Michael List是 Harris List的改进版，它有两个算法。</p>
<ol>
<li>使用tagged pointer来解决ABA问题。采用 freelist 解决内存管理问题，实际上就是永远不释放内存。如果内存永远不释放，节点从链表摘除后依然可以访问，这时就只需要解决ABA问题。<ul>
<li>tagged pointer：之所以出现ABA问题，是复用了一块内存，并且next指向的位置也是一样的，解决方法把一个指针64bit中拿出16bit做版本号，这16个bit每次指针做改变时都加一，这样就算两个指针用的内存是一样的，但是版本号是不一样的。<br>  <strong>这个方法好不好？</strong><ul>
<li>48bit做内存地址不一定够用，但是给多了版本号又不一定够用了。</li>
</ul>
</li>
</ul>
<p>	</p>
</li>
<li>采用风险指针来解决ABA和内存管理问题（不需要tagged pointer)。</li>
</ol>
<h3 id="风险指针（Hazard-Pointers）解决问题3和4"><a href="#风险指针（Hazard-Pointers）解决问题3和4" class="headerlink" title="风险指针（Hazard Pointers）解决问题3和4"></a>风险指针（Hazard Pointers）解决问题3和4</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215325302.png" alt="image.png"></p>
<ul>
<li>每个线程把自己正在访问的节点的地址存放在全局可见的地方，比如线程A正在访问两个节点，它们的地址是P1和P2,这些全局可见的指针称为风险指针。线程在访问完这些节点之后，清除自己的风险指针。</li>
<li>每个线程有一个free list,节点10从链表摘除后，被放到当前线程的freelist。然后检查所有其它线程的风险指针，看有没有和节点10的地址相同的，如果没有，说明没有其它人正在访问节点10，节点10可以从内存释放。<br><strong>正确性</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215330139.png" alt="image.png"><br><strong>这个方法好不好？</strong></li>
<li>读者性能很差</li>
</ul>
<h2 id="Lazy-List"><a href="#Lazy-List" class="headerlink" title="Lazy List"></a>Lazy List</h2><h3 id="Remove-1"><a href="#Remove-1" class="headerlink" title="Remove"></a>Remove</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215343335.png" alt="image.png"></p>
<p>用”四阶段”的方法删除节点（以删除节点20为例）：</p>
<ol>
<li>对节点10和20加锁。</li>
<li>检查节点10和20没有被标记，而且节点10依然指向节点20。</li>
<li>标记节点20的next指针。</li>
<li>修改前驱节点（节点10）的next指针，指向后继节点30。</li>
</ol>
<h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215349219.png" alt="image.png"></p>
<p>用”三阶段”的方法插入节点（以插入节点15为例）：</p>
<ol>
<li>对节点10和20加锁。</li>
<li>检查节点10和20没有被标记，而且节点10依然指向节点20。</li>
<li>添加节点15。</li>
</ol>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215355274.png" alt="image.png"></p>
<h2 id="Shared-ptr"><a href="#Shared-ptr" class="headerlink" title="Shared_ptr"></a>Shared_ptr</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215400988.png" alt="image.png"></p>
<p>使用 atomic shared_ptr来解决内存管理的问题。对 Shared_ptr List的批评：</p>
<ol>
<li>在遍历链表时，每走过一个节点，都需要引用计数加1，离开时需要引用计数减1，而原子操作比较慢，违反了CSDS要求搜索快的原则。</li>
<li>采用递归方式释放整个链表，或者链表中的一段，有可能导致栈溢出。</li>
<li>如果一个线程获得一个节点的引用计数，但是被卡住，等了好久才释放这个引用计数，那么该节点的所有后继节点都会被延迟释放。</li>
</ol>
<h2 id="Performance-Evaluation"><a href="#Performance-Evaluation" class="headerlink" title="Performance Evaluation"></a>Performance Evaluation</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215405537.png" alt="image.png"></p>
<p>为什么lazy list效果好？因为尽管删除和插入操作加了锁，在遍历&#x2F;搜索时效率变高了</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>在不考虑内存管理的情况下，LazyList是目前最好的并行单链表，Michael List是最好的无锁单链表。</li>
<li>在考虑内存管理的情况下，目前没有工程上比较实用的解法。<ol>
<li>RCU和风险指针实现复杂，使用也不友好。我们等待它们被标准化到C++标准库，并且采用了sys_membarrier优化，这样才具备在工程上广泛使用的基础。</li>
<li>引用计数虽然使用方便，但是性能不好，违反了搜索快的原则。我们等待引用计数将来能够采用类似”weighted reference counting”之类的优化，解决性能问题。</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2024/12/23/Transformer/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。</p>
<p>在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。</p>
<p>Attention is All You Need：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<span id="more"></span>

<h2 id="1-Transformer-整体结构"><a href="#1-Transformer-整体结构" class="headerlink" title="1. Transformer 整体结构"></a>1. Transformer 整体结构</h2><p>首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204611805.png" alt="image.png"></p>
<p>Transformer 的整体结构，左图Encoder和右图Decoder</p>
<p>可以看到 <strong>Transformer 由 Encoder 和 Decoder 两个部分组成</strong>，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p>
<p><strong>第一步：</strong>获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204624351.png" alt="image.png"></p>
<p>Transformer 的输入表示</p>
<p><strong>第二步：</strong>将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。单词向量矩阵用 Xn×d 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d&#x3D;512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204644319.png" alt="image.png"></p>
<p>Transformer Encoder 编码句子信息<br><strong>第三步</strong>：将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204703053.png" alt="image.png"></p>
<p>Transofrmer Decoder 预测<br>上图 Decoder 接收了 Encoder 的编码矩阵 <strong>C</strong>，然后首先输入一个翻译开始符 “<Begin>“，预测第一个单词 “I”；然后输入翻译开始符 “<Begin>“ 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p>
<h2 id="2-Transformer-的输入"><a href="#2-Transformer-的输入" class="headerlink" title="2. Transformer 的输入"></a>2. Transformer 的输入</h2><p>Transformer 中单词的输入表示 <strong>x</strong>由<strong>单词 Embedding</strong> 和<strong>位置 Embedding</strong> （Positional Encoding）相加得到。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204753829.png" alt="image.png"></p>
<p>Transformer 的输入表示</p>
<h3 id="2-1-单词-Embedding"><a href="#2-1-单词-Embedding" class="headerlink" title="2.1 单词 Embedding"></a>2.1 单词 Embedding</h3><p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p>
<h3 id="2-2-位置-Embedding"><a href="#2-2-位置-Embedding" class="headerlink" title="2.2 位置 Embedding"></a>2.2 位置 Embedding</h3><p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。<strong>因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</strong>所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p>
<p><strong>方法一：使用[0,1]范围分配</strong></p>
<p>这个方法的分配方式是，将0-1这个范围的，将第一个token分配0，最后一个token分配去1，其余的token按照文章的长度平均分配。具体形式如下：</p>
<blockquote>
<p>我喜欢吃洋葱 【0 0.16 0.32…..1】</p>
<p>我真的不喜欢吃洋葱【0 0.125 0.25…..1】</p>
</blockquote>
<p>问题：我们可以看到，如果句子长度不同，那么位置编码是不一样，所以无法表示句子之间有什么相似性。</p>
<p><strong>方法二：1-n正整数范围分配</strong></p>
<p>这个方法比较直观，就是按照输入的顺序，一次分配给token所在的索引位置。具体形式如下：</p>
<blockquote>
<p>我喜欢吃洋葱 【1，2，3，4，5，6】</p>
<p>我真的不喜欢吃洋葱【1，2，3，4，5，6，7】</p>
</blockquote>
<p>问题：往往句子越长，后面的值越大，数字越大说明这个位置占的权重也越大，这样的方式无法凸显每个位置的真实的权重。</p>
<p><strong>方法三：三角函数表示</strong></p>
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204808623.png" alt="image.png"></p>
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p>
<ul>
<li><p>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</p>
</li>
<li><p>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) &#x3D; Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) &#x3D; Cos(A)Cos(B) - Sin(A)Sin(B)。</p>
</li>
</ul>
<p>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <strong>x</strong>，<strong>x</strong> 就是 Transformer 的输入。</p>
<h2 id="3-Self-Attention（自注意力机制）"><a href="#3-Self-Attention（自注意力机制）" class="headerlink" title="3. Self-Attention（自注意力机制）"></a>3. Self-Attention（自注意力机制）</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204817764.png" alt="image.png"></p>
<p>Transformer Encoder 和 Decoder</p>
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 <strong>Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p>
<h3 id="3-1-Self-Attention-结构"><a href="#3-1-Self-Attention-结构" class="headerlink" title="3.1 Self-Attention 结构"></a>3.1 Self-Attention 结构</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204827884.png" alt="image.png"></p>
<p>Self-Attention 结构</p>
<p>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵<strong>Q(查询),K(键值),V(值)<strong>。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而</strong>Q,K,V</strong>正是通过 Self-Attention 的输入进行线性变换得到的。</p>
<h3 id="3-2-Q-K-V-的计算"><a href="#3-2-Q-K-V-的计算" class="headerlink" title="3.2 Q, K, V 的计算"></a>3.2 Q, K, V 的计算</h3><p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵<strong>WQ,WK,WV</strong>计算得到<strong>Q,K,V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204837811.png" alt="image.png"></p>
<p>Q, K, V 的计算</p>
<h3 id="3-3-Self-Attention-的输出"><a href="#3-3-Self-Attention-的输出" class="headerlink" title="3.3 Self-Attention 的输出"></a>3.3 Self-Attention 的输出</h3><p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204845263.png" alt="image.png"></p>
<p>Self-Attention 的输出</p>
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以 dk 的平方根。<strong>Q</strong>乘以<strong>K</strong>的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<strong>Q</strong>乘以 KT ，1234 表示的是句子中的单词。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204851643.png" alt="image.png"></p>
<p>Q乘以K的转置的计算</p>
<p>得到QKT 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204858967.png" alt="image.png"></p>
<p>对矩阵的每一行进行 Softmax</p>
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204907248.png" alt="image.png"></p>
<p>Self-Attention 输出</p>
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 Z1 等于所有单词 i 的值 Vi 根据 attention 系数的比例加在一起得到，如下图所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204917105.png" alt="image.png"></p>
<p>Zi 的计算方法</p>
<h3 id="3-4-Multi-Head-Attention"><a href="#3-4-Multi-Head-Attention" class="headerlink" title="3.4 Multi-Head Attention"></a>3.4 Multi-Head Attention</h3><p>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204924298.png" alt="image.png"></p>
<p>Multi-Head Attention</p>
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。下图是 h&#x3D;8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204931261.png" alt="image.png"></p>
<p>多个 Self-Attention</p>
<p>得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)<strong>，然后传入一个</strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223204940131.png" alt="image.png"></p>
<p>Multi-Head Attention 的输出</p>
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的。</p>
<p>具体来说，单头自注意力机制将所有位置的特征向量都看作等价的，而多头自注意力机制能够在不同的“视角”下对输入进行建模。通过将输入的特征向量划分成$h$个多头，模型能够在$h$个不同的子空间上计算注意力，从而能够学习到多个不同的、互补的特征表示，从而更加<strong>全面地捕捉输入序列的语义信息</strong>。例如，对于一句话来说，不同的多头可以学习到句子的不同方面，如主语、宾语、谓语、修饰语等，从而能够更好地表示句子的语义信息。</p>
<p>此外，多头自注意力机制还可以<strong>并行计算</strong>，因为每个头的注意力计算是独立的，可以并行地进行。这在计算效率上有一定的优势，可以加速模型的训练和推理过程。</p>
<p>需要注意的是，在多头自注意力机制中，头的数量$h$需要根据任务的复杂度和数据集的规模进行调整，过多或过少的头都可能会影响模型的性能。通常情况下，头的数量$h$在4-16之间较为常见。</p>
<h2 id="4-Encoder-结构"><a href="#4-Encoder-结构" class="headerlink" title="4. Encoder 结构"></a>4. Encoder 结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205007335.png" alt="image.png"></p>
<p>Transformer Encoder block</p>
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, <strong>Add &amp; Norm, Feed Forward, Add &amp; Norm</strong> 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p>
<h3 id="4-1-Add-Norm"><a href="#4-1-Add-Norm" class="headerlink" title="4.1 Add &amp; Norm"></a>4.1 Add &amp; Norm</h3><p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205016584.png" alt="image.png"></p>
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X</strong> 维度是一样的，所以可以相加)。</p>
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，将子层的输出和输入进行残差连接（residual connection），并进行元素级别的加法，得到增强了的特征表示。这个残差连接可以有效地防止梯度消失问题，避免训练过程中信息的损失。在 ResNet 中经常用到：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205023109.png" alt="image.png"></p>
<p>残差连接</p>
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，进行归一化操作，以缩放增强后的特征表示，并减少内部协变量位移（Internal Covariate Shift），从而加快模型的收敛速度和提高性能。</p>
<h3 id="4-2-Feed-Forward"><a href="#4-2-Feed-Forward" class="headerlink" title="4.2 Feed Forward"></a>4.2 Feed Forward</h3><p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205033278.png" alt="image.png"></p>
<p>Feed Forward</p>
<p><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与<strong>X</strong>一致。</p>
<p>Feed Forward层由两个全连接层组成，中间用一个非线性激活函数进行连接，如ReLU（Rectified Linear Unit）激活函数。在Transformer模型中，第一个全连接层将特征表示映射到一个更高维度的空间，可以增强模型的非线性建模能力，提高模型的性能和泛化能力；第二个全连接层将其再次映射回原始的维度，从而降低模型的计算复杂度和内存占用。</p>
<h3 id="4-3-组成-Encoder"><a href="#4-3-组成-Encoder" class="headerlink" title="4.3 组成 Encoder"></a>4.3 组成 Encoder</h3><p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵 X(n×d) ，并输出一个矩阵 O(n×d) 。通过多个 Encoder block 叠加就可以组成 Encoder。</p>
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205045993.png" alt="image.png"></p>
<p>Encoder 编码句子信息</p>
<h2 id="5-Decoder-结构"><a href="#5-Decoder-结构" class="headerlink" title="5. Decoder 结构"></a>5. Decoder 结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205051556.png" alt="image.png"></p>
<p>Transformer Decoder block</p>
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p>
<ul>
<li><p>包含两个 Multi-Head Attention 层。</p>
</li>
<li><p>第一个 Multi-Head Attention 层采用了 Masked 操作。</p>
</li>
<li><p>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</p>
</li>
<li><p>最后有一个 Softmax 层计算下一个翻译单词的概率。</p>
</li>
</ul>
<h3 id="5-1-第一个-Multi-Head-Attention"><a href="#5-1-第一个-Multi-Head-Attention" class="headerlink" title="5.1 第一个 Multi-Head Attention"></a>5.1 第一个 Multi-Head Attention</h3><p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。</p>
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “<Begin>“ 预测出第一个单词为 “I”，然后根据输入 “<Begin> I” 预测下一个单词 “have”。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205108154.png" alt="image.png"></p>
<p>Decoder 预测</p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “<Begin> I have a cat <end>“。</strong></p>
<p><strong>第一步：</strong>是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 “<Begin> I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205119517.png" alt="image.png"></p>
<p>输入矩阵与 Mask 矩阵</p>
<p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵<strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和 KT 的乘积 QKT 。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205125576.png" alt="image.png"></p>
<p>Q乘以K的转置</p>
<p><strong>第三步：</strong>在得到 QKT 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205132032.png" alt="image.png"></p>
<p>Softmax 之前 Mask</p>
<p>得到 <strong>Mask</strong> QKT 之后在 <strong>Mask</strong> QKT上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p><strong>第四步：</strong>使用 <strong>Mask</strong> QKT与矩阵 <strong>V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 Z1 是只包含单词 1 信息的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205141462.png" alt="image.png"></p>
<p>Mask 之后的输出</p>
<p><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 Zi ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出Zi 然后计算得到第一个 Multi-Head Attention 的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p>
<h3 id="5-2-第二个-Multi-Head-Attention"><a href="#5-2-第二个-Multi-Head-Attention" class="headerlink" title="5.2 第二个 Multi-Head Attention"></a>5.2 第二个 Multi-Head Attention</h3><p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C</strong> 计算的。</p>
<p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出 <strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p>
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p>
<h3 id="5-3-Softmax-预测输出单词"><a href="#5-3-Softmax-预测输出单词" class="headerlink" title="5.3 Softmax 预测输出单词"></a>5.3 Softmax 预测输出单词</h3><p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205150944.png" alt="image.png"></p>
<p>Decoder Softmax 之前的 Z</p>
<p>Softmax 根据输出矩阵的每一行预测下一个单词：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223205156309.png" alt="image.png"></p>
<p>Decoder Softmax 预测</p>
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p>
<h2 id="6-Transformer-总结"><a href="#6-Transformer-总结" class="headerlink" title="6. Transformer 总结"></a>6. Transformer 总结</h2><ul>
<li><p>Transformer 与 RNN 不同，可以比较好地并行训练。</p>
</li>
<li><p>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</p>
</li>
<li><p>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q, K, V</strong>矩阵通过输出进行线性变换得到。</p>
</li>
<li><p>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>并行无锁数据结构(下) - 跳表, LRU Cache, FIFO队列</title>
    <url>/2024/12/16/%E5%B9%B6%E8%A1%8C%E6%97%A0%E9%94%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%B8%8B-%E8%B7%B3%E8%A1%A8-LRU-Cache-FIFO%E9%98%9F%E5%88%97/</url>
    <content><![CDATA[<h1 id="Skip-List"><a href="#Skip-List" class="headerlink" title="Skip List"></a>Skip List</h1><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>跳表比起树, 对并行化更友好<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215544008.png" alt="image.png"></p>
<span id="more"></span>
<h2 id="并行跳表的两种实现"><a href="#并行跳表的两种实现" class="headerlink" title="并行跳表的两种实现"></a>并行跳表的两种实现</h2><ol>
<li>直面并行性的问题，硬解各种冲突。由于跳表可以看成是多个链表的集合，所以每种并行单链表算法都能衍生出一个并行跳表算法。代表算法是：Fraser Skip List，Fomitchev Skip List和Lazy Skip List。</li>
<li>规避并行性，把塔的升降交给单独线程来管理，代表算法是：No-Hot-Spot Skip List和 Rotating Skip List。</li>
</ol>
<h3 id="Fraser-Skip-List"><a href="#Fraser-Skip-List" class="headerlink" title="Fraser Skip List"></a>Fraser Skip List</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215558305.png" alt="image.png"></p>
<ul>
<li>每一层链表都是独立的，可以利用Harris List做到无锁。</li>
<li>新节点先插入最底层链表，然后逐级往上长高，每长高一级就是往上一级链表中插入一个新节点。每次插入都是无锁的。长高的过程可以逐级进行，不需要整体上保持原子性，但必须从下往上升。释放节点必须从上往下逐级降低，反其道而行。</li>
<li>Fraser Skip List潜在的问题：塔下降时不能保证上层链表是下层链表的子集。</li>
</ul>
<h3 id="No-Hot-Spot-Skip-List"><a href="#No-Hot-Spot-Skip-List" class="headerlink" title="No-Hot-Spot Skip List"></a>No-Hot-Spot Skip List</h3><h4 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215604974.png" alt="image.png"></p>
<p>新节点只插入Base链表。然后由一个辅助线程在适当的时候对跳表进行整理。整理的过程就是把每个节点长高。这个整理是由单线程完成，因此不必考虑写写冲突，而且塔高度的分配是确定的。</p>
<h4 id="Remove"><a href="#Remove" class="headerlink" title="Remove"></a>Remove</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215614936.png" alt="image.png"></p>
<h3 id="Performance-Evaluation"><a href="#Performance-Evaluation" class="headerlink" title="Performance Evaluation"></a>Performance Evaluation</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215623612.png" alt="image.png"></p>
<h1 id="LRU-Cache"><a href="#LRU-Cache" class="headerlink" title="LRU Cache"></a>LRU Cache</h1><h3 id="Key-Value-Cache"><a href="#Key-Value-Cache" class="headerlink" title="Key-Value Cache"></a>Key-Value Cache</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215632441.png" alt="image.png"></p>
<ul>
<li>每个桶都是一个跳表，插入和查找并行，支持标记删除（假设valuesize远大于keysize)。</li>
<li>当一个跳表中被标记的节点太多时，采取如下三步做物理删除：<ol>
<li>创建一个新的跳表1a,把跳表1中的节点拷贝到1a(指针拷贝)。</li>
<li>将指向bucket1的指针p从1切换到1a,切换需要加锁，且导致少量数据丢失。</li>
<li>释放老的bucket1。</li>
</ol>
</li>
</ul>
<h3 id="Cache替换策略-近似LRU"><a href="#Cache替换策略-近似LRU" class="headerlink" title="Cache替换策略 - 近似LRU"></a>Cache替换策略 - 近似LRU</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215638764.png" alt="image.png"></p>
<ul>
<li>每个节点包含一个count,初值是100。该节点每次被用户读到，它的count加一。</li>
<li>单独的线程在适当的时候遍历所有节点，把每个节点的count减少delta,如果结果小于等于0，就逻辑删除该节点。</li>
<li>delta初始值是1。每次遍历后，线程都将知道Cache中count最小的n个节点，在这些count中挑选一个作为下一次的delta(如果想加快回收，可以把delta调大）。</li>
<li>对count规定一个上限（比如1000），到达1000以后count就不再增加。防止短时间内大量访问导致该节点在变冷后很长时间不能删除。</li>
</ul>
<h3 id="有关-Cache-的结论"><a href="#有关-Cache-的结论" class="headerlink" title="有关 Cache 的结论"></a>有关 Cache 的结论</h3><ul>
<li>只有一个临界区，而且很小（几条赋值指令），不常发生。</li>
<li>Cache的整体性能接近无锁，同时避免了内存管理的复杂性，实现简单，正确性容易推理。</li>
<li>在新跳表替换老跳表的瞬间，可能有数据丢失，但是对于Cache不是问题。（如果愿意，可以在替换后再次遍历老跳表，补回丢失的数据）。</li>
<li>使用count的方法，替换策略近似LRU,但不必维护全局LRU链表，因此并行度高。</li>
</ul>
<h1 id="FIFO-Queue"><a href="#FIFO-Queue" class="headerlink" title="FIFO Queue"></a>FIFO Queue</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215646802.png" alt="image.png"></p>
<p>功能（没有查询操作，比单链表简单）：</p>
<ol>
<li>从头删除</li>
<li>从尾插入<br>需要解决2个问题：</li>
<li>空队列（空队列的插入和删除互相冲突）</li>
<li>ABA问题<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215652387.png" alt="image.png"></li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241216215658408.png" alt="image.png"></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>多处理器编程中的一致性问题</title>
    <url>/2024/12/23/%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/reliveIT/article/details/105902477?spm=1001.2014.3001.5501">Why Memory Barriers？中文翻译（上）</a><br><a href="https://zhuanlan.zhihu.com/p/48157076">高并发编程–多处理器编程中的一致性问题(上)</a><br><a href="https://zhuanlan.zhihu.com/p/48161056">高并发编程–多处理器编程中的一致性问题(下)</a><br><a href="https://luyuhuang.tech/2022/06/25/cpp-memory-order.html">谈谈 C++ 中的内存顺序 (Memory Order)</a><br><a href="https://www.zhihu.com/column/c_1634220655800573952">C++ 相关知识总结分享</a></p>
<h1 id="Memory-Barrier"><a href="#Memory-Barrier" class="headerlink" title="Memory Barrier"></a>Memory Barrier</h1><h2 id="为什么需要内存屏障？"><a href="#为什么需要内存屏障？" class="headerlink" title="为什么需要内存屏障？"></a>为什么需要内存屏障？</h2><p>由于CPU的速度要快于（数量级上的差异）memory以及他们之间的互连器件（interconnect），因此引入了 Store Buffer 和 Invalidate Queues， 可能会导致多线程下 Inconsistency 的情况</p>
<h2 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113405743.png" alt="image.png"></p>
<span id="more"></span>

<h2 id="Cache-Coherency-Protocols-MSEI"><a href="#Cache-Coherency-Protocols-MSEI" class="headerlink" title="Cache Coherency Protocols - MSEI"></a>Cache Coherency Protocols - MSEI</h2><h3 id="为什么需要缓存一致性协议？"><a href="#为什么需要缓存一致性协议？" class="headerlink" title="为什么需要缓存一致性协议？"></a>为什么需要缓存一致性协议？</h3><p>cache独占导致的数据不一致：因为每个core都有自己独立的L1 cache，对于一个共享的memory location两个cache可以有自己的copy。那么这就会出现了数据不一致的状况，两个core可能同时访问这个memory location，且如果一个core是对这个memory location进行修改，那么这就需要两边的cache进行同步，防止数据不一致。这个工作就是cache coherence protocol要做的事情。</p>
<h3 id="Cacheline-State"><a href="#Cacheline-State" class="headerlink" title="Cacheline State"></a><strong>Cacheline State</strong></h3><ul>
<li><strong>Modified(M):</strong><br>  当一个core 的cacheline的状态是M时，说明当前core最近修改了这个cache，那么其他core的cache不能再修改当前cache line对应的memory location，除非该cache将这个修改同步到了memory。这个core对这个memory location可以理解为Owned。</li>
<li><strong>Exclusive(E):</strong><br>  E这个状态与M很像，区别在于当前core并没有修改当前的cacheline，这意味着当前cacheline存储的memory location的值是最新的。当前core可以对该cacheline进行modify且不需要与其他core的cache同步。这个core对这个memory location可以理解为Owned。</li>
<li><strong>Share(S):</strong><br>  S表示当前cacheline在其他core的cache也存在copy，当前core如果需要修改该cacheline则需要与其他core的cache进行提前沟通。</li>
<li><strong>Invalid(I):</strong><br>  I表示当前cacheline是空的。</li>
</ul>
<h3 id="Protocol-Message"><a href="#Protocol-Message" class="headerlink" title="Protocol Message"></a>Protocol Message</h3><ul>
<li><strong>Read</strong><br>  当一个cache需要读取某个cacheline消息的时候就会发起read消息。</li>
<li><strong>Read Response</strong><br>  read response是read的回应，这response可以来自其他core的cache也可以来自memory。当其他core中对当前cacheline是M状态时，则会发起read response。</li>
<li><strong>Invalidate</strong><br>  Invalidate消息包含对应的memory location，接收到这个消息的cache需要将自己cacheline内容剔除，并响应。</li>
<li><strong>Invalidate Acknowledge</strong><br>  接收到Invalidate后删除cacheline中的数据就向发起者回复invalidate ack。</li>
<li><strong>Read Invalidate</strong><br>  这个消息包含两个操作，read和invalidate，那么它也需要接收read response和多个invalidate ack响应。</li>
<li><strong>Write Back</strong><br>  writeback包含数据和地址，会将这个地址对应的数据刷到内存中。</li>
</ul>
<h3 id="State-Machine"><a href="#State-Machine" class="headerlink" title="State Machine"></a>State Machine</h3><ul>
<li>**Modified(M) -&gt; Exclusive(E)**：cache可以通过writeback transaction将一个cacheline的数据写回到memory中（或者下一级cache中），这时候，该cacheline的状态从Modified迁移到Exclusive状态。对于cpu而言，cacheline中的数据仍然是最新的，而且是该cpu独占的，因此可以不通知其他cpu cache而直接修改之。</li>
<li>**Exclusive(E) -&gt; Modified(M)**：在Exclusive状态下，cpu可以直接将数据写入cacheline，不需要其他操作。相应的，该cacheline状态从Exclusive状态迁移到Modified状态。这个状态迁移过程不涉及bus上的Transaction（即无需MESI Protocol Messages的交互）。</li>
<li>**Modified(M) -&gt; Invalid(I)**：CPU 在总线上收到一个read invalidate的请求，同时，该请求是针对一个处于modified状态的cacheline，在这种情况下，CPU必须该cacheline状态设置为无效，并且用read response”和“invalidate acknowledge来回应收到的read invalidate的请求，完成整个bus transaction。一旦完成这个transaction，数据被送往其他cpu cache中，本地的copy已经不存在了。</li>
<li>**Invalid(I) -&gt; Modified(M)**：CPU需要执行一个原子的readmodify-write操作，并且其cache中没有缓存数据，这时候，CPU就会在总线上发送一个read invalidate用来请求数据，同时想独自霸占对该数据的所有权。该CPU的cache可以通过read response获取数据并加载cacheline，同时，为了确保其独占的权利，必须收集所有其他cpu发来的invalidate acknowledge之后（其他cpu没有local copy），完成整个bus transaction。</li>
<li>**Share(S) -&gt; Modified(M)**：CPU需要执行一个原子的readmodify-write操作，并且其local cache中有read only的缓存数据（cacheline处于shared状态），这时候，CPU就会在总线上发送一个invalidate请求其他cpu清空自己的local copy，以便完成其独自霸占对该数据的所有权的梦想。同样的，该cpu必须收集所有其他cpu发来的invalidate acknowledge之后，才算完成整个bus transaction。</li>
<li>**Modified(M) -&gt; Share(S)**：在本cpu独自享受独占数据的时候，其他的cpu发起read请求，希望获取数据，这时候，本cpu必须以其local cacheline的数据回应，并以read response回应之前总线上的read请求。这时候，本cpu失去了独占权，该cacheline状态从Modified状态变成shared状态（有可能也会进行写回的动作）。</li>
<li>**Exclusive(E) -&gt; Share(S)**：这个迁移和f类似，只不过开始cacheline的状态是exclusive，cacheline和memory的数据都是最新的，不存在写回的问题。总线上的操作也是在收到read请求之后，以read response回应。</li>
<li>**Share(S) -&gt; Exclusive(E)**：如果cpu认为自己很快就会启动对处于shared状态的cacheline进行write操作，因此想提前先霸占上该数据。因此，该cpu会发送invalidate敦促其他cpu清空自己的local copy，当收到全部其他cpu的invalidate acknowledge之后，transaction完成，本cpu上对应的cacheline从shared状态切换exclusive状态。还有另外一种方法也可以完成这个状态切换：当所有其他的cpu对其local copy的cacheline进行写回操作，同时将cacheline中的数据设为无效（主要是为了为新的数据腾些地方），这时候，本cpu坐享其成，直接获得了对该数据的独占权。</li>
<li>**Exclusive(E) -&gt; Invalid(I)**：其他的CPU进行一个原子的read-modify-write操作，但是，数据在本cpu的cacheline中，因此，其他的那个CPU会发送read invalidate，请求对该数据以及独占权。本cpu回送read response”和“invalidate acknowledge”，一方面把数据转移到其他cpu的cache中，另外一方面，清空自己的cacheline。</li>
<li>**Invalid(I) -&gt; Exclusive(E)**：cpu想要进行write的操作但是数据不在local cache中，因此，该cpu首先发送了read invalidate启动了一次总线transaction。在收到read response回应拿到数据，并且收集所有其他cpu发来的invalidate acknowledge之后（确保其他cpu没有local copy），完成整个bus transaction。当write操作完成之后，该cacheline的状态会从Exclusive状态迁移到Modified状态。</li>
<li>**Invalid(I) -&gt; Share(S)**：本CPU执行读操作，发现local cache没有数据，因此通过read发起一次bus transaction，来自其他的cpu local cache或者memory会通过read response回应，从而将该cacheline从Invalid状态迁移到shared状态。</li>
<li>**Share(S) -&gt; Invalid(I)**：当cacheline处于shared状态的时候，说明在多个cpu的local cache中存在副本，因此，这些cacheline中的数据都是read only的，一旦其中一个cpu想要执行数据写入的动作，必须先通过invalidate获取该数据的独占权，而其他的CPU会以invalidate acknowledge回应，清空数据并将其cacheline从shared状态修改成invalid状态。</li>
</ul>
<h3 id="MESI-Protocol-Example"><a href="#MESI-Protocol-Example" class="headerlink" title="MESI Protocol Example"></a>MESI Protocol Example</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113559619.png" alt="image.png"></p>
<p>第一列是操作序列号，第二列是执行操作的CPU，第三列是具体执行哪一种操作，第四列描述了各个cpu local cache中的cacheline的状态（用meory address&#x2F;状态表示），最后一列描述了内存在0地址和8地址的数据内容的状态：V表示是最新的，和cache一致，I表示不是最新的内容，最新的内容保存在cache中。<br><strong>sequence 0</strong>，各个cpu cache中的cacheline都是Invalid状态，而Memory中的数据都保存了最新的数据。<br><strong>sequence 1</strong>，CPU 0执行了load操作，将address 0的数据加载到寄存器，这个操作使得保存0地址数据的那个cacheline从invalid状态迁移到shared状态。<br><strong>sequence 2</strong>，CPU3也对0地址执行了load操作，导致其local cache上对应的cacheline也切换到shared状态。当然，这时候，memory仍然是最新的。<br><strong>sequence 3</strong>，CPU 0执行了对地址8的load操作，由于地址0和地址8都是选择同一个cache set，而且，我们之前已经说过，该cache是direct-mapped的（即每个set只有一个cacheline），因此需要首先清空该cacheline中的数据（该操作被称为Invalidation），由于cacheline的状态是shared，因此，不需要通知其他CPU。Invalidation local cache上的cacheline之后，cpu 0的load操作将该cacheline状态修改成Shared状态（保存地址8的数据）。<br><strong>sequence 4</strong>，CPU 2也开始执行load操作了，虽然是load操作，但是CPU知道程序随后会修改该值（不是原子操作的read-modify-write，否就是迁移到Modified状态了，也不是单纯的load操作，否则会迁移到shared状态），因此向总线发送了read invalidate命令，一方面获取该数据（自己的local cache中没有地址0的数据），另外，CPU 2想独占该数据（因为随后要write）。这个操作导致CPU 3的cacheline迁移到invalid状态。当然，这时候，memory仍然是最新的有效数据。<br><strong>Sequence 5</strong>，CPU 2的store操作很快到来，由于准备工作做的比较充分（Exclusive状态，独占该数据），cpu直接修改cacheline中的数据（对应地址0），从而将其状态迁移到modified状态，同时要注意的是：memory中的数据已经失效，不是最新的数据了，任何其他CPU发起对地址0的load操作都不能从memory中读取，而是通过嗅探（snoop）的方式从CPU 2的local cache中获取。<br><strong>sequence 6</strong>，CPU 1对地址0的数据执行原子的加1操作，这时候CPU 1会发出read invalidate命令，将地址0的数据从CPU 2的cacheline中嗅探得到，同时通过invalidate其他CPU local cache的内容而获得独占性的数据访问权。这时候，CPU 2中的cacheline状态变成invalid状态，而CPU 1将从invalid状态迁移到modified状态。<br><strong>sequence 7</strong>，CPU 1对地址8进行load操作，由于cacheline被地址0占据，因此需要首先将其驱逐出cache，于是执行write back操作将地址0的数据写回到memory，同时发送read命名，从CPU 0的cache中获得数据加载其cacheline，最后，CPU1的cache变成shared状态（保存地址8的数据）。由于执行了write back操作，memory中地址0的数据又变成最新的有效数据了。</p>
<h2 id="Store-Buffer"><a href="#Store-Buffer" class="headerlink" title="Store Buffer"></a>Store Buffer</h2><h3 id="为什么要引入-store-buffer"><a href="#为什么要引入-store-buffer" class="headerlink" title="为什么要引入 store buffer?"></a>为什么要引入 store buffer?</h3><p>cpu 0发起一次对某个地址的写操作，但是local cache没有数据，该数据在CPU 1的local cache中，因此，为了完成写操作，CPU 0发出invalidate的命令，invalidate其他CPU的cache数据。只有完成了这些总线上的transaction之后，CPU 0才能正在发起写的操作，这是一个漫长的等待过程。但是，其实没必要等待这么长的时间，毕竟，物理CPU 1中的cacheline保存有什么样子的数据，其实都没有意义，这个值都会被CPU 0新写入的值覆盖的。</p>
<h3 id="什么是-store-buffer"><a href="#什么是-store-buffer" class="headerlink" title="什么是 store buffer?"></a><strong>什么是 store buffer</strong>?</h3><p>有一种可以阻止cpu进入无聊等待状态的方法就是在CPU和cache之间增加store buffer这个HW block，如下图所示：<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113619007.png" alt="image.png"></p>
<p>一旦增加了store buffer，那么cpu0无需等待其他CPU的相应，只需要将要修改的内容放入store buffer，然后继续执行就OK了。当cacheline完成了bus transaction，并更新了cacheline的状态后，要修改的数据将从store buffer进入cacheline。这些store buffer对于cpu而言是local的</p>
<h3 id="会引入什么新的问题？"><a href="#会引入什么新的问题？" class="headerlink" title="会引入什么新的问题？"></a><strong>会引入什么新的问题？</strong></h3><ul>
<li><p><strong>存在多个数据副本（store buffer一份，CPU cache一份）- store forwarding 解决</strong><br>  当CPU执行load操作的时候，不但要看cache，还有看store buffer是否有内容，如果store buffer有该数据，那么就采用store buffer中的值。因此，即便是store操作还没有写入cacheline，store forwarding的效果看起来就好象cpu的store操作被向前传递了一样（后面的load的指令可以感知到这个store操作） 。</p>
</li>
<li><p><strong>导致存储系统重排序</strong>- <strong>memory barriers解决</strong><br>  因为现代计算的可见性是通过锁cache+cache coherency protocol缓存一致性协议来解决的，引入了store buffer，program order代码顺序上先写的因为需要bus transaction，所以写到store buffer，从而导致后写的先写入cache可见，这就是存储系统重排序导致的可见性问题。</p>
<p>  看这个例子：</p>
  <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>(); <span class="comment">// MODIFIED: memory barrier</span></span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  我们假设CPU 0执行foo函数，CPU 1执行bar函数。我们再进一步假设a变量在CPU 1的cache中，b在CPU 0 cache中，执行的操作序列如下</p>
<ol>
<li>CPU 0执行a&#x3D;1的赋值操作，由于a不在local cache中，因此，CPU 0将a值放到store buffer中之后，发送了read invalidate命令到总线上去。</li>
<li>CPU 1执行 while (b &#x3D;&#x3D; 0) 循环，由于b不在CPU 1的cache中，因此，CPU发送一个read message到总线上，看看是否可以从其他cpu的local cache中或者memory中获取数据</li>
<li>CPU 0继续执行b&#x3D;1的赋值语句，由于b就在自己的local cache中（cacheline处于modified状态或者exclusive状态），因此CPU0可以直接操作将新的值1写入cache line。</li>
<li>CPU 0收到了read message，将最新的b值”1“回送给CPU 1，同时将b cacheline的状态设定为shared</li>
<li>CPU 1收到了来自CPU 0的read response消息，将b变量的最新值”1“值写入自己的cacheline，状态修改为shared。</li>
<li>由于b值等于1了，因此CPU 1跳出while (b &#x3D;&#x3D; 0)的循环，继续前行。</li>
<li>CPU 1执行assert(a &#x3D;&#x3D; 1)，这时候CPU 1的local cache中还是旧的a值，因此assert(a &#x3D;&#x3D; 1)失败。</li>
<li>CPU 1收到了来自CPU 0的read invalidate消息，以a变量的值进行回应，同时清空自己的cacheline，但是这已经太晚了。</li>
<li>CPU 0收到了read response和invalidate ack的消息之后，将store buffer中的a的最新值”1“数据写入cacheline，然并卵，CPU 1已经assertion fail了。<br>  但是在CPU设计层面是无法判断当前core中执行的变量是否与其他的core中的变量存在关系，因为CPU在执行代码的时候他认为这个当前所执行程序就是一个单线程的，他无法感知多线程的存在。因此这个问题无法在CPU设计层面解决，这个就需要编码人员介入了，编码人员需要告诉CPU现在需要将storebuffer flush到cache里，于是CPU设计者提供了叫memory barrier的工具。<br>  smp_mb()会在执行的时候将storebuffer中的数据全部刷进cache。这样assert就会执行成功了。</li>
</ol>
</li>
</ul>
<h2 id="Invalidate-Queues"><a href="#Invalidate-Queues" class="headerlink" title="Invalidate Queues"></a>Invalidate Queues</h2><h3 id="为什么需要-Invalidate-Queues"><a href="#为什么需要-Invalidate-Queues" class="headerlink" title="为什么需要 Invalidate Queues?"></a><strong>为什么需要 Invalidate Queues?</strong></h3><p>store buffer的容量是有限的，如果出现大量的bus transaction，把store buffer打满了，也会导致CPU流水线阻塞. 在这种状况下，CPU只能又进入等待状态，直到cache line完成invalidation和ack的交互之后，可以将store buffer的entry写入cacheline，从而为新的store让出空间之后，CPU才可以继续执行。这种状况也可能发生在调用了memory barrier指令之后，因为一旦store buffer中的某个entry被标记了，那么随后的store都必须等待invalidation完成，因此不管是否cache miss，这些store都必须进入store buffer。</p>
<p>引入invalidate queues可以缓解这个状况。store buffer之所以很容易被填充满，主要是其他CPU回应invalidate acknowledge比较慢，如果能够加快这个过程，让store buffer尽快进入cacheline，那么也就不会那么容易填满了。</p>
<p>CPU其实不需要完成invalidate操作就可以回送acknowledgement消息，这样，就不会阻止发生invalidate请求的那个CPU进入无聊的等待状态。CPU可以buffer这些invalidate message（放入Invalidate Queues），然后直接回应acknowledgement，表示自己已经收到请求，随后会慢慢处理。当然，再慢也要有一个度，例如对a变量cacheline的invalidate处理必须在该CPU发送任何关于a变量对应cacheline的操作到bus之前完成。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113629110.png" alt="image.png"></p>
<h3 id="Example-for-Invalidate-Queues"><a href="#Example-for-Invalidate-Queues" class="headerlink" title="Example for Invalidate Queues"></a><strong>Example for Invalidate Queues</strong></h3><p>之前的代码引入 Invalidate Queues 会带来新的问题</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>();</span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">smp_mb</span>(); <span class="comment">// MODIFIED: memory barrier</span></span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>CPU 0执行a&#x3D;1的赋值操作，由于a在CPU 0 local cache中的cacheline处于shared状态，因此，CPU 0将a的新值“1”放入store buffer，并且发送了invalidate消息去清空CPU 1对应的cacheline。</li>
<li>CPU 1执行while (b &#x3D;&#x3D; 0)的循环操作，但是b没有在local cache，因此发送read消息试图获取该值。</li>
<li>CPU 1收到了CPU 0的invalidate消息，放入Invalidate Queue，并立刻回送Ack。</li>
<li>CPU 0收到了CPU 1的invalidate ACK之后，即可以越过程序设定内存屏障（第四行代码的smp_mb() ），这样a的新值从store buffer进入cacheline，状态变成Modified。</li>
<li>CPU 0 越过memory barrier后继续执行b&#x3D;1的赋值操作，由于b值在CPU 0的local cache中，因此store操作完成并进入cache line。</li>
<li>CPU 0收到了read消息后将b的最新值“1”回送给CPU 1，并修正该cacheline为shared状态。</li>
<li>CPU 1收到read response，将b的最新值“1”加载到local cacheline。</li>
<li>对于CPU 1而言，b已经等于1了，因此跳出while (b &#x3D;&#x3D; 0)的循环，继续执行后续代码</li>
<li>CPU 1执行assert(a &#x3D;&#x3D; 1)，但是由于这时候CPU 1 cache的a值仍然是旧值0，因此assertion 失败</li>
<li>该来总会来，Invalidate Queue中针对a cacheline的invalidate消息最终会被CPU 1执行，将a设定为无效，但素，大错已经酿成。<br>很明显，在上文中的场景中，加速Invalidation response导致foo函数中的memory barrier失效了，因此，这时候对Invalidation response已经没有意义了，毕竟程序逻辑都错了。怎么办？其实我们可以让memory barrier指令和Invalidate Queue进行交互来保证确定的memory order。具体做法是这样的：当CPU执行memory barrier指令的时候，对当前Invalidate Queue中的所有的entry进行标注，这些被标注的项次被称为marked entries，而随后CPU执行的任何的load操作都需要等到Invalidate Queue中所有marked entries完成对cacheline的操作之后才能进行。因此，要想保证程序逻辑正确，我们需要给bar函数增加内存屏障的操作<br>程序修改之后，我们再来看看CPU的执行序列：<br>   1 - 8 相同</li>
<li>CPU 1现在不能继续执行代码，只能等待，直到Invalidate Queue中的message被处理完成</li>
<li>CPU 1处理队列中缓存的Invalidate消息，将a对应的cacheline设置为无效。</li>
<li>由于a变量在local cache中无效，因此CPU 1在执行assert(a &#x3D;&#x3D; 1)的时候需要发送一个read消息去获取a值。<br>1. CPU 0用a的新值1回应来自CPU 1的请求。<br>2. CPU 1获得了a的新值，并放入cacheline，这时候assert(a &#x3D;&#x3D; 1)不会失败了。</li>
</ol>
<h2 id="Read-and-Write-Memory-Barriers"><a href="#Read-and-Write-Memory-Barriers" class="headerlink" title="Read and Write Memory Barriers"></a><strong>Read and Write Memory Barriers</strong></h2><p>在我们上面的例子中，memory barrier指令对store buffer和invalidate queue都进行了标注，不过，在实际的代码片段中，foo函数不需要mark invalidate queue，bar函数不需要mark store buffer<br>因此，许多CPU architecture提供了弱一点的memory barrier指令只mark其中之一。如果只mark invalidate queue，那么这种memory barrier被称为read memory barrier。相应的，write memory barrier只mark store buffer。一个全功能的memory barrier会同时mark store buffer和invalidate queue。<br>我们一起来看看读写内存屏障的执行效果：对于read memory barrier指令，它只是约束执行CPU上的load操作的顺序，具体的效果就是CPU一定是完成read memory barrier之前的load操作之后，才开始执行read memory barrier之后的load操作。read memory barrier指令象一道栅栏，严格区分了之前和之后的load操作。同样的，write memory barrier指令，它只是约束执行CPU上的store操作的顺序，具体的效果就是CPU一定是完成write memory barrier之前的store操作之后，才开始执行write memory barrier之后的store操作。全功能的memory barrier会同时约束load和store操作，当然只是对执行memory barrier的CPU有效。<br>现在，我们可以改一个用读写内存屏障的版本了，具体如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// On CPU 0 - b loaded</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  a=<span class="number">1</span>;</span><br><span class="line">  <span class="built_in">smp_wmb</span>();</span><br><span class="line">  b=<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// on CPU 1 - a loaded​</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">  <span class="built_in">smp_rmb</span>();</span><br><span class="line">  <span class="built_in">assert</span>(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Memory-Consistency-Memory-Model"><a href="#Memory-Consistency-Memory-Model" class="headerlink" title="Memory Consistency (Memory Model)"></a>Memory Consistency (Memory Model)</h1><h2 id="Reorder"><a href="#Reorder" class="headerlink" title="Reorder"></a>Reorder</h2><p><strong>同一线程中，彼此没有依赖关系的指令会被乱序执行。</strong><br>我们编码并发布运行需要经过编译器编译后然后在CPU上运行，通常我们认为我们所写的代码是按照顺序执行下去的，就是说上一个语句一定在下一个语句执行之前执行。这是我们的潜意识，然而事实可能并不是这样，因为中间经过了编译器也经过了CPU。编译器和CPU为了充分提高程序运行性能会在内部进行一系列优化，这些优化方法有很多，也很复杂。比较典型的有reorder，Speculative execution等。编译器会对我们写的代码顺序进行reorder，CPU执行的时候也会进行reorder，也就是说在执行时，我们写的代码并不是一定按照我们所看到顺序。但是不用担心，CPU或者编译器在reorder的时候并不会无厘头的reorder，他们至少要保证的是，<strong>在reorder之后，程序所表现出来的行为效果与单线程执行效果是一致的</strong>。这里提到的是单线程，也就是说CPU和编译器并不能感知道你的代码是多线程还是单线程，他只能保证单线程状况时正确的，多线程就不得而知了。<br>比如下面两条指令的执行顺序完全无法预测：</p>
<ol>
<li>x&#x3D;1;</li>
<li>y&#x3D;2;<br>但是下面两条指令将被串行执行（存在依赖关系）：<br>1. x &#x3D; some input value;<br>2. y&#x3D;x+z;</li>
</ol>
<h3 id="乱序执行的原因"><a href="#乱序执行的原因" class="headerlink" title="乱序执行的原因"></a>乱序执行的原因</h3><ol>
<li><p><strong>编译器</strong><br> 编译优化假设程序是单线程（Single-Threaded Optimizations）：如果编译器希望对程序指令的执行顺序做出改变，只要这些改变不影响该程序在单线程情况下的运行结果，那么这些改变就是允许的。<br> <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113641723.png" alt="image.png"></p>
</li>
<li><p><strong>处理器</strong><br> 现代处理器允许指令乱序执行，<strong>以避免因指令等待资源而导致处理器处于闲置状态。</strong><br> 处理器顺序执行：</p>
 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">y=x; <span class="comment">//cache miss，没有立即得到×的值</span></span><br><span class="line">z=y<span class="number">+1</span>; <span class="comment">//等待直到×被读到</span></span><br><span class="line">w=m+n; <span class="comment">//等待直到z得到结果</span></span><br></pre></td></tr></table></figure>
<p> 处理器乱序执行：</p>
 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">y=x; <span class="comment">//cache miss，没有立即得到×的值</span></span><br><span class="line">w=m+n; <span class="comment">//如果m和n已经准备好，这条指令可以先执行</span></span><br><span class="line">z=y<span class="number">+1</span>; <span class="comment">//等待直到被读到</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>存储系统</strong><br> 为了省去写入一致性cache (l2 cache)需要的10个clock cycle和写入memory需要的100个clock cycle，引入了store buffer（写入只需要1个clock cycle）。所以在CPU流水线里一个写指令结束的时候，其实他的数据并没有真正写到memory或者cache里，而是进了store buffer。</p>
</li>
<li><p><strong>On-chip Network</strong><br> CPU核之间的通信</p>
</li>
</ol>
<h2 id="Memory-consistency-motivation"><a href="#Memory-consistency-motivation" class="headerlink" title="Memory consistency motivation"></a>Memory consistency motivation</h2><p>对于C1而言，CPU可以执行S2-&gt;S1,也可以执行S1-&gt;S2, 对于这两种执行方式在C1看来是没有问题的，因为单线程而言这两种执行方式最后达到的效果是一样的。（因为S2和S1是对不同的memory location的操作，所以会reorder，如果是对同一个memory location操作是不允许出现这种reorder的）<br>如果C1的执行顺序是S2-&gt;L1-&gt;L2-&gt;S1，那么得到的结果r2 &#x3D; 0，而不是向我们预期的r2 &#x3D; NEW。对我们而言这种结果是超出预期的，是错的。<br>那么为了保证不会出现这种超出预期的行为，我们就需要一种规则来约束这种行为不能出现。这个任务就是memory consistency需要保证的。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113649801.png" alt="image.png"></p>
<h2 id="Memory-order"><a href="#Memory-order" class="headerlink" title="Memory order"></a>Memory order</h2><p>代码执行的顺序，这个是全局的，每个CPU core对共享内存的执行都会出现在memory order中如下图所示，每个core的代码都会对应到memory order这条执行线上。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113655885.png" alt="image.png"></p>
<h2 id="Sequential-Consistency-SC"><a href="#Sequential-Consistency-SC" class="headerlink" title="Sequential Consistency (SC)"></a>Sequential Consistency (SC)</h2><h3 id="并行程序中的顺序一致性"><a href="#并行程序中的顺序一致性" class="headerlink" title="并行程序中的顺序一致性"></a>并行程序中的顺序一致性</h3><p><strong>如果程序没有竞争（race）,那么程序运行起来就好像是顺序一致性</strong><br>这句话这么理解：<br><strong>竞争</strong>：两个线程访问同一个变量，而且其中有一个操作是写<br><strong>没有竞争</strong>：用某种方法把全局变量保护了起来，比如锁，原子指令等等<br><strong>顺序一致性</strong>：可以这么理解，在只有一个核的处理器上运行多线程程序，所以这些指令都是交织在一起，按照全局序执行的，这种方式运行的结果我们认为是正确的<br><strong>好像顺序一致性</strong>：运行结果跟某种顺序一致性程序运行的结果是一样的</p>
<p>The result of any execution is the same as if the operations of all processors (cores) were executed in some sequential order, and the operations of each individual processor (core) appear in this sequence in the order specified by its program</p>
<p>如上图中，S1 与 S2的program order可以表示为： S1 &lt;p S2； S1与L2的memory order可以表示为 S1 &lt;m L2。 用&lt;p 表示program order的先于顺序，&lt;m表示memory order的先于顺序。</p>
<p><strong>定义</strong></p>
<ol>
<li>All cores insert their loads and stores into the order &lt;m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a&#x3D;b or a&#x3D;̸b).<br> 所有对共享内存的操作都可以抽象成load(读取)和store(写入)，每一core执行load和store是按照其program order，那么就有S1 &lt;p S2肯定会推出 S1 &lt;m S2，SC的定义也由此引入了load和store的四种关系。在SC的定义中这四种关系是不允许被reorder的，即使是对不同memory location的操作。</li>
<li>Every load gets its value from the last store before it (in global memory order) to the same address</li>
</ol>
<p>只要符合上述两个条件，那么我们就可以说这个memory操作是符合顺序一致性的。</p>
<p>Example:<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113707830.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113714781.png" alt="image.png"></p>
<p>从图中可以看出在保证program order不变的情况下，memory order的顺序可以随意排列。</p>
<h3 id="竞争和-Object-Layout-有关"><a href="#竞争和-Object-Layout-有关" class="headerlink" title="竞争和 Object Layout 有关"></a>竞争和 Object Layout 有关</h3><p>第一种情况：全局变量s,类型是struct {char c;char d;}  &#x3D;&#x3D;没有竞争&#x3D;&#x3D;<br>第二种情况：全局变量s，类型是struct {int c:9;int d:7;} &#x3D;&#x3D;有竞争&#x3D;&#x3D;<br>在上述两种情况下，下面的程序是否有竞争？<br>线程1：s.c&#x3D;1;<br>线程2：s.d&#x3D;2;<br>在第二种情况下，声明了一个32 bit int大小的结构体，前9bit的是int c，后面7bit的是int d，然而bit并不是c++保证原子性的单位，因此写c或者d的时候，整个32 bit的int都会被牵连</p>
<h2 id="Total-Store-Order-TSO"><a href="#Total-Store-Order-TSO" class="headerlink" title="Total Store Order (TSO)"></a>Total Store Order (TSO)</h2><p>TSO的定义与SC的定义有两个变化：<br>    1.  <strong>不保证storeload顺序</strong><br>        举个例子：Core C1中S1和L1， S1先去L1执行，但是S1只是将值送入了write buffer就返回了，紧接着执行L1，L1在memory order中的点执行完之后，S1的write buffer这时候flush到内存，那么S1在memory order这条线上真正执行的点在L1之后了，那么这时候S1与L1就出现了reorder了。<br>    2. <strong>Every load gets its value from the last store before it to the same address:</strong><br>     需要注意的是，无论是TSO还是SC都需要至少保证一点，即使允许reorder也要保证program执行的结果与单线程执行的结果是一致的。比如一对操作：  S1: x &#x3D; new  L1: y &#x3D; x.  无论是TSO还是SC都需要保证y读到的是x&#x3D;new的值（排除其他线程在这两个语句之前对x进行store操作。）<br>     因为TSO引入了write buffer，那么上述x&#x3D;new会写入buffer，如何确保L1会读到最新的值呢，TSO引入了一种叫“bypass”的概念，就是对于<strong>同一memory location</strong>的读写会保障load会读到store的最新值无论这个store会不会进入write buffer。<br>     如下图所示：  L1读取的是S1的值，即使L1 &lt;m S1 且 S1 &lt;p L1.<br>     <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113725472.png" alt="image.png"></p>
<p>TSO在CPU与memory之间引入了write buffer。CPU写入的时候先写入write buffer然后就返回了，这样就将cpu与memory之间的差距隐藏了，但是这样同样带来了一个问题。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113731619.png" alt="image.png"></p>
<p>还是上面这个例子，S1将x&#x3D;NEW放到了core C1的write buffer中，S2将y&#x3D;NEW放到了C2的write buffer中，那么在执行L1,L2的时候，r1与r2这时候从memory读到是0。这个是违背了SC的，但是这样的设计确实带来了性能的提升。那么在TSO模型下的执行结果如下：前三种与SC一致，第四个执行结果则是TSO独有的，可以看出，TSO中允许执行线交叉。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113738004.png" alt="image.png"></p>
<p>如果我们想避免这种问题，那么需要在上层代码中添加FENCE，这个fence可以理解为memory barrier，他的作用是将write buffer中的记录flush到内存。<br>FENCE会强制保证program order。</p>
<ul>
<li>If S(a) &lt;p FENCE ⇒ S(a) &lt;m FENCE &#x2F;* Store → FENCE *&#x2F;</li>
<li>If FENCE &lt;p L(a) ⇒ FENCE &lt;m L(a) &#x2F;* FENCE → Load *&#x2F;<br>如果再S1与L1之间加上FENCE，就保证了S1 &lt;p L1 和 S1 &lt;m L1.</li>
<li><strong>Relaxed memory consistency</strong><br>  SC和TSO严格意义上来说都是一种强一致性模型，因为他们都对程序的执行顺序做了一定的约束，既然存在约束那么就会带来一定的性能损耗。<br>  那么有没有一种没这么多的约束的一致性模型，能够使机器进行深度的优化并发挥极致性能。那么执行顺序的正确性就只能有编码人员来保证了。<br>  relaxed memory consistency实现对于load与store顺序完全放开，除了对同一memory  location的操作保证load看到是最新的store以外其他都不进行约束，编码人员如果想强加order可以通过上述的FENCE。</li>
</ul>
<h1 id="Memory-Order"><a href="#Memory-Order" class="headerlink" title="Memory Order"></a>Memory Order</h1><p>C++11在标准库中引入了memory model，这应该是C++11最重要的特性之一了。C++11引入memory model的意义在于我们可以在high level language层面实现对在多处理器中多线程共享内存交互的控制。我们可以在语言层面忽略compiler，CPU arch的不同对多线程编程的影响了。我们的多线程可以跨平台了。<br>C++ atomic操作数中有一个选项可以指定对应的memory_order，这里的memory order可以理解为上面章节中的memory order。C++11中提供了六种不同memory_order选项，不同的选项会定义不同的memory consistency类型。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> std &#123;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="keyword">enum</span> <span class="title class_">memory_order</span> &#123;</span><br><span class="line">    memory_order_relaxed, memory_order_consume, memory_order_acquire,</span><br><span class="line">      memory_order_release, memory_order_acq_rel, memory_order_seq_cst</span><br><span class="line">&#125; memory_order;</span><br><span class="line"><span class="function">The enumeration memory_order specifies the detailed <span class="title">regular</span> <span class="params">(non-atomic)</span> memory synchronization order as defined in 1.10 <span class="keyword">and</span> may provide <span class="keyword">for</span> operation ordering. [10]      </span></span><br></pre></td></tr></table></figure>

<p>memory order指定了对应的对共享内存的operation order的关系。memory order也是一致性模型的一种反映。</p>
<h2 id="Happens-before"><a href="#Happens-before" class="headerlink" title="Happens-before"></a>Happens-before</h2><p><strong>Happens-before</strong> 是一个非常重要的概念. 如果操作 a “happens-before” 操作 b, 则操作 a 的结果对于操作 b 可见. happens-before 的关系可以建立在用一个线程的两个操作之间, 也可以建立在不同的线程的两个操作之间.</p>
<ul>
<li><p><strong>单线程的情况: sequenced-before</strong><br>  函数的语句按顺序依次执行, 前面的语句先执行, 后面的后执行. 正式地说, 前面的语句总是 <strong>“sequenced-before”</strong> 后面的语句.</p>
</li>
<li><p><strong>多线程的情况: synchronizes-with 和 inter-thread happens-before</strong><br>  一般来说多线程都是并发执行的, 如果没有正确的同步操作, 就无法保证两个操作之间有 happens-before 的关系. 如果我们通过一些手段, 让不同线程的两个操作同步, 我们称这两个操作之间有 <strong>synchronizes-with</strong> 的关系<br>  <strong>if thread A stores a value and thread B reads that value, there’s a synchronizes-with relationship between the store in thread A and the load in thread B.</strong><br>  <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113748857.png" alt="image.png"></p>
<p>  现在我们来看一个例子. 假设下面的代码中 <code>unlock()</code> 操作 “synchronizes-with” <code>lock()</code> 操作.</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	a += <span class="number">1</span> <span class="comment">// (1)  </span></span><br><span class="line">	<span class="built_in">unlock</span>(); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">	  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="built_in">lock</span>(); <span class="comment">// (3)  </span></span><br><span class="line">	cout &lt;&lt; a &lt;&lt; endl; <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>  假设直到 <code>thread1</code> 执行到 (2) 之前, <code>thread2</code> 都会阻塞在 (3) 处的 <code>lock()</code> 中. 那么可以推导出:</p>
<ul>
<li>根据语句顺序, 有 (1) “sequenced-before” (2) 且 (3) “sequenced-before” (4);</li>
<li>因为 (2) “synchronizes-with” (3) 且 (3) “sequenced-before” (4), 所以 (2) “inter-thread happens-before” (4);</li>
<li>因为 (1) “sequenced-before” (2) 且 (2) “inter-thread happens-before” (4), 所以 (1) “inter-thread happens-before” (4); 所以 (1) “happens-before” (4).</li>
</ul>
<p>  因此 (4) 可以读到 (1) 对变量 <code>a</code> 的修改.</p>
</li>
</ul>
<p>	</p>
<h2 id="memory-order-relaxed"><a href="#memory-order-relaxed" class="headerlink" title="memory_order_relaxed"></a><strong>memory_order_relaxed</strong></h2><p>宽松内存模型，这种内存模型对当前原子操作周围的内存访问顺序不做任何保证，也就是允许全部的内存乱序发生。包括 Load-Load、Load-Store、Store-Load 和 Store-Store 乱序。</p>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><h4 id="计数器（Counter）"><a href="#计数器（Counter）" class="headerlink" title="计数器（Counter）"></a>计数器（Counter）</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113759993.png" alt="image.png"></p>
<ul>
<li>count是个原子变量，初值为0。</li>
<li>这里可以用relaxed因为join_workers起到了acquire&#x2F;release的作用（可以假设join_workers内部包含一个 memory barrier）。而在join_workers之前，没有读操作，写操作的顺序也不重要(blindwrite)。</li>
</ul>
<h4 id="简单标志（Simple-Flag-Setting"><a href="#简单标志（Simple-Flag-Setting" class="headerlink" title="简单标志（Simple Flag Setting)"></a>简单标志（Simple Flag Setting)</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223113805089.png" alt="image.png"></p>
<p>dirty 和 stop 是原子布尔变量，初始值为 false。dirty可以使用 relaxed 的 原因和计数器相同，dirty其实就是一个最大值为1的计数器。</p>
<h4 id="引用计数（Reference-Counting）"><a href="#引用计数（Reference-Counting）" class="headerlink" title="引用计数（Reference Counting）"></a>引用计数（Reference Counting）</h4><p> <code>std::shared_ptr</code> 增加引用计数时用的就是 <code>memory_order_relaxed</code>, 因为不需要同步; 但是减小应用计数不能用它, 因为需要与析构操作同步.<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223114032860.png" alt="image.png"></p>
<p>线程1：从0加到1，没有共享，私有变量，不存在memoryorder的问题：在1以上增加，没有区别，refcnt&#x3D;2或者3对删除来说都是一样的，即：不删除。所以我不关心谁把refcnt从1变成2，谁把它从2变成3。也就是说：加1操作的顺序不重要，只要最后加上去就可以。所以加1操作可以是relaxed。<br><strong>对于B操作需要 release 语义</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115041399.png" alt="image.png"></p>
<p>如果B使用relaxed,那么A和B可以乱序，假设发生在线程2a中。2a的B把refcnt从2减到1，然后线程卡住。线程2b中A和B没有乱序，2b在2a睡眠期间把refcnt从1减到0，并且释放了control_block_ptr。然后线程2a醒过来执行A,就会访问已经释放的对象。为防止A和B乱序，B需要release语义。<br><strong>对于B操作需要 acquire_release 语义</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115131954.png" alt="image.png"><br>假设B使用release语义，线程2b中B后面的指令C和D就可以提前到B之前。因此线程2a写入的最新的×可能不能被2b看到。更有甚者，D被移到B之前是灾难性的。</p>
<h4 id="单例模式-Singleton"><a href="#单例模式-Singleton" class="headerlink" title="单例模式 (Singleton)"></a>单例模式 (Singleton)</h4><h5 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h5><p>这个实现是否正确？double-check locking<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115437774.png" alt="image.png"></p>
<p>如果第一个线程执行到B,然后卡住，C尚未执行；第二线程会在A得到一个非空的指针，并返回给用户，但是这个指针指向一个未初始化的对象。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115453757.png" alt="image.png"></p>
<h5 id="正确写法"><a href="#正确写法" class="headerlink" title="正确写法"></a>正确写法</h5><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115504723.png" alt="image.png"></p>
<h5 id="最佳写法"><a href="#最佳写法" class="headerlink" title="最佳写法"></a>最佳写法</h5><p>c++11后 static 默认是线程安全的<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115515680.png" alt="image.png"></p>
<p>注意: singleton 通常是不释放的</p>
<h4 id="初始化-atomic-array"><a href="#初始化-atomic-array" class="headerlink" title="初始化 atomic array"></a>初始化 atomic array</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223115524889.png" alt="image.png"></p>
<p><strong>四种方法</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134818787.png" alt="image.png"></p>
<ul>
<li>方法1: 普通的 for 循环, 但是不安全, read, write的时候会 out of order</li>
<li>方法2: 单次循环之后, 会有一个mfence, 很消耗时间</li>
<li>方法3: 和方法1一模一样, mfence没了 </li>
<li>方法4: mfence在循环体外面, 只执行一次, 性能开销小</li>
</ul>
<h4 id="“volatile”-关键字"><a href="#“volatile”-关键字" class="headerlink" title="“volatile” 关键字"></a>“volatile” 关键字</h4><ul>
<li>volatile (Java) !&#x3D; volatile(C, C+)</li>
<li>mutex, atomics和memory barriers用于控制程序对内存的访问(原子性和顺序)。</li>
<li>volatile用于控制对I&#x2F;O的访问，比如I&#x2F;O寄存器，这些寄存器虽然映射在内存中，但是完全不遵循常规的内存模型。编译器唯一能做的就是不做任何优化，老老实实的去生成汇编指令。volatile不保证原子性。</li>
</ul>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_relaxed)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>()); <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>thread1</code> 对不同的变量执行 store 操作. 那么在某些线程看来, 有可能是 <code>x</code> 先变为 <code>true</code>, y 后变为 <code>true</code>; 另一些线程看来, 又有可能是 <code>y</code> 先变为 <code>true</code>, <code>x</code> 后变为 <code>true</code>.<br>(4) 处的断言就有可能失败. 因为 (2) 与 (3) 之间没有 synchronizes-with 的关系, 所以就不能保证 (1) “happens-before” (4). 因此 (4) 就有可能读到 <code>false</code>.</p>
<h2 id="memory-order-seq-cst"><a href="#memory-order-seq-cst" class="headerlink" title="memory_order_seq_cst"></a>memory_order_seq_cst</h2><p>顺序一致性，也是默认的选项，这个选项不允许reorder。</p>
<h3 id="Acquire-和-Release-不保证全局序"><a href="#Acquire-和-Release-不保证全局序" class="headerlink" title="Acquire 和 Release 不保证全局序"></a>Acquire 和 Release 不保证全局序</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134832484.png" alt="image.png"></p>
<p><strong>On-Chip Network 不保证全局序</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134838406.png" alt="image.png"></p>
<p>片上网络不保证消息传播的序。比如：处理器3收到x&#x3D;1,但是处理器4还没有收到x&#x3D;1(消息还在路上)；同时，处理器4收到y&#x3D;1,但是处理器3还没有收到y&#x3D;1(消息还在路上)。这时处理器3和4都打印。</p>
<h3 id="Sequentially-Consistency-SC-保证写操作全局序"><a href="#Sequentially-Consistency-SC-保证写操作全局序" class="headerlink" title="Sequentially Consistency(SC)保证写操作全局序"></a>Sequentially Consistency(SC)保证写操作全局序</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134844716.png" alt="image.png"></p>
<p><strong>SC强迫On-Chip Network保证全局序</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134850394.png" alt="image.png"></p>
<p>片上网络保证消息传播的序，即：先传播×&#x3D;1到所有处理器，等到所有处理器都收到x&#x3D;1之后，再传播y&#x3D;1;或者反过来。总之，写操作串行地使用片上网络，从而在片上网络这一层产生了全局序。</p>
<h3 id="std-atomic语义总结"><a href="#std-atomic语义总结" class="headerlink" title="std:atomic&lt;..&gt;语义总结"></a>std:atomic&lt;..&gt;语义总结</h3><ol>
<li>原子性：读写都原子，你不会读到部分结果或者中间结果。</li>
<li>顺序性：所有针对原子变量的读写都会按序执行，实现顺序一致性(SC)。(假定使用std:atomic&lt;..&gt;的默认内存模型)</li>
</ol>
<h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_seq_cst); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_seq_cst); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br><span class="line">std::atomic&lt;<span class="type">int</span>&gt; z&#123;<span class="number">0</span>&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_x_then_y</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!x.<span class="built_in">load</span>(std::memory_order_seq_cst)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="keyword">if</span> (y.<span class="built_in">load</span>(std::memory_order_seq_cst)) ++z; <span class="comment">// (4)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_y_then_x</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_seq_cst)); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="keyword">if</span> (x.<span class="built_in">load</span>(std::memory_order_seq_cst)) ++z; <span class="comment">// (6)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="function">std::thread <span class="title">a</span><span class="params">(thread1)</span>, <span class="title">b</span><span class="params">(thread2)</span>, <span class="title">c</span><span class="params">(read_x_then_y)</span>, <span class="title">d</span><span class="params">(read_y_then_x)</span></span>;  </span><br><span class="line">	a.<span class="built_in">join</span>(), b.<span class="built_in">join</span>(), c.<span class="built_in">join</span>(), d.<span class="built_in">join</span>();  </span><br><span class="line">	<span class="built_in">assert</span>(z.<span class="built_in">load</span>() != <span class="number">0</span>); <span class="comment">// (7)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(7) 处的断言永远不会失败. 因为 <code>x</code> 和 <code>y</code> 的修改顺序是全局一致的, 如果先执行 (1) 后执行 (2), 则 <code>read_y_then_x</code> 中循环 (5) 退出时, 能保证 <code>y</code> 为 <code>true</code>, 此时 <code>x</code> 也必然为 <code>true</code>, 因此 (6) 会被执行; 同理, 如果先执行 (2) 后执行 (1), 则循环 (3) 退出时 <code>y</code> 也必然为 <code>true</code>, 因此 (4) 会被执行. 无论如何, <code>z</code> 最终都不会等于 0.</p>
<p>Sequencial consistent 可以实现 synchronizes-with 的关系. 如果一个 <code>memory_order_seq_cst</code> 的 load 操作在某个原子变量上读到了一个 <code>memory_order_seq_cst</code> 的 store 操作在这个原子变量中写入的值, 则 store 操作 “synchronizes-with” load 操作. 在上面的例子中, 有 (1) “synchronizes-with” (3) 和 (2) “synchronizes-with” (5).</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>实现 sequencial consistent 模型有一定的开销. 现代 CPU 通常有多核, 每个核心还有自己的缓存. 为了做到全局顺序一致, 每次写入操作都必须同步给其他核心. 为了减少性能开销, 如果不需要全局顺序一致, 我们应该考虑使用更加宽松的顺序模型.</p>
<h2 id="Acquire-Release"><a href="#Acquire-Release" class="headerlink" title="Acquire-Release"></a>Acquire-Release</h2><p><strong>Acquire means “after is after”：之后的所有指令不会早于这条指令开始执行，尤其是读指令不会。即后面访存指令勿重排至此条指令之前</strong><br><strong>Release means “before is before”：之前的所有指令都已经执行完，尤其是写指令的结果已经全局可见。即前面访存指令勿重排至此条指令之后，当此条指令的结果对其他线程可见后，之前的所有指令都可见</strong><br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134903299.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134917311.png" alt="image.png"></p>
<p>在 acquire-release 模型中, 会使用 <code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code> 这三种内存顺序. 它们的用法具体是这样的:</p>
<ul>
<li>对原子变量的 load 可以使用 <code>memory_order_acquire</code> 内存顺序. 这称为 <strong>acquire 操作</strong>.</li>
<li>对原子变量的 store 可以使用 <code>memory_order_release</code> 内存顺序. 这称为 <strong>release 操作</strong>.</li>
<li>read-modify-write 操作即读 (load) 又写 (store), 它可以使用 <code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code>:<ul>
<li>如果使用 <code>memory_order_acquire</code>, 则作为 acquire 操作;</li>
<li>如果使用 <code>memory_order_release</code>, 则作为 release 操作;</li>
<li>如果使用 <code>memory_order_acq_rel</code>, 则同时为两者.<br>Acquire-release 可以实现 synchronizes-with 的关系. 如果一个 acquire 操作在同一个原子变量上读取到了一个 release 操作写入的值, 则这个 release 操作 “synchronizes-with” 这个 acquire 操作. 我们来看一个例子:<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">bool</span>&gt; x&#123;<span class="literal">false</span>&#125;, y&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>(std::memory_order_relaxed)); <span class="comment">// (4)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
在上面的例子中, 语句 (2) 使用 <code>memory_order_release</code> 在 <code>y</code> 中写入 <code>true</code>, 语句 (3) 中使用 <code>memory_order_acquire</code> 从 <code>y</code> 中读取值. 循环 (3) 退出时, 它已经读取到了 <code>y</code> 的值为 <code>true</code>, 也就是读取到了操作 (2) 中写入的值. 因此有 (2) “synchronizes-with” (3). 我们可以推导出:</li>
</ul>
</li>
<li>因为 (2) “synchronizes-with” (3) 且 (3) “sequenced-before” (4), 所以 (2) “inter-thread happens-before” (4);</li>
<li>因为 (1) “sequenced-before” (2) 且 (2) “inter-thread happens-before” (4), 所以 (1) “inter-thread happens-before” (4);<br>所以 (1) “happens-before” (4). 因此 (4) 能读取到 (1) 中写入的值, 断言永远不会失败. 即使 (1) 和 (4) 用的是 <code>memory_order_relaxed</code>.</li>
</ul>
<p>事实上, 内存顺序为 <code>memory_order_seq_cst</code> 的 load 操作和 store 操作可以分别视为 acquire 操作和 release 操作. 因此对于两个指定了 <code>memory_order_seq_cst</code> 的 store 操作和 load 操作, 如果后者读到了前者写入的值, 则前者 “synchronizes-with” 后者.</p>
<p>为了实现 synchronizes-with 关系, acquire 操作和 release 操作应该成对出现. 如果 <code>memory_order_acquire</code> 的 load 读到了 <code>memory_order_relaxed</code> 的 store 写入的值, 或者 <code>memory_order_relaxed</code> 的 load 读到了 <code>memory_order_release</code> 的 store 写入的值, 都不能实现 synchronizes-with 的关系.</p>
<p>虽然 sequencial consistent 模型能够像 acquire-release 一样实现同步, 但是反过来 acquire-release 模型不能像 sequencial consistent 一样提供全局顺序一致性. 如果将例子中的 <code>memory_order_seq_cst</code> 换成 <code>memory_order_acquire</code> 和 <code>memory_order_release</code></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="literal">true</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_x_then_y</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!x.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (3)  </span></span><br><span class="line">	<span class="keyword">if</span> (y.<span class="built_in">load</span>(std::memory_order_acquire)) ++z; <span class="comment">// (4)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">read_y_then_x</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="keyword">if</span> (x.<span class="built_in">load</span>(std::memory_order_acquire)) ++z; <span class="comment">// (6)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>则最终不能保证 <code>z</code> 不为 0. 在同一次运行中, <code>read_x_then_y</code> 有可能看到先 (1) 后 (2), 而 <code>read_y_then_x</code> 有可能看到先 (2) 后 (1). 这样有可能 (4) 和 (6) 的 load 的结果都为 <code>false</code>, 导致最后 <code>z</code> 仍然为 0.</p>
<p>Acquire-release 的开销比 sequencial consistent 小. 在 x86 架构下, <code>memory_order_acquire</code> 和 <code>memory_order_release</code> 的操作不会产生任何其他的指令, 只会影响编译器的优化: 任何指令都不能重排到 acquire 操作的前面, 且不能重排到 release 操作的后面; 否则会违反 acquire-release 的语义. 因此很多需要实现 synchronizes-with 关系的场景都会使用 acquire-release.</p>
<h3 id="Acquire-和-Release-的实现（编译器）"><a href="#Acquire-和-Release-的实现（编译器）" class="headerlink" title="Acquire 和 Release 的实现（编译器）"></a>Acquire 和 Release 的实现（编译器）</h3><p>1. Acquire 和 release 操作的内部实现需要利用 memory barrier 指令。<br>2. Acquire和 release操作内部由汇编语言编写，因此可以排除编译优化的影响，同时通过汇编语言也可以方便地嵌入memory barrier指令。<br>3. 当编译器看到 memory barrier 指令时，不会把 acquire后面的指令挪到acquire前面，也不会把 release前面的指令挪到 release后面。<br>4. 编译器不能把一个函数调用后面的指令挪到该函数调用的前面，也不能把一个函数调用前面的指令挪到该函数调用的后面，因为编译器不知道该函数调用内部是否使用了memory barrier指令（如果编译器能够判断该函数内部不涉及 memory barrier指令，另当别论）。</p>
<h3 id="Acquire-和-Release-的实现（处理器）"><a href="#Acquire-和-Release-的实现（处理器）" class="headerlink" title="Acquire 和 Release 的实现（处理器）"></a>Acquire 和 Release 的实现（处理器）</h3><p>PowerPC的 lwsync指令是 memory barrier指令，其工作原理是堵在处理器流水线的入口，不让后续指令进入流水线，直到前面已经进入流水线的指令执行完，并且storebuffer清空。逻辑上看：lwsync保证它前面的指令不会被挪到它后面，它后面的指令不会被挪到它前面，因此是一个双向的 memory barrier。<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134932447.png" alt="image.png"></p>
<h4 id="单独的-memory-barrier-指令代价大"><a href="#单独的-memory-barrier-指令代价大" class="headerlink" title="单独的 memory barrier 指令代价大"></a>单独的 memory barrier 指令代价大</h4><p>Release操作的要求：</p>
<ol>
<li>lwsync前面的指令（比如“writex”）不能挪到lwsync之后；</li>
<li>“Ready&#x3D;1”这一条写指令不能挪到lwsync之前。lwsync可以满足1，但同时要求lwsync之后所有指令都不能挪到lwsync之前，超过了实际需要，有损编译优化。 <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134941526.png" alt="image.png"></li>
</ol>
<h4 id="合并的“Acquire”和“Release”"><a href="#合并的“Acquire”和“Release”" class="headerlink" title="合并的“Acquire”和“Release”"></a>合并的“Acquire”和“Release”</h4><p>Intel IA64处理器把 memory barrier指令和读写指令合并，提供了带 acquire语义的读指令ld.acq（称为acquire load）和带 release语义的写指令 st.rel（称为release store）。因此，线程1的”read&#x2F;writey’可以挪到“st.rel ready1”之前，线程2的“read&#x2F;writez”可以挪到“ld.acqr0,ready”之后，增加了编译优化的可能性。<img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223134950542.png" alt="image.png"></p>
<h4 id="Acquire和-release-自动化"><a href="#Acquire和-release-自动化" class="headerlink" title="Acquire和 release 自动化"></a>Acquire和 release 自动化</h4><ol>
<li>不要自己手工地使用 memory barrier指令。</li>
<li>使用锁或者原子变量来自动实现acquire和 release操作。</li>
</ol>
<ul>
<li>Lock acquire&#x2F;release<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">mutex<span class="number">1.l</span>ock(); <span class="comment">//ld.acq mutex1</span></span><br><span class="line">read/write x;</span><br><span class="line">mutex<span class="number">1.</span><span class="built_in">unlock</span>(); <span class="comment">//st.rel mutex1</span></span><br></pre></td></tr></table></figure></li>
<li>std:atomic&lt;bool&gt; flag &#x3D; 0;<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (!flag.<span class="built_in">compre_exchange_strong</span>(..); <span class="comment">//ld.acq flag</span></span><br><span class="line">read/write x;</span><br><span class="line">flag = <span class="number">0</span>; <span class="comment">//st.rel flag</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Release-sequences"><a href="#Release-sequences" class="headerlink" title="Release sequences"></a>Release sequences</h3><p>到目前为止我们看到的, 无论是 sequencial consistent 还是 acquire-release, 要想实现 synchronizes-with 的关系, acquire 操作必须在同一个原子变量上读到 release 操作的写入的值. 如果 acquire 操作没有读到 release 操作写入的值, 那么它俩之间通常没有 synchronizes-with 的关系. 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;<span class="type">int</span>&gt; x&#123;<span class="number">0</span>&#125;, y&#123;<span class="number">0</span>&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	x.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_relaxed); <span class="comment">// (1)  </span></span><br><span class="line">	y.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	y.<span class="built_in">store</span>(<span class="number">2</span>, std::memory_order_release); <span class="comment">// (3)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread3</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (!y.<span class="built_in">load</span>(std::memory_order_acquire)); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(x.<span class="built_in">load</span>(std::memory_order_relaxed) == <span class="number">1</span>); <span class="comment">// (5)  </span></span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>上面的例子中, 只要 <code>y</code> 的值非 0 循环 (4) 就会退出. 当它退出时, 有可能读到 (2) 写入的值, 也有可能读到 (3) 写入的值. 如果是后者, 则只能保证 (3) “synchronizes-with” (4), 不能保证与 (2) 与 (4) 之间有同步关系. 因此 (5) 处的断言就有可能失败.</p>
<p>但并不是只有在 acquire 操作读取到 release 操作写入的值时才能构成 synchronizes-with 关系. 为了说这种情况, 我们需要引入 <strong>release sequence</strong> 这个概念.</p>
<p>针对一个原子变量 M 的 release 操作 A 完成后, 接下来 M 上可能还会有一连串的其他操作. 如果这一连串操作是由</p>
<ul>
<li>同一线程上的写操作, 或者</li>
<li>任意线程上的 read-modify-write 操作<br>这两种构成的, 则称这一连串的操作为<strong>以 release 操作 A 为首的 release sequence</strong>. 这里的写操作和 read-modify-write 操作可以使用任意内存顺序.</li>
</ul>
<p>如果一个 acquire 操作在同一个原子变量上读到了一个 release 操作写入的值, 或者读到了以这个 release 操作为首的 release sequence 写入的值, 那么这个 release 操作 “synchronizes-with” 这个 acquire 操作. 我们来看个例子</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::vector&lt;<span class="type">int</span>&gt; data;  </span><br><span class="line">std::atomic&lt;<span class="type">int</span>&gt; flag&#123;<span class="number">0</span>&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	data.<span class="built_in">push_back</span>(<span class="number">42</span>); <span class="comment">// (1)  </span></span><br><span class="line">	flag.<span class="built_in">store</span>(<span class="number">1</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line"><span class="type">int</span> expected = <span class="number">1</span>;  </span><br><span class="line">	<span class="keyword">while</span> (!flag.<span class="built_in">compare_exchange_strong</span>(expected, <span class="number">2</span>, std::memory_order_relaxed)) <span class="comment">// (3)  </span></span><br><span class="line">	expected = <span class="number">1</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread3</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (flag.<span class="built_in">load</span>(std::memory_order_acquire) &lt; <span class="number">2</span>); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(data.<span class="built_in">at</span>(<span class="number">0</span>) == <span class="number">42</span>); <span class="comment">// (5)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的例子中, (3) 处的 <code>compare_exchange_strong</code> 是一种 read-modify-write 操作, 它判断原子变量的值是否与期望的值 (第一个参数) 相等, 如果相等则将原子变量设置成目标值 (第二个参数) 并返回 <code>true</code>, 否则将第一个参数 (引用传递) 设置成原子变量当前值并返回 <code>false</code>. 操作 (3) 会一直循环检查, 当 <code>flag</code> 当值为 1 时, 将其替换成 2. 所以 (3) 属于 (2) 的 release sequence. 而循环 (4) 退出时, 它已经读到了 (3) 写入的值, 也就是 release 操作 (2) 为首的 release sequence 写入的值. 所以有 (2) “synchronizes-with” (4). 因此 (1) “happens-before” (5), (5) 处的断言不会失败.</p>
<p>注意 (3) 处的 <code>compare_exchange_strong</code> 的内存顺序是 <code>memory_order_relaxed</code>, 所以 (2) 与 (3) 并不构成 synchronizes-with 的关系. 也就是说, 当循环 (3) 退出时, 并不能保证 <code>thread2</code> 能读到 <code>data.at(0)</code> 为 42. 但是 (3) 属于 (2) 的 release sequence, 当 (4) 以 <code>memory_order_acquire</code> 的内存顺序读到 (2) 的 release sequence 写入的值时, 可以与 (2) 构成 synchronizes-with 的关系.</p>
<h2 id="memory-order-consume"><a href="#memory-order-consume" class="headerlink" title="memory_order_consume"></a>memory_order_consume</h2><p><strong>后面依赖此原子变量的访存指令勿重排至此条指令之前</strong><br><code>memory_order_consume</code> 其实是 acquire-release 模型的一部分, 但是它比较特殊, 它涉及到数据间相互依赖的关系. 为此我们又要提出两个新概念: <strong>carries dependency</strong> 和 <strong>dependency-ordered before</strong>.<br>如果操作 a “sequenced-before” b, 且 b 依赖 a 的数据, 则 a “carries a dependency into” b. 一般来说, 如果 a 的值用作 b 的一个操作数, 或者 b 读取到了 a 写入的值, 都可以称为 b 依赖于 a. 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">p++; <span class="comment">// (1)  </span></span><br><span class="line">i++; <span class="comment">// (2)  </span></span><br><span class="line">p[i]; <span class="comment">// (3)</span></span><br></pre></td></tr></table></figure>
<p>有 (1) “sequenced-before” (2) “sequenced-before” (3); (1) 和 (2) 的值作为 (3) 的下标运算符 <code>[]</code> 的操作数, 所以有 (1) “carries a dependency into” (3) 和 (2) “carries a dependency into” (3). 但是 (1) 和 (2) 并没有相互依赖, 它们之间没有 carries dependency 的关系. 类似于 sequenced-before, carries dependency 关系具有传递性.</p>
<p><code>memory_order_consume</code> 可以用于 load 操作. 使用 <code>memory_order_consume</code> 的 load 称为 consume 操作. 如果一个 consume 操作在同一个原子变量上读到了一个 release 操作写入的值, 或以其为首的 release sequence 写入的值, 则这个 release 操作 “dependency-ordered before” 这个 consume 操作.</p>
<p>概念很复杂, 但是基本思路是:</p>
<ul>
<li>release 操作和 acquire 操作构成的 synchronizes-with 可以后接 sequenced-before 构成 inter-thread happens-before 的关系;</li>
<li>release 操作和 consume 操作构成的 dependency-ordered before 则只能后接 carries dependency 构成 inter-thread happens-before 的关系.</li>
<li>无论 inter-thread happens-before 是怎么构成的, 都可以前接 sequenced-before 以延伸其范围.</li>
</ul>
<p>我们来看一个例子:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::atomic&lt;std::string*&gt; ptr;  </span><br><span class="line"><span class="type">int</span> data;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	std::string* p = <span class="keyword">new</span> std::<span class="built_in">string</span>(<span class="string">&quot;Hello&quot;</span>); <span class="comment">// (1)  </span></span><br><span class="line">	data = <span class="number">42</span>; <span class="comment">// (2)  </span></span><br><span class="line">	ptr.<span class="built_in">store</span>(p, std::memory_order_release); <span class="comment">// (3)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	std::string* p2;  </span><br><span class="line">	<span class="keyword">while</span> (!(p2 = ptr.<span class="built_in">load</span>(std::memory_order_consume))); <span class="comment">// (4)  </span></span><br><span class="line">	<span class="built_in">assert</span>(*p2 == <span class="string">&quot;Hello&quot;</span>); <span class="comment">// (5)  </span></span><br><span class="line">	<span class="built_in">assert</span>(data == <span class="number">42</span>); <span class="comment">// (6)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(4) 处的循环退出时, consume 操作 (4) 读取到 release 操作 (3) 写入的值, 因此 (3) “dependency-ordered before” (4). 由此可以推导出:</p>
<ul>
<li><code>p2</code> 的值作为 (5) 的操作数, 因此 (4) “carries a dependency into” (5);</li>
<li>因为 (3) “dependency-ordered before” (4) 且 (4) “carries a dependency into” (5), 所以 (3) “inter-thread happens-before” (5);</li>
<li>因为 (1) “sequenced-before” (3) 且 (3) “inter-thread happens-before” (5), 所以 (1) “inter-thread happens-before” (5);</li>
</ul>
<p>所以 (1) “happens-before” (5). 因此 (5) 可以读到 (1) 写入的值, 断言 (5) 不会失败. 但是操作 (6) 并不依赖于 (4), 所以 (3) 和 (6) 之间没有 inter-thread happens-before 的关系, 因此断言 (6) 就有可能失败. 回想 2.2 节强调过的, happens-before 没有传递性. 所以不能说因为 (3) “happens-before” (4) 且 (4) “happens-before” (6) 所以 (2) “happens-before” (6).</p>
<p>与 acquire-release 类似, 在 x86 下使用 <code>memory_order_consume</code> 的操作不会产生任何其他的指令, 只会影响编译器优化. 与 consume 操作有依赖关系的指令都不会重排到 consume 操作前面. 它对重排的限制比 acquire 宽松些, acquire 要求所有的指令都不能重排到它的前面, 而 consume 只要求有依赖关系的指令不能重排到它的前面. 因此在某些情况下, consume 的性能可能会高一些.</p>
<h2 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h2><ul>
<li><p><strong>自旋锁</strong><br>  在一些场景下, 如果锁被占用的时间很短, 我们会选择自旋锁, 以减少上下文切换的开销. 锁一般用来保护临界数据的读写, 我们希望同一时间只有一个线程能获取到锁, 且获取到锁后, 被锁保护的数据总是最新的. 前者通过原子操作即可保证, 而后者就需要考虑内存顺序了.</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">std::deque&lt;<span class="type">int</span>&gt; queue;  </span><br><span class="line">spinlock mu;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="type">int</span> val;  </span><br><span class="line">	<span class="keyword">while</span> ((val = <span class="built_in">read_from_remote</span>())) &#123;  </span><br><span class="line">		mu.<span class="built_in">lock</span>(); <span class="comment">// (1)  </span></span><br><span class="line">		queue.<span class="built_in">push_back</span>(val); <span class="comment">// (2)  </span></span><br><span class="line">		mu.<span class="built_in">unlock</span>(); <span class="comment">// (3)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="keyword">while</span> (<span class="literal">true</span>) &#123;  </span><br><span class="line">		mu.<span class="built_in">lock</span>(); <span class="comment">// (4)  </span></span><br><span class="line">		cout &lt;&lt; queue.<span class="built_in">front</span>() &lt;&lt; endl;  </span><br><span class="line">		queue.<span class="built_in">pop_front</span>(); <span class="comment">// (5)  </span></span><br><span class="line">		mu.<span class="built_in">unlock</span>(); <span class="comment">// (6)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  两个线程并发运行, <code>thread1</code> 往队列里写入数据, <code>thread2</code> 从队列里读出数据. 入队操作 (2) 可能需要复制数据, 移动指针, 甚至 resize 队列, 因此我们要保证获取到锁时, 这些操作的结果完全可见. 出队操作也是同理. 所以自旋锁要保证 unlock 操作 “synchronizes-with” lock 操作, 保证锁保护的数据是完整的.</p>
<p>  我们可以用 acquire-release 模型实现自旋锁. 下面是一个简单的实现:</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">spinlock</span> &#123;  </span><br><span class="line">	std::atomic&lt;<span class="type">bool</span>&gt; flag&#123;<span class="literal">false</span>&#125;;  </span><br><span class="line">	<span class="keyword">public</span>:  </span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">		<span class="keyword">while</span> (flag.<span class="built_in">exchange</span>(<span class="literal">true</span>, std::memory_order_acquire)); <span class="comment">// (1)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">		flag.<span class="built_in">store</span>(<span class="literal">false</span>, std::memory_order_release); <span class="comment">// (2)  </span></span><br><span class="line">	&#125;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>  上面的实现中, (1) 处加锁用到的 <code>exchange</code> 是一种 read-modify-write 操作, 它将目标值 (第一个参数) 写入原子变量, 并返回写入前的值. 在这个实现中, 锁被占用时 <code>flag</code> 为 <code>true</code>. 如果锁被占用, (1) 处的 exchange 操作会一直返回 <code>true</code>, 线程阻塞在循环中; 直到锁被释放, <code>flag</code> 为 <code>false</code>, exchange 操作将 <code>flag</code> 重新置为 <code>true</code> 以抢占锁, 并且返回其原来的值 <code>false</code>, 循环退出, 加锁成功. 解锁则很简单, 将 <code>flag</code> 置为 <code>false</code> 即可.</p>
<p>  由于解锁操作使用 <code>memory_order_release</code> 且加锁操作使用 <code>memory_order_acquire</code>, 所以能保证加锁成功时与上一次解锁操作构成 “synchronizes-with” 的关系, 也就是 unlock 操作 “synchronizes-with” lock 操作.</p>
<p>  加锁时的 exchange 操作是一个 read-modify-write 操作, 它既读又写. 当它使用 <code>memory_order_acquire</code> 时, 只能保证它读的部分是一个 acquire 操作. 如果有两个线程抢占同一个锁</p>
  <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">spinlock mu;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread1</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	<span class="comment">// some operations  </span></span><br><span class="line">	mu.<span class="built_in">lock</span>(); <span class="comment">// (1)  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">thread2</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">	mu.<span class="built_in">lock</span>(); <span class="comment">// (2)  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  (1) 和 (2) 之间没有任何同步关系, 假设先执行操作 (1) 后执行操作 (2), 那么 <code>thread1</code> 中 (1) 之前的操作结果不一定对 <code>thread2</code> 可见. 但能确定的是, 只会有一个线程得到锁, 这是由原子变量的修改顺序 (modification order) 所保证的. 要么 <code>thread1</code> 先将 <code>flag</code> 置为 <code>true</code>, 要么 <code>thread2</code> 先将 <code>flag</code> 置为 <code>true</code>, 这个顺序是全局一致的.</p>
</li>
</ul>
<h2 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h2><h3 id="Atomics-Code-Gen-for-x86-x64"><a href="#Atomics-Code-Gen-for-x86-x64" class="headerlink" title="Atomics Code Gen for x86&#x2F;x64"></a>Atomics Code Gen for x86&#x2F;x64</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135006264.png" alt="image.png"></p>
<p>在 x86&#x2F;x64 处理器上：</p>
<ul>
<li>Reads are not reordered with other reads. <strong>load自带acquire语义</strong></li>
<li>Writes are not reordered with other writes (with some minor exceptions). <strong>write自带release语义</strong></li>
<li>Writes are not reordered with older reads. </li>
<li>Reads may be reordered with older writes to different locations. <strong>可以理解为x86处理器没有out of order. 但是由于芯片里有store buffer, 后面的load可能会超越之前的store (因为存在store buffer里), 看上去好像out of order了. 因此使用强一致性的store时, 会加上一个mfence, 防止后面的load读不到store的值.</strong></li>
</ul>
<h3 id="测试程序"><a href="#测试程序" class="headerlink" title="测试程序"></a>测试程序</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135016920.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135022786.png" alt="image.png"></p>
<h3 id="性能综述"><a href="#性能综述" class="headerlink" title="性能综述"></a>性能综述</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135027809.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135033175.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135039124.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20241223135046595.png" alt="image.png"></p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><ul>
<li><code>memory_order_relaxed</code>: 最宽松的内存顺序, 只保证操作的<strong>原子性</strong>和<strong>修改顺序 (modification order)</strong>.</li>
<li><code>memory_order_acquire</code>, <code>memory_order_release</code> 和 <code>memory_order_acq_rel</code>: 实现 <strong>acquire 操作</strong>和 <strong>release 操作</strong>, 如果 acquire 操作读到了 release 操作写入的值, 或其 release sequence 写入的值, 则构成 <strong>synchronizes-with 关系</strong>, 进而可以推导出 <strong>happens-before 的关系</strong>.</li>
<li><code>memory_order_consume</code>: 实现 <strong>consume 操作</strong>, 能实现数据依赖相关的同步关系. 如果 consume 操作读到了 release 操作写入的值, 或其 release sequence 写入的值, 则构成 <strong>dependency-ordered before 的关系</strong>, 对于有数据依赖的操作可以进而推导出 <strong>happens-before 的关系</strong>.</li>
<li><code>memory_order_seq_cst</code>: 加强版的 acquire-release 模型, 除了可以实现 <strong>synchronizes-with 关系</strong>, 还保证<strong>全局顺序一致</strong>.</li>
</ul>
<h1 id="Memory-Order-v-s-Memory-Model-v-s-Cache-Coherence"><a href="#Memory-Order-v-s-Memory-Model-v-s-Cache-Coherence" class="headerlink" title="Memory Order v.s. Memory Model v.s. Cache Coherence"></a>Memory Order v.s. Memory Model v.s. Cache Coherence</h1><ul>
<li><strong>Memory order</strong><br>  因为每个 CPU 硬件平台提供的内存一致性模型不一样（比如 X86 是 TSO 模型，而 arm 则是弱内存模型），因此默认情况下，每个 CPU 执行指令期间允许的内存乱序情况是不一样的。编程语言在语言层面上都提供了一些接口，能够忽略compiler，CPU arch的不同对多线程编程的影响。</li>
<li><strong>Memory Model</strong><br>  处理器执行期间的指令重排，注重的是全局的memory order，是保证多处理器编程中的正确性，我们在讨论这个的时候可以把cache当做一个黑盒子来处理，也就是说即使没有cache，我们也同样需要memory consistency来保证正确性。</li>
<li><strong>Cache Coherence</strong><br>  处理器执行期间因为缓存不一致引起的数据不可见，关注于一个memory location。</li>
</ul>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT (Vision Transformer)</title>
    <url>/2025/01/10/ViT-Vision-Transformer/</url>
    <content><![CDATA[<h2 id="VIT-是什么"><a href="#VIT-是什么" class="headerlink" title="VIT 是什么"></a>VIT 是什么</h2><p>在计算机视觉领域中，多数算法都是保持CNN整体结构不变，在CNN中增加attention模块或者使用attention模块替换CNN中的某些部分。有研究者提出，没有必要总是依赖于CNN。因此，作者提出<strong>ViT (Vision Transformer)</strong> 算法，仅仅使用Transformer结构也能够在图像分类任务中表现很好。</p>
<p>受到NLP领域中Transformer成功应用的启发，ViT算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。具体来讲，ViT算法中，会将整幅图像拆分成小图像块，然后把这些小图像块的线性嵌入序列作为Transformer的输入送入网络，然后使用监督学习的方式进行图像分类的训练。</p>
<span id="more"></span>
<p>该算法在中等规模（例如ImageNet）以及大规模（例如ImageNet-21K、JFT-300M）数据集上进行了实验验证，发现：</p>
<ul>
<li><p>Transformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果。具体表现为使用中等规模的ImageNet训练的Transformer会比ResNet在精度上低几个百分点。</p>
</li>
<li><p>当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平。</p>
</li>
</ul>
<h2 id="模型结构与实现"><a href="#模型结构与实现" class="headerlink" title="模型结构与实现"></a>模型结构与实现</h2><p>我们先结合下面的动图来粗略地分析一下ViT的工作流程，如下：<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/18f056b695693c40b440e3ba9c368e07%20(1).gif"></p>
<ul>
<li><p>将一张图片分成patches</p>
</li>
<li><p>将patches铺平</p>
</li>
<li><p>将铺平后的patches的线性映射到更低维的空间</p>
</li>
<li><p>添加位置embedding编码信息</p>
</li>
<li><p>将图像序列数据送入标准Transformer encoder中去</p>
</li>
<li><p>在较大的数据集上预训练</p>
</li>
<li><p>在下游数据集上微调用于图像分类</p>
</li>
</ul>
<h3 id="图像分块嵌入"><a href="#图像分块嵌入" class="headerlink" title="图像分块嵌入"></a>图像分块嵌入</h3><p>  <img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181841575.png" alt="image.png"></p>
<p>考虑到在Transformer结构中，输入是一个二维的矩阵，矩阵的形状可以表示为$(N,D)$，其中$N$是sequence的长度，而$D$是sequence中每个向量的维度。因此，在ViT算法中，首先需要设法将$H×W×C (224 \times 224 \times 3)$的三维图像转化为$(N,D)$的二维输入。</p>
<p>ViT中的具体实现方式为：将$H×W×C$的图像，变为一个$N×(P^2∗C) &#x3D; 196 \times (16^2 * 3)$的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了$N&#x3D;HW&#x2F;P^2 &#x3D; 196$个图像块，每个图像块的维度则是$P^2∗C &#x3D; 16^2 * 3$。其中$P$是图像块的大小，$C$是通道数量。经过如上变换，就可以将$N$视为sequence的长度了。</p>
<p>但是，此时每个图像块的维度是$(P^2∗C)$，而我们实际需要的向量维度是$D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个$(P^2∗C)$的图像块做一个线性变换，将维度压缩为$D&#x3D;768$即可。</p>
<p>参考了BERT，在头部插入了维度为768的一个表征分类结果的 embedding，因此最后的输入维度为 197 * 768</p>
<h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>将图像转化为 N×(P2∗C) 的序列后，就可以将其输入到 Transformer 结构中进行特征提取了，如图所示。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181859449.png" alt="image.png"></p>
<p>Transformer 结构中最重要的结构就是 Multi-head Attention，即多头注意力结构。具有2个head的 Multi-head Attention 结构如图所示。输入$a^i$经过转移矩阵，并切分生成 $q^{(i,1)}、q^{(i,2)}、k^{(i,1)}、k^{(i,2)}、v^{(i,1)}、v^{(i,2)}$，然后 $q^{(i,1)}$与 $k^{(i,1)}$做 attention，得到权重向量 $\alpha$，将 $\alpha$与 $v^{(i,1)}$进行加权求和，得到最终的$b^{(i,1)}$，同理可以得到$b^{(i,2)}$。接着将它们拼接起来，通过一个线性层进行处理，得到最终的结果。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181908858.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110181919268.png" alt="image.png"></p>
<h3 id="MLP-头"><a href="#MLP-头" class="headerlink" title="MLP 头"></a>MLP 头</h3><p>上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的，以ViT-B&#x2F;16为例，输入的是[197, 768]输出的还是[197, 768]。这里我们只是需要分类的信息，所以我们只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。</p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ul>
<li>为什么小数据时CNN效果要好于VIT，大数据时VIT效果更好一些？</li>
</ul>
<p>传统的CNN卷积神经网络有很强的inductive biases（归纳偏置），是根据图像的本质而设计的一种网络，比如卷积核的平移不变性和局部性。因此在小数据时，VIT学不到这些，自然效果就不好，但是数据多起来后，就不太需要针对图像特意去设计这些小tips了。</p>
<ul>
<li>混合架构</li>
</ul>
<p>作为原始图像图块的替代，可以从CNN的特征图形成输入序列。在该混合模型中，将patch embedding投影应用于从CNN特征图提取的图块。（即，将图像块先经过CNN网络，然后将CNN的网络输出的特征矩阵输进Transformer中，这样也是可以的，论文有尝试过）</p>
<ul>
<li>位置信息</li>
</ul>
<p>Transformer和CNN不同，需要position embedding来编码tokens的位置信息，这主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。但是CNN是滑动的，是本身就具有位置信息的。所以在使用VIT时要手动加上位置信息。论文中试验过使用1-D和2-D的位置信息效果差不多。</p>
<ul>
<li>微调和更高的分辨率</li>
</ul>
<p>通常，我们在大型数据集上对ViT进行预训练，并微调到（较小的）下游任务。为此，我们删除了预训练的预测head，并附加了一个零初始化的D*K的前馈层，其中K是下游类的数量。以比预训练更高的分辨率进行微调通常是有益的。当提供更高分辨率的图像时，我们将图块大小保持不变，这会导致更大的有效序列长度。ViT可以处理任意序列长度（直到内存限制），但是，预训练的位置embedding可能不再有意义。因此，我们根据预先训练的位置嵌入在原始图像中的位置执行2D插值。请注意，只有在分辨率调整和色块提取中，将有关图像2D结构的感应偏差手动注入到Vision Transformer中。</p>
<p>参考链接：</p>
<ul>
<li><p>﻿<a href="https://blog.csdn.net/lsb2002/article/details/135320751">Visual Transformer (ViT)模型详解-CSDN博客</a>﻿</p>
</li>
<li><p>﻿<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/classification/ViT.html">https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision&#x2F;classification&#x2F;ViT.html</a>﻿</p>
</li>
<li><p>﻿<a href="https://blog.csdn.net/gailj/article/details/123664828">经典论文阅读笔记——VIT、Swin Transformer、MAE、CILP_clip与vit的关系-CSDN博客</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>cv</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>02 CUDA Shared Memory</title>
    <url>/2025/01/16/02-CUDA-Shared-Memory/</url>
    <content><![CDATA[<h2 id="1-D-Stencil-问题"><a href="#1-D-Stencil-问题" class="headerlink" title="1-D Stencil 问题"></a><strong>1-D Stencil 问题</strong></h2><p>stencil可以理解为一个计算的窗口，如下图为半径为3的1-D stencil，宽度为7（宽度一般为奇数），一次进行对七个元素的计算</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162607310.png" alt="image.png"></p>
<span id="more"></span>

<p>已知stencil 半径为3，宽度为7，步长为1（即每个元素会有7次参与到运算），下图为stencil计算过程</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162615963.png" alt="image.png"></p>
<p>在向量加法问题中，单一输入点仅贡献于单一输出点，而单一输出点也仅依赖于单一输入点。</p>
<p>而在 stencil 问题中，单一输入点贡献于七个输出点，单一输出点也依赖于七个输入点。</p>
<p>stencil 问题依赖于 cuda 提供的共享内存（shared memory）</p>
<h2 id="共享内存（shared-memory）"><a href="#共享内存（shared-memory）" class="headerlink" title="共享内存（shared memory）"></a>共享内存（shared memory）</h2><p>thread 之间的数据共享基于 shared memory</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162631214.png" alt="image.png"></p>
<ul>
<li><p>每一个线程都有私有的寄存器，64&#x2F;128KB的寄存器堆会划分到每一个线程上</p>
</li>
<li><p>每一个<strong>线程块</strong>都有一块<strong>共享内存</strong>，共享内存实际上是在GPU芯片上实现的，因此我们可以称之为片上内存（on-chip），也被称为用户管理的快速缓存（与之对比的是真的的cache由硬件管理）</p>
</li>
<li><p>每一个<strong>网格</strong>共用一块大的<strong>全局共享内存</strong>（同时有一个768KB的共享L2），这是在 host 上通过<code>cudaMalloc</code>分配的内存，这并非位于GPU芯片本身，而是由多个高速内存设备组成，与GPU相连，比线程块之间的共享内存慢五倍。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162637944.png" alt="image.png"></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>变量声明</td>
<td>存储器</td>
<td>作用域</td>
<td>生存周期</td>
</tr>
<tr>
<td><code>int var</code></td>
<td>寄存器</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><code>int array_var[100]</code></td>
<td>寄存器&#x2F;本地</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><code>__shared__ int shared_var</code></td>
<td>共享</td>
<td>线程块</td>
<td>线程块</td>
</tr>
<tr>
<td><code>__device__ int global_var</code></td>
<td>全局</td>
<td>全局</td>
<td>应用程序</td>
</tr>
<tr>
<td><code>__constant__ int constant_var</code></td>
<td>常量</td>
<td>全局</td>
<td>应用程序</td>
</tr>
</tbody></table>
<h2 id="回到1-D-Stencil-问题"><a href="#回到1-D-Stencil-问题" class="headerlink" title="回到1-D Stencil 问题"></a>回到<strong>1-D Stencil 问题</strong></h2><p>思路：</p>
<ol>
<li><p>把（blockDim(每个block包含的threads数量) + 2 * 半径（也被称为光晕））数量的数据从全局内存中加载入共享内存中</p>
</li>
<li><p>计算出 blockDim 维度的输出</p>
</li>
<li><p>把 blockDim 维度的计算结果写回全局内存</p>
</li>
</ol>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162646435.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162652825.png" alt="image.png"></p>
<ul>
<li><p>﻿<code>gindex</code>:全局内存中的数据索引</p>
</li>
<li><p>﻿<code>lindex</code>:共享内存中的数据索引</p>
</li>
<li><p>在radius为3的情况下，会有三个线程负责将左右的光晕load到共享内存中</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162659403.png" alt="image.png"></p>
<p>以上代码的问题：线程并没有做同步</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162709653.png" alt="image.png"></p>
<p><strong>解决方法：引入</strong><code>**__syncthreads()**</code><strong>同步</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162717375.png" alt="image.png"></p>
<ul>
<li><strong>所有线程都必须能够参与并执行到同步线程（sync thread)语句。</strong></li>
</ul>
<p>修改后的正确版本</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162725282.png" alt="image.png"></p>
<p>02-CUDA-Shared-Memory.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>01 CUDA C Basics</title>
    <url>/2025/01/16/01-CUDA-C-Basics/</url>
    <content><![CDATA[<h2 id="课程链接"><a href="#课程链接" class="headerlink" title="课程链接"></a>课程链接</h2><p>﻿<a href="https://www.bilibili.com/video/BV1JJ4m1P7xW">【Nvidia官方课程】CUDA入门课【中英字幕】_哔哩哔哩_bilibili</a>﻿<br>﻿<a href="https://www.olcf.ornl.gov/cuda-training-series/">https://www.olcf.ornl.gov/cuda-training-series/</a></p>
<h2 id="CUDA-Intro"><a href="#CUDA-Intro" class="headerlink" title="CUDA Intro"></a>CUDA Intro</h2><p><strong>异构计算</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162334293.png" alt="image.png"></p>
<p>GPU加速</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162343989.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162352994.png" alt="image.png"></p>
<span id="more"></span>

<h2 id="第一个cuda程序：1-D-向量加法"><a href="#第一个cuda程序：1-D-向量加法" class="headerlink" title="第一个cuda程序：1-D 向量加法"></a><strong>第一个cuda程序：1-D 向量加法</strong></h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162400442.png" alt="image.png"></p>
<p>关键词<code>__global__</code>告诉<code>nvcc</code>编译器这个函数编译成能在gpu上运行的代码，其他函数用<code>gcc</code>等编译</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162407419.png" alt="image.png"></p>
<p>gpu函数调用方法</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162415839.png" alt="image.png"></p>
<p>内存管理</p>
<p>使用<code>cudaMalloc</code>创建的指针仅能在gpu中解引用，使用<code>malloc</code>创建的指针仅能在cpu中解引用</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162422783.png" alt="image.png"></p>
<h3 id="层次结构"><a href="#层次结构" class="headerlink" title="层次结构"></a>层次结构</h3><p>GPU 层次结构：Thread, Block 以及 Grid：</p>
<ul>
<li><p>Thread 是最基本的执行单位，<strong>每一个 Thread 都会把你写的 CUDA Kernel 从头到尾完整地执行一遍</strong>。</p>
</li>
<li><p>每一个 Block 中包含若干个 Thread，每一个 Thread 都会有一个<code>threadIdx</code>，代表这个 Thread 在它所在的 Block 中的 id。可以使用<code>blockDim</code>来获取 Block 中有多少个 Thread。</p>
</li>
<li><p>每一个 Grid 包含若干个 Block，每一个 Thread 也有一个<code>blockIdx</code>，代表这个 Thread 所在的 Block 在 Grid 中的 id。可以使用<code>gridDim</code>来获取 Grid 中有多少个 Block。每一次启动 CUDA Kernel 时都会生成一个 Grid（某种意义上可以理解为一个 “执行上下文”。</p>
</li>
</ul>
<p>每一个cuda线程都有自己的<strong>控制流、PC、寄存器、堆栈</strong>，能够访问GPU任意全局内存地址</p>
<p>threadIdx.{x,y,z}<br>blockIdx.{x,y}</p>
<p>Kernel上的两层线程组织结构如下(2-dim)</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162432922.png" alt="image.png"></p>
<p>并行运行 <code>block</code>﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162445545.png" alt="image.png"></p>
<p>&lt;&lt;&lt;&gt;&gt;&gt; 表示 &lt;&lt;&lt;每个 Grid 中有多少 Block, 每个 Block 中有多少 Thread&gt;&gt;&gt;</p>
<p>向量加法函数<code>add</code>编写方法</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162455671.png" alt="image.png"></p>
<p>内存分配过程</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162503572.png" alt="image.png"></p>
<blockquote>
<p>声明<code>int *a</code>但未初始化时</p>
<ul>
<li><p>﻿<code>a</code> 的值是未定义的，因为它指向一个随机的内存地址。</p>
</li>
<li><p>﻿<code>*a</code> 的值是未定义的，访问它可能导致程序崩溃。</p>
</li>
<li><p>﻿<code>&amp;a</code> 的值是指针变量 <code>a</code> 的内存地址，它是定义明确的。</p>
</li>
</ul>
</blockquote>
<p>把数据复制到gpu -&gt; 在gpu上并行计算 -&gt; 把数据复制回cpu -&gt; 释放内存</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162516907.png" alt="image.png"></p>
<p>尽管cuda宣称符合c++14的标准，但它并未将标准库纳入其中，这意味着 std:: 开头的函数不被支持</p>
<p>﻿<code>thread</code>﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162525426.png" alt="image.png"></p>
<p>全局 index</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162533645.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116162542386.png" alt="image.png"></p>
<p>一个线程需要两个内置的坐标变量<code>(blockIdx,threadIdx)</code>来唯一标识，都是<code>dim3</code>变量。</p>
<p>关于<code>dim3</code>的结构类型</p>
<ul>
<li><p>﻿<code>dim3</code>是基于<code>uint3</code>定义的矢量类型，相当于由3个<code>unsigned int</code>型组成的结构体。<code>uint3</code>类型有三个数据成员<code>unsigned int x; unsigned int y; unsigned int z;</code>﻿</p>
</li>
<li><p>可使用于一维、二维或三维的索引来标识线程，构成一维、二维或三维线程块(block)。</p>
</li>
<li><p>相关的几个内置变量</p>
<ul>
<li><p>﻿<code>threadIdx</code>，顾名思义获取线程<code>thread</code>的ID索引；如果线程是一维的那么就取<code>threadIdx.x</code>，二维的还可以多取到一个值<code>threadIdx.y</code>，以此类推到三维<code>threadIdx.z</code>。</p>
</li>
<li><p>﻿<code>blockIdx</code>，线程块的ID索引；同样有<code>blockIdx.x</code>，<code>blockIdx.y</code>，<code>blockIdx.z</code>。</p>
</li>
<li><p>﻿<code>blockDim</code>，线程块的维度，同样有<code>blockDim.x</code>，<code>blockDim.y</code>，<code>blockDim.z</code>。</p>
</li>
<li><p>﻿<code>gridDim</code>，线程格的维度，同样有<code>gridDim.x</code>，<code>gridDim.y</code>，<code>gridDim.z</code>。</p>
</li>
</ul>
</li>
<li><p>对于一维的<code>block</code>，线程的<code>threadID = threadIdx.x</code>﻿</p>
</li>
<li><p>对于大小为<code>(blockDim.x, blockDim.y)</code>的二维<code>block</code>﻿</p>
<ul>
<li><p>﻿<code>int i = blockIdx.x * blockDim.x + threadIdx.x;</code>﻿</p>
</li>
<li><p>﻿<code>int j = blockIdx.y * blockDim.y + threadIdx.y;</code></p>
</li>
</ul>
<p>﻿
      </p>
</li>
<li><p>对于大小为<code>(blockDim.x, blockDim.y, blockDim.z)</code>的三维<code>block</code>，线程的<code>threadID = threadIdx.x+threadIdx.y*blockDim.x+threadIdx.z*blockDim.x*blockDim.y</code>﻿</p>
</li>
<li><p>对于计算线程索引偏移增量为已启动线程的总数，如<code>stride = blockDim.x * gridDim.x; threadId += stride</code></p>
</li>
</ul>
<p>﻿</p>
<p>01-CUDA-C-Basics.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>03 Fundamental CUDA Optimization (Part 1)</title>
    <url>/2025/01/16/03-Fundamental-CUDA-Optimization-Part-1/</url>
    <content><![CDATA[<p><strong>这一章节将探讨GPU架构和第一个CUDA优化重点，即实现充分的并行性</strong></p>
<h2 id="Nvidia-GPU-架构演进"><a href="#Nvidia-GPU-架构演进" class="headerlink" title="Nvidia GPU 架构演进"></a>Nvidia GPU 架构演进</h2><p><strong>对 GPU 架构的一定了解，将有助于你从根本上编写出在 GPU 上性能优异的代码</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163109194.png" alt="image.png"></p>
<span id="more"></span>


<h3 id="KEPLER-架构"><a href="#KEPLER-架构" class="headerlink" title="KEPLER 架构"></a>KEPLER 架构</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163126366.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163115967.png" alt="image.png"></p>
<p><strong>CC</strong> (Compute Capability，计算能力)：计算能力，数字越大能力越强</p>
<p><strong>SM</strong> (Streaming Multiprocessor，流式多处理器) ：GPU 架构的 buliding block，但我们构建一个大的 GPU 时，我们通过在芯片上集成大量的 SM 来达成这一目标</p>
<p><strong>SP Unit&#x2F;Core</strong> (Streaming processor Unit) ：执行 32 位浮点乘法、浮点加法以及浮点融合乘加运算的算数单元</p>
<p><strong>DP Unit</strong>（Double Precision Unit）：执行 64 位浮点运算的算数单元</p>
<p><strong>SFU Unit</strong>（Special Function Unit，特殊运算单元）负责特殊的 ALU 运算，如 SIN 和 COS</p>
<p><strong>LD&#x2F;ST Unit</strong>（Load-Store Unit，存储负载单元）：在 GPU 上执行操作，例如 32 位浮点乘法时，首先执行的指令是将操作数据加载到寄存器文件中，随后执行一条指令在 Register File 内部完成乘法运算，当需要保存结果时则通过加载存储单元进行存储。加载存储单元作为独立的逻辑单元，负责管理大部分进出内存的数据流。</p>
<p><strong>Warp Scheduler</strong>（Warp 调度器）：SM 中的指令分发单元，warp 调度器将决定何时以及哪些指令被执行。</p>
<h4 id="Wrap"><a href="#Wrap" class="headerlink" title="Wrap"></a>Wrap</h4><p><strong>一个 warp 是 32 个线程的集合。GPU 并非向单个线程发出指令，指令总是以 warp 为单位进行发布。因此，当发出一条指令时，根据定义一个线程束中的 32 个线程將执行相同的指令。</strong></p>
<p>若一个线程块被分配至SM，该线程块将逻辑上分解为一系列的 warp。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163144224.png" alt="image.png"></p>
<h3 id="Maxwell-Pascal-架构"><a href="#Maxwell-Pascal-架构" class="headerlink" title="Maxwell &amp; Pascal 架构"></a>Maxwell &amp; Pascal 架构</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163151335.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163158377.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163206360.png" alt="image.png"></p>
<p><strong>CC6.1 SM 引入了名为 INT8 的新计算能力</strong>：在一个时钟周期内能执行四个 8 位宽的整数运算</p>
<h3 id="Pascal-Volta-架构"><a href="#Pascal-Volta-架构" class="headerlink" title="Pascal &amp; Volta 架构"></a>Pascal &amp; Volta 架构</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163214641.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163220770.png" alt="image.png"></p>
<p><strong>支持 FP16 的运算</strong>：每个 SP 单元（核心）能执行一个 32 位浮点运算或两个 16 位浮点运算</p>
<p>在此之前，我们讨论的是拥有大约 20 到 24 个 SM 的 GPU，而现在我们谈论的是 50 到 80 个 SM 的 GPU</p>
<p><strong>CC7.0 SM 引入了名为 TensorCore 的新计算能力</strong></p>
<h4 id="TensorCore"><a href="#TensorCore" class="headerlink" title="TensorCore"></a>TensorCore</h4><p>Tensor Core 的思路从系统设计上还是相当直接的，目前深度学习的 workload 中最主要的计算量都在矩阵的乘加上，因此为了专门去高效地支持这些 workload，就增加一些专用于矩阵运算的专用部件进去。</p>
<p><strong>Tensor Core 为 GPU 的原始计算能力带来了 5 至 10 倍的提升,特别是在 16 位浮点数的矩阵乘法运算模式下</strong></p>
<p>这也是常见的AI ASIC（如Google的TPU和其他厂商的各种xPU等）通常采用的策略，只不过 ASIC 可以从一开始就是针对特定的 workload 去的，因此设计上可以更直接更激进一些，直接上大量的 MMU（Matrix Multiply Unit），然后采用例如脉冲阵列这种设计去最大化它的 throughput。</p>
<p>而 NV 的 GPU 毕竟还要用作其他一些通用的运算，所以只能往原本的 SM 流水线里面插进去一些额外的专用部件 lane 了。开个脑洞，如果有一天发现除了FMA之外，还有其他形式的运算存在大量需求，未来的 GPU 设计里面说不定也会出现其他 xx Core。好在 FMA 除了深度学习以外在 HPC 的 workload 里面也是挺常见的，这个设计以后还是比较有用的。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163229634.png" alt="image.png"></p>
<p>﻿<a href="https://jcf94.com/download/2020-05-24-nvidia-arch-tensorcore1.png">Tensor Core 4x4 Matrix Multiply and Accumulate</a>﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163237236.png" alt="image.png"></p>
<p>﻿<a href="https://jcf94.com/download/2020-05-24-nvidia-arch-tensorcore2.png">Mixed Precision Multiply and Accumulate in Tensor Core</a>﻿</p>
<p>Tensor Core 这个部件直接从 SM 的寄存器里面取两个 FP16 的矩阵作为输入，进行全精度的矩阵乘之后得到的结果可以是 FP16 或者 FP32 的，然后累加到 FP16&#x2F;FP32 的 accumulator 里面去。选择 FP16 作为输入数据类型，并输出为 FP32，这一做法可能旨在确保结果不会因溢出而受损，然后在加速部件设计等等方面做了一些 trade off。</p>
<h3 id="软件和硬件的映射关系"><a href="#软件和硬件的映射关系" class="headerlink" title="软件和硬件的映射关系"></a>软件和硬件的映射关系</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163244836.png" alt="image.png"></p>
<h2 id="Launch-Configuration"><a href="#Launch-Configuration" class="headerlink" title="Launch Configuration"></a>Launch Configuration</h2><p><strong>首先聚焦于两大性能目标中的第一个，即实现充分的并行性（尽可能多地使用线程）。但我们要搞明白为什么要尽可能多地使用线程。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163253901.png" alt="image.png"></p>
<ul>
<li><p>如前所述，指令是按 warp 宽度发布的，并且也按顺序执行，因此GPU并非乱序执行的机器。若线程执行到某条指令时，发现下一条待执行的指令所需的 operand 尚未就绪，该线程便会 stall。operand 是指令的输入数据，比如进行乘法运算，所乘的两个数即为 operand。</p>
</li>
<li><p>延迟是试图在GPU中避免的问题，若仅使用单一线程，会因为等待数据出现而遇到停滞。在不同GPU架构中，全局内存延迟是所面临的最长延迟之一，通常为100个时钟周期以上，而算数延迟通常在10个时钟周期左右。</p>
</li>
<li><p>我们的具标在于启动足够多的线程以掩盖延迟</p>
</li>
</ul>
<h3 id="一个简单的程序"><a href="#一个简单的程序" class="headerlink" title="一个简单的程序"></a>一个简单的程序</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163301686.png" alt="image.png"></p>
<ul>
<li><p>Wrap 调度器在一个 Warp 因数据未准备好而 stall 时向下一个 Warp 发送指令，以尽可能保证每个 Clock Cycle 都是忙碌的。</p>
</li>
<li><p><strong>Warp 数量越多越能更好地隐藏延迟。</strong></p>
</li>
<li><p>假设一个 SM 有64个 warp，而一个 wrap 由32个线程组成，因此我们希望每个 SM 有2048个线程</p>
</li>
<li><p><strong>要最大化任何GPU的性能，关键在于使其饱和，即提供足够多的线程以使机器完全占用，从而为延迟隐藏创造最佳条件。</strong></p>
</li>
<li><p>具有双发射能力的 wrap scheduler：如果指令流中的两条相邻指令彼此之间没有依赖关系，那么wrap调度器有可能同时发出这两条指令。</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163311797.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163318519.png" alt="image.png"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p>﻿<a href="https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/">https://jcf94.com/2020/05/24/2020-05-24-nvidia-arch/</a>﻿</p>
</li>
<li><p>﻿<a href="https://cnblogs.com/upyun/p/17824106.html">https://cnblogs.com/upyun/p/17824106.html</a></p>
</li>
</ul>
<p>﻿</p>
<p>03-CUDA-Fundamental-Optimization-Part-1.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>04 Fundamental CUDA Optimization (Part 2)</title>
    <url>/2025/01/16/04-Fundamental-CUDA-Optimization-Part-2/</url>
    <content><![CDATA[<p><strong>这一章节将探讨第二个CUDA优化重点，即高效利用内存子系统（memory subsystems）</strong></p>
<h2 id="内存层级结构"><a href="#内存层级结构" class="headerlink" title="内存层级结构"></a>内存层级结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163332513.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163340975.png" alt="image.png"></p>
<span id="more"></span>

<p>多图方便理解</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163348818.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163354805.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163400595.png" alt="image.png"></p>
<h2 id="全局内存优化"><a href="#全局内存优化" class="headerlink" title="全局内存优化"></a><strong>全局内存优化</strong></h2><h3 id="LD-ST-操作"><a href="#LD-ST-操作" class="headerlink" title="LD&#x2F;ST 操作"></a>LD&#x2F;ST 操作</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163406857.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163411994.png" alt="image.png"></p>
<p>使用 Non-caching Load 操作的场景举例：当希望在一个线程写入数据，并使其对另一线程可见，而该线程可能运行在另一SM中时，这两个SM之间的L1缓存并不保证一致性。</p>
<h3 id="Load-Operation"><a href="#Load-Operation" class="headerlink" title="Load Operation"></a>Load Operation</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163422653.png" alt="image.png"></p>
<ul>
<li><p>内存操作是按warp为单位发出的。</p>
</li>
<li><p>warp中的每个线程可能正在为该加载操作请求不同的内存地址</p>
</li>
<li><p>内存控制器的作用是，一旦指令发出，它將获取那些多达32个地址的模式，并确定需要哪些行或段。</p>
</li>
<li><p>从DRAM请求的数据并非单个字节、浮点数或其他类似形式，而是一个内存事务 (memory transaction)，所请求或存储的单位为32字节的段</p>
</li>
</ul>
<p><strong>四种 Caching Load 的情况</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163430150.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163437442.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163446716.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163453826.png" alt="image.png"></p>
<p><strong>Non-Caching Load 的情况：由于cache line 大小从128字节变为32字节，增加了利用率</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163502578.png" alt="image.png"></p>
<h3 id="全局内存优化准则"><a href="#全局内存优化准则" class="headerlink" title="全局内存优化准则"></a><strong>全局内存优化准则</strong></h3><p>基本思路是，我们希望一个 warp 尽可能在连续区域内进行访问</p>
<p>对于memory bound的代码，尽可能提高并发数来使总线利用率饱和</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163515541.png" alt="image.png"></p>
<h2 id="共享内存优化"><a href="#共享内存优化" class="headerlink" title="共享内存优化"></a>共享内存优化</h2><p>共享内存是每个SM的资源，其功能一是提升性能，功能二是为线程块内的线程间通信提供了一种方式</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163553301.png" alt="image.png"></p>
<h3 id="共享内存的结构"><a href="#共享内存的结构" class="headerlink" title="共享内存的结构"></a>共享内存的结构</h3><p>可以将共享内存视为一个二维的内存数组，列是bank（存储体），每个bank的宽度是4 bytes，数组的行数是共享内存的大小（比如48KB）&#x2F;每行的大小（128B</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163600875.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163609859.png" alt="image.png"></p>
<p>最优访问（行式访问）：从 Bank 0-31各取4 byte</p>
<p>最差访问（列式访问）：从全部从 Bank 0 访问，完全串行化</p>
<h3 id="如何避免-Bank-冲突"><a href="#如何避免-Bank-冲突" class="headerlink" title="如何避免 Bank 冲突"></a>如何避免 Bank 冲突</h3><p><strong>在不改变物理结构的情况下，我们可以改变逻辑数组的结构，如果我们将32列的二维数组拓展为33列，则可以利用到共享内存的最佳性能</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163621621.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163627802.png" alt="image.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163633978.png" alt="image.png"></p>
<ul>
<li><p>全局内存具有一种倾向于合并的访问模式，为了最大限度地提高全局内存的吞吐量，需使同一个 wrap 内线程的访问地址尽可能连续，相邻，成组。</p>
</li>
<li><p>当我们使用共享内存时，需要留意“bank冲突“</p>
</li>
<li><p>共享内存相较于全局内存在进行分散访问时，具有更多的灵活性</p>
</li>
</ul>
<p>04-CUDA-Fundamental-Optimization-Part-2.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>BLIP</title>
    <url>/2025/01/16/BLIP/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131594387">https://blog.csdn.net/lansebingxuan/article/details/13159...</a>﻿</p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/2201.12086.pdf">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>﻿</p>
<p>论文代码：<a href="https://github.com/salesforce/BLIP">https://github.com/salesforce/BLIP</a>﻿</p>
<p>BLIP是ALBEF原班人马做的, 借鉴了很多ALBEF的思想, <strong>BLIP设计了一个多模态混合编解码器（MED），能够实现 3 种功能：单模态编码、基于图像的文本编码、基于图像的文本解码</strong></p>
<h3 id="研究动机及本文贡献"><a href="#研究动机及本文贡献" class="headerlink" title="研究动机及本文贡献"></a>研究动机及本文贡献</h3><ol>
<li><p><strong>从模型角度</strong>：最近的一些方法通常有2种模型实现方式，1）Transformer Encoder结果的模型，比如Clip、ALBEF，2）Transformer Encoder、Decoder结构的模型，比如SimVLM。第一种Encoder Only的模型无法直接运用到Text Generation的任务，比如图像生成字幕，因为它只有编码器没有解码器，需要加一些模块做Text Generation的任务；第二种Encoder、Decoder虽然有Decoder可以做生成的任务，但因为没有一个统一的框架，所以它不能直接用来做Image Text Retrieval的任务，因此需要提出一个Unified的统一的框架，用一个模型把所有的任务都解决。BLIP这篇论文就是（利用了很多VLMO里的想法）<strong>把模型设计成一个灵活的框架，从而构造一个Unified Framework</strong>。</p>
</li>
<li><p><strong>从数据角度</strong>：目前表现出色的方法比如Clip、ALBEF和SimVLM都是在大规模从网上爬下来的非常Noisy的Image Text Pair数据集上预训练模型，虽然当数据集足够大能够弥补一些嘈杂数据集带来的影响，但BLIP论文指出，使用Noisy的数据集去预训练效果不佳，不是最优解，BLIP论文提出<strong>Captioner和Filter 模块</strong>，Captioner的作用是给定任意一张图片，用Captioner生成一些字幕，从而得到大量的合成数据Synthetic Data，同时去训练一个Filtering Model，把图像和文本不匹配的对从数据集里删掉。作者训练的Captioner模型，可以生成非常好的有描述性的文本，因此训练的filtering模型会选择这个图像文本对去进行模型的训练，而不用原来那个真实的图像文本对去进行训练，这样能够有效地清洗Noisy的DataSet，让模型更好地利用数据集里的图像文本配对信息。</p>
<span id="more"></span></li>
</ol>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>由于BLIP论文借鉴了ALBEF和VLMO的思想，因此先简单介绍这两篇论文。﻿</p>
<h4 id="ALBEF模型"><a href="#ALBEF模型" class="headerlink" title="ALBEF模型"></a>ALBEF模型</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161154646.png" alt="image.png"></p>
<p>ALBEF的模型分成三个结构：1）视觉编码器，2）文本编码器，3）多模态编码器。训练步骤如下：</p>
<ol>
<li><p>输入一张图像进入Transformer Encoder，它一共有N层；输入一个文本进入这个文本的编码器，它有L层。得到对应的图像文本特征之后，先做一个ITC（Image Text Contrasting）对比学习的Loss，把图像和文本特征分别学好。</p>
</li>
<li><p>文本特征继续进入Self-Attention Layer去训练，图像特征通过一个Cross-Attention Layer进来，然后和文本特征去进行融合，经历了N-L层的多模态的编码器之后，得到多模态的特征。</p>
</li>
<li><p>用多模态的特征做Image Text Matching任务，从而训练更好的模型。</p>
</li>
</ol>
<p>文本端要把一个N层的Transformer Encoder分成L层和N-L层，因为作者想维持计算量不变，跟Clip相同，左边一个12层的Transformer Encoder，右边也是一个12层的Transformer Encoder，不想增加更多的多模态融合的计算量，但是多模态这部分又特别的重要，相对而言文本这端不那么重要，所以就把这边12层的计算量给分成了两部分。</p>
<h4 id="VLMO模型"><a href="#VLMO模型" class="headerlink" title="VLMO模型"></a>VLMO模型</h4><p>&#96;<br><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161211803.png" alt="image.png"></p>
<p>&#96;<br>针对维持计算量的问题，VLMO没有拆分两部分，而是设计一个Mixer of Expert MoE网络，让它变得极其的灵活。它只有一个网络，Self-Attention层全都是共享参数，唯一根据模态不同而改变的地方是Feed Forward Network，用Feed Forward Vision、Feed Forward Attacks和Feed Forward Multi Model去区别不同的Modality，训练不同的Expert。这样就用统一的一个模型，即训练的时候是一个模型，推理的时候可以根据不同的任务选择这个模型中的某一部分去做推理。VLMO用大量的实验证明Self-Attention层确实是可以共享参数的，它跟模态没什么关系。</p>
<h3 id="本文贡献1–Unified-Framework"><a href="#本文贡献1–Unified-Framework" class="headerlink" title="本文贡献1–Unified Framework"></a>本文贡献1–Unified Framework</h3><h4 id="BLIP模型"><a href="#BLIP模型" class="headerlink" title="BLIP模型"></a>BLIP模型</h4><p>BLIP的模型结构称为MED（Mixture of Encoder and Decoder），就是把编码器和解码器混到一起，模型包含四个部分，图像有一个N层的标准的VIT模型，Self-Attention和Feed Forward均采用标准格式，文本有三个模型，分别算三个不同的目标函数，跟VLMO非常像。它根据输入模态的不同、目标函数的不同，它选择一个大模型里不同的部分去做模型的forward。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161220256.png" alt="image.png"></p>
<ol>
<li><p>第一个文本模型是Text Encoder，N层做分类的任务，当得到了文本特征之后，它就去跟这个视觉特征去做ITC loss。</p>
</li>
<li><p>第二个文本模型是Image Grounded Text Encoder，是一个多模态的编码器，借助图像的信息去完成一些多模态的任务，得到ITM loss。到这里左边这一部分就是一个ALBEF，但是它跟ALBEF有一点不同，就是它借鉴了VLMO，Self Attention层可以共享参数，就不需要把一个文本模型拆分成两个部分使用，这里<strong>同样的颜色代表共享参数</strong>，图中的SA层是共享参数的。所以第一个文本编码器和第二个文本编码器基本一样，SA和FF全都是一致的，只不过第二个多了一个Cross Attention层。</p>
</li>
<li><p>第三个文本模型是Image-grounded Text decoder。目前的结构还是只能做这种VQA VRVE这种Understanding的任务，所以在后面再加一个文本的Decoder，就可以做生成的任务，对于Decoder来说它的输入输出的形式和尤其是第一层的这个Self-Attention不太一样，因为它不能看到完整的句子，必须像训练GPT模型一样把后面的句子都mask掉，只通过前面的信息去推测后面的句子，所以它的第一层用的是Causal的Self-Attention，也就是因果关系的自注意力去做一些因果推理。这里它做的是Causal Self-Attention跟前面的Bidirectional Self-Attention就不一样了。除了第一层的这个自注意力之外，后面的这个Cross-Attention和Feed-forward就跟前面全都是共享参数的。所以它新添加了第三个Text Decoder，但事实上参数量并没有增加多少，只是增加了一些Causal的Self-Attention。最后的目标函数就是用的GPT系列的Language Modeling，也就是给定一些词，去预测剩下的那些词，这篇论文里作者要做生成式的任务，所以更好的选择是使用Language Modeling的目标函数。</p>
</li>
</ol>
<h4 id="BLIP目标函数"><a href="#BLIP目标函数" class="headerlink" title="BLIP目标函数"></a>BLIP目标函数</h4><p>BLIP三个目标函数中，前两个跟ALBEF和VLMO都是一样，使用ITC（Image-Text Contrastive loss）和ITM（Image-Text Matching loss），只不过第三个从MLM（Masked Language Modeling）换成了LM（Language Modeling）。</p>
<ol>
<li><p>ITC: 使用对比损失来约束image与text的特征，positive靠近，negative远离。</p>
</li>
<li><p>ITM: 选取对比计算中的hard negative，要求网络计算其是否匹配，赋予网络具有挑战的任务。</p>
</li>
<li><p>MLM: BERT的预训练函数，ALBEF和VLMO也用这个目标函数，类似完形填空，将一个句子某个中间词mask掉，再预测这个中间词，属于双向模型</p>
</li>
<li><p>LM：mask掉句子后半部分，然后用前半部分去预测句子后面内容。</p>
</li>
</ol>
<p>其他细节：</p>
<ol>
<li><p>对于三个文本模型来说，它们对应的token不一样，第一个文本模型就用的是CLS Token，第二个用的是Encode，第三个用的是Decode，这些模型都很难训练，因为在做每一次Training Iteration的时候，图像端只需要做一次forward，但文本端在要做三次forward，要分别通过这三个模型去得到对应的特征，然后去算对应的目标函数，所以非常耗时。</p>
</li>
<li><p>BLIP是ALBEF的原班人马做的,所以用到了很多ALBEF的技巧:1)算ITC的时候也用了Momentum Encoder去做更好的Knowledge Distillation和更好的数据级的清理。2）算ITM Loss的时候，也像ALBEF一样利用ITC算的Similarity Score做Hard Negative Mining，从而每次都用最难的负样本去算ITM，增加Loss的有效性。</p>
</li>
</ol>
<h3 id="本文贡献2–Cap-Filter-Model"><a href="#本文贡献2–Cap-Filter-Model" class="headerlink" title="本文贡献2–Cap Filter Model"></a>本文贡献2–Cap Filter Model</h3><p>BLIP这篇论文第二个贡献点，也是最重要的贡献称为Cap Filter Model。目前从网页上爬取的数据集最大的问题是图片文本对不匹配，也就是说这里的TW不好，作者用红色表示，然后Coco是手工标注的，作者认为图片文本一定匹配，用绿色来表示。</p>
<h4 id="filter-model"><a href="#filter-model" class="headerlink" title="filter model"></a>filter model</h4><ol>
<li><p>动机<br> 作者认为，如果用Noisy的数据集去预训练一个模型，效果就不是最好，因此需要清理数据集，从而达到最优解。</p>
</li>
<li><p>训练方法<br> 本文训练一个能够提供图像文本之间相似度的模型，相似度高的匹配，相似度不高的不匹配。训练方法是将已经提前预训练好的MED，也就是BLIP模型拿出来，把图像模型和两个文本模型，就是分别做ITC和ITM的那两个文本模型拿出来在Coco数据集上做微调，微调过后的MED就叫做filter。接下来用这个模型计算图像文本的相似度，尤其是image text matching分数，就可以确认图像和文本是不是一个match。若不match就可以把它拿掉。通过filter作者把原始爬下来的noisy的IT的文本对（红色的tw）变成了更干净的图像文本对（绿色的tw），到这里任务就完成了。</p>
</li>
</ol>
<h4 id="captioner-model"><a href="#captioner-model" class="headerlink" title="captioner model"></a>captioner model</h4><p>作者发现，BLIP模型训练好的decoder非常强，有时候生成的句子比原始的图像文本对好很多，即使原来的图像文本对是一个match，但是新生成的文本更匹配，质量更高。所以作者用生成的文本充当新的训练数据集，具体的，作者在coco数据集上把已经训练好的image grounded text decoder又微调了一下，得到了captioner，然后给定任意一张从网上爬下的图片，用这个captioner给这个图片生成新的字幕，也就是红色这里的ts，经过filter筛选后，添加到数据集中，它是synthetic data。最后通过captioner和filter数据集变大。以cc12million为例，（Iw，Tw）是filter过后的cc12million，它还是原来从网上爬下的图像文本对，只不过是filter过后变少，但质量也变高了。（Iw，Ts）是cc12million合成的新生成的图像文本对。（Ih，Th）是手工标注的Coco数据集。总之数据集不仅变得更大，而且质量变得更高了。再用新的d去预训练BLIP模型，模型的提升非常显著。这就是本文提出的第二个创新点，这个funtune的capfilter模型做到了数据集上的bootstrapping（Bootstrapping算法，指的就是利用有限的样本资料经由多次重复抽样，重新建立起足以代表母体样本分布的新样本）。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161230353.png" alt="image.png"></p>
<p>图中上面的Tw是直接从网页端下载的文本，下面的Ts是captioner新生成的文本，红色的代表被filter掉的文本，绿色代表filter以后保留下来的文本，也就是跟图片更匹配的文本，可以看出filter和captioner的强大之处</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161239353.png" alt="image.png"></p>
<p>表1中对比了captioner ，filtering模式到底带来什么样的提升，C代表captioner，F代表filter。可以观察到：</p>
<ol>
<li><p>不论是用了filter还是用captioner，效果都会有提升。</p>
</li>
<li><p>用captioner以后提升是更加显著的，也就意味着captioner带来的data diversity多样性更让模型受益，因为尤其是对大模型或者大数据集的训练来说，偶尔数据集有点noise无所谓，模型都是能够处理的，但因为模型参数量太大，需要大量大量的数据，所以只要能生成更多更好的数据，它往往就能够受益。</p>
</li>
<li><p>captioner和filter同时用效果就达到最好。这个表格里倒数第3，4两行都是打了对号，也就说都用了captioner和filter，但一个叫base一个叫large因为BLIP跟VLMO一样分阶段训练：<br> 1）stage1：用嘈杂的数据集预训练了一个模型。<br> 2）stage2：用CoCo去funtune captioner和filter，再把数据集重新处理，得到一个新的更大的质量更好的数据集。<br> 3）stage3：用新的数据集又pre-train一个BLIP。<br> 这几个步骤互不相干，可以分开训练或者分开使用。因此第二阶段生成新的数据集的时候，可以用更大的模型去生成更好质量更高的数据集，并不一定是backbone这用的模型是base，capfilter模型就一定要用base，生成数据这一步完全是一个额外的步骤，理论上也可以用任何一种方式去生成pseudo-labeling伪标签。</p>
</li>
</ol>
<p>因此，BLIP large训练出来的这个captioner filter生成更好的数据后，可以训练其他模型，比如VLMO，CoCa，BEiT-3，它是一个非常通用的工具。例如，Laion Coco这个数据集刚开始先推出了Laion 400 Million，和OpenAI的Clip的400 Million数据集去对齐，接下来又推出了更大的Laion 2 Billion，Laion 5 Billion这些开源的大规模数据集，极大的促进了这个多模态学习的进展。他们也用BLIP模型和两个Clip模型不停的做filtering和captioning的过程，最后得到Laion Coco 600 Million这个数据集。它具体的做法：</p>
<p>1）给定任何一张图片，它先用最大的BLIP模型去生成40个caption。</p>
<p>2）用Clip去做Ranking看看最后Retrieve谁排前谁排后，这里用OpenAI Vision Transformer Large选最好的5个caption。</p>
<p>3）用OpenAI的Clip模型，但这次是用ResNet50做一次重新的Ranking然后把最好的那个选出来，这样子你就有一个图像文本对，一个图像对应一个文本。</p>
<p>4）用一个比较小的T0的模型去修复了一下语法，文本的标点符号使文本看起来更真实更正确。</p>
<p>所以BLIP是一篇非常好的论文，不只提出了一个模型框架，而是它提出的Caption Filtering这个方法非常有效，而且具有普适性，可以拿它去做很多的工作。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>05 Atomics, Reductions, and Warp Shuffle</title>
    <url>/2025/01/16/05-Atomics-Reductions-and-Warp-Shuffle/</url>
    <content><![CDATA[<h2 id="归约问题该使用什么线程策略？"><a href="#归约问题该使用什么线程策略？" class="headerlink" title="归约问题该使用什么线程策略？"></a><strong>归约问题该使用什么线程策略？</strong></h2><h3 id="变换（Transformation）问题和归约（Reduction）问题"><a href="#变换（Transformation）问题和归约（Reduction）问题" class="headerlink" title="变换（Transformation）问题和归约（Reduction）问题"></a>变换（Transformation）问题和归约（Reduction）问题</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163703880.png" alt="image.png"></p>
<span id="more"></span>


<h3 id="朴素策略：为每个输入点分配一个线程"><a href="#朴素策略：为每个输入点分配一个线程" class="headerlink" title="朴素策略：为每个输入点分配一个线程"></a>朴素策略：为每个输入点分配一个线程</h3><p><strong>问题：cuda 不保证线程执行顺序，会出现 data race</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163711567.png" alt="image.png"></p>
<h3 id="解决方法：使用-built-in-的原子-read-modify-write-指令，在这里是atomicAdd"><a href="#解决方法：使用-built-in-的原子-read-modify-write-指令，在这里是atomicAdd" class="headerlink" title="解决方法：使用 built-in 的原子 read-modify-write 指令，在这里是atomicAdd"></a>解决方法：使用 built-in 的原子 read-modify-write 指令，在这里是<code>atomicAdd</code></h3><ul>
<li><p>当线程发起原子指令时，L2 缓存中存在一个协调执行机制，可以将其视为一种协调者，它会逐一处理所有这些原子操作。</p>
</li>
<li><p>通过GPU内置的原子硬件，以性能换取可预测性和正确性</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163719716.png" alt="image.png"></p>
<h3 id="其他原子操作"><a href="#其他原子操作" class="headerlink" title="其他原子操作"></a>其他原子操作</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163726438.png" alt="image.png"></p>
<h3 id="原子操作的其他用途"><a href="#原子操作的其他用途" class="headerlink" title="原子操作的其他用途"></a>原子操作的其他用途</h3><h4 id="每个线程获得队列中的下一项"><a href="#每个线程获得队列中的下一项" class="headerlink" title="每个线程获得队列中的下一项"></a>每个线程获得队列中的下一项</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163733534.png" alt="image.png"></p>
<h4 id="每个线程往一片共享内存中写入不定长，连续的一段数据"><a href="#每个线程往一片共享内存中写入不定长，连续的一段数据" class="headerlink" title="每个线程往一片共享内存中写入不定长，连续的一段数据"></a>每个线程往一片共享内存中写入不定长，连续的一段数据</h4><p>可以用atomic加操作预定一个在共享内存中长为 dsize 的片段，atomicAdd操作返回的旧值即为在共享内存中预留空间的起始位置。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163741818.png" alt="image.png"></p>
<h2 id="Classical-Parallel-Reduction"><a href="#Classical-Parallel-Reduction" class="headerlink" title="Classical Parallel Reduction"></a>Classical Parallel Reduction</h2><p>我们需要一个策略能够让我们利用大量的线程，尤其是在输入数据集大小能够支持的情况下，以规避原子操作的限制</p>
<h3 id="朴素策略：基于树的思想"><a href="#朴素策略：基于树的思想" class="headerlink" title="朴素策略：基于树的思想"></a>朴素策略：基于树的思想</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163748963.png" alt="image.png"></p>
<p>上图方法的问题：例如11需要等待4和7的结果算出来才能计算，因此需要做同步操作</p>
<h3 id="解决方法：全局同步操作"><a href="#解决方法：全局同步操作" class="headerlink" title="解决方法：全局同步操作"></a>解决方法：全局同步操作</h3><p>我们需要在整个网格中，跨越所有线程块实现这一功能。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163758817.png" alt="image.png"></p>
<ul>
<li>方案一：分解为多个kernel</li>
</ul>
<p>一种可能的解决方案是将树分解，可以理解为将其分解为一系列kernel，每个树级对应一个kernel启动。对于gpu来说，kernel的执行顺序是确定的，即如果你按顺序执行两个kernel，那第二个kernel不会在第一个kernel执行完之前执行。这样，内核启动的边界为我们提供了全局同步。</p>
<p>问题：</p>
<ul>
<li><ul>
<li><p>内核存在启动开销</p>
<ul>
<li>树的底部所对应的 grid 太小了</li>
</ul>
</li>
</ul>
</li>
<li><p>方案二：线程块耗尽方法</p>
</li>
</ul>
<p>另一种解决方案是使用原子操作，原子操作能确定最后一个修改的线程，一旦我们确定了最后一个完成的线程块，我们就可以将额外的工作分配给该线程块，因为我们知道其他所有线程块均已完成。</p>
<ul>
<li>方案三：协同组（cooperative groups）</li>
</ul>
<p>协同组是CUDA编程模型中较新的特性，它解决CUDA在提供线程协作和线程块分解方面的构造和原语不够丰富的问题，协同组提供了一套新的内置函数和基本组件，使我们能够构建规模更为灵活的线程组，这些线程组能够协同工作，共同执行任务。</p>
<h4 id="方案一的示例，这里使用一个kernel来避免启动内核的开销"><a href="#方案一的示例，这里使用一个kernel来避免启动内核的开销" class="headerlink" title="方案一的示例，这里使用一个kernel来避免启动内核的开销"></a>方案一的示例，这里使用一个kernel来避免启动内核的开销</h4><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163806867.png" alt="image.png"></p>
<h4 id="Grid-Stride-Loops"><a href="#Grid-Stride-Loops" class="headerlink" title="Grid-Stride Loops"></a>Grid-Stride Loops</h4><p>这里的构想和目标在于我们希望创建的 kernel 函数能够实现一种解耦，即 kernel 函数的规模(换言之，网格的大小，即执行操作的线程数量)与数据集大小之间的解耦。即我们希望能够编写并启动一个具有特定线程配置的 kernel，使其能够正确执行操作，无论输入数据集是小于、等于还是大于所使用的线程数量。</p>
<p>我们的想法是拥有固定数量的线程，数量等于 grid 的宽度，这些线程最初将处理输入数据集中与网格宽度相对应的大小，处理完毕后，它们将跨步前进，将一个索引值</p>
<p>加入操作中，然后利用下一块数据重复该操作。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163812754.png" alt="image.png"></p>
<h4 id="方案一的完整实现"><a href="#方案一的完整实现" class="headerlink" title="方案一的完整实现"></a>方案一的完整实现</h4><p>优化：每个 block 仅最后执行一次原子操作，从而避免了启动第二个 kernel 对 out 里每个 block 产出的归约结果进行二次归约</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163823849.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163829778.png" alt="image.png"></p>
<h2 id="Wrap-Shuffle"><a href="#Wrap-Shuffle" class="headerlink" title="Wrap Shuffle"></a>Wrap Shuffle</h2><p>共享内存是实现线程间数据通信的途径。为了以这种方式使用共享内存，需要进行两个操作。一个线程需向共享内存写入数据，另一个线程需要从共享内存中的该位置读取数据。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163836837.png" alt="image.png"></p>
<p>如果有一种方法可以直接在不同线程间传递数据，而不使用共享内存，那岂不是很好？</p>
<p><strong>Warp Shuffle背后的理念是，它允许 warp 内部实现这种直接的线程间通信。</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163844387.png" alt="image.png"></p>
<p>例如<code>__shfl_down_sync()</code>这个函数是将被 mask 指定的线程A，再会往后偏移 delta，这样得出来的线程B，线程A会拿到线程B的var，其他如果偏移过头的线程会返回0。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163852373.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163858917.png" alt="image.png"></p>
<h3 id="修改之前的归约代码"><a href="#修改之前的归约代码" class="headerlink" title="修改之前的归约代码"></a>修改之前的归约代码</h3><ul>
<li><p>现在每个线程不再将运行总和保存在共享内存中，而是将其作为名为val的局部变量保存。</p>
</li>
<li><p>mask 表示我们的洗牌操作需要利用整个 warp 中的所有32个线程</p>
</li>
<li><p>Lane 指的是在 warp 中处于哪个线程，WarpID 指的是处于哪个 wrap</p>
</li>
<li><p>1^st warp-shuffle reduction：初始 offset 设为 warpSize 的一半（即16）。在第一轮循环中，这意味着对于线程零，它将取其自身的 val 值，并将其与来自线程束大小的一半（即16）的 val值相加。最后每个 warp 算出的归约结果将存在 sdata 中。一个 block 中最多有1024个线程，因此最多有32个 wrap。</p>
</li>
<li><p>我们必须确保每个 warp 都完成了该操作然后才能允许任何进程继续进行，因此使用<code>__Syncthreads()</code>同步。</p>
</li>
<li><p>2^st warp-shuffle reduction：接下来执行另一次 warp 洗牌操作，但仅需使用第0个 warp, 因为现在共享内存中最多只有32个项</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163906176.png" alt="image.png"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163914434.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116163922813.png" alt="image.png"></p>
<h2 id="Loop-Unrolling"><a href="#Loop-Unrolling" class="headerlink" title="Loop Unrolling"></a>Loop Unrolling</h2><p>循环展开的原理是，对于一个已知范围的循环，例如 for 循环，编译器能够确定其范围，通常会自动为你展开这个循环。这意味着将循环的每一次迭代按顺序展开，因此，你正在摒弃循环，而是将循环的每次迭代按顺序编入代码中。</p>
<p>这仅仅是优化的第一步，编译器希望这样做的原因之一是，优化过程的下一步通常涉及指令重排。编译器会在保持代码正确性的同时，积极地重新排序操作，以提供最佳性能。我们希望实现的是尽可能立即启动所有内存操作，以便它们能迅速进入队列，从而使内存子系统尽早工作起来。越早将内存子系统加载请求，数据返回得就越快，计算结果的速度也会随之提升。</p>
<p>如果我在循环中每次迭代都从全局内存加载数据，那么在第10次循环迭代时，如果仅遵循循环语义，我将加载第10次循环所需要的数据。但是经常发生的情况是加载操作是独立的，那么就没有理由不能立即进行加载，立即启动该加载操作，然后按照可完成顺序执行我能进行的操作。</p>
<p>通常情况下，了解循环的总次数，即遍历范围，是很有必要的。若能编写具有可识别循环次数的循环，编译器更可能对其进行积极展开。</p>
<p>05_Atomics_Reductions_Warp_Shuffle.pdf</p>
]]></content>
      <categories>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>BeiTv3</title>
    <url>/2025/01/16/BeiTv3/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161612285.png" alt="image.png"></p>
<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131609713">https://blog.csdn.net/lansebingxuan/article/details/13160...</a>﻿</p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/2208.10442">Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks</a>﻿</p>
<p>论文代码：<a href="https://github.com/microsoft/unilm/tree/master/beit3">BEiT-3</a>﻿</p>
<h2 id="引言-Big-Convergence（大一统）"><a href="#引言-Big-Convergence（大一统）" class="headerlink" title="引言 Big Convergence（大一统）"></a>引言 Big Convergence（大一统）</h2><p>最近不论是在Language、Vision还是在多模态领域，Big Convergence是大势所趋，也就是在超大的数据集上做大规模的预训练，一旦模型训练好之后，它的特征就已经非常好了，可以直接Transfer到下游任务上去，尤其是当模型足够大、数据足够多的时候，有可能预训练出来一个有通用性能的Foundation Model，这个Foundation Model能解决各种的模态或者各种下游任务，非常的强大。</p>
<p>本文将大一统继续向前推进，彻底将多模态尤其是Vision Language预训练得很好，主要是从以下的三个方面实现大一统：</p>
<span id="more"></span>

<p><strong>1. 模型：</strong></p>
<p>从模型角度来说，Transformer非常关键：</p>
<p><strong>1）Transformer框架相比CNN的优势</strong>：未来肯定是多模态的，一个模型做所有的Modality，所有的Task，肯定是一个大一统的框架，CNN不太适合做其他的Modality，而Transformer就适合做很多的Modality。从这一点上对比Transformer就胜出了。</p>
<p><strong>2）多模态学习常用框架</strong>：Transformer刚开始是NLP用的，然后逐渐用到Vision和多模态领域，现在对于Vision Language Modeling多模态学习来说，有几个常用的方式：</p>
<ul>
<li><p>Clip的Dual Encoder框架：该框架非常适合做快速的Retrieval。</p>
</li>
<li><p>Encoder、Decoder框架：该框架适合做Generation Task，比如BLIP、Coca。</p>
</li>
<li><p>Fusion Encoder框架：只用Encoder，但是它有多模态融合部分，ALBEF、VLMO都属于这一类，能做Image Text Encoding。</p>
</li>
</ul>
<p>不论是哪种方式，这些模型在遇到下游任务的时候，因为输入的形式可能会改变或者输出的形式有时候会改变，所以模型需要根据下游任务去做一些改进，因此不是真正意义上的General Purpose Modeling。BEITV3用1个统一的Masked Data Modeling模型，每一个训练好的Transformer Block，SA层，或者Feed Forward Network都可以随意的拼装和组合。</p>
<p><strong>2. 预训练的目标函数</strong></p>
<p>因为掩码学习已经能够很好的学习图像文本或者多模态的特征，因此本文验证只用这一个目标函数训练模型。这种方式的优点如下：</p>
<p>1）当数据和模型变大的时候，用更多的目标函数训练速度肯定会变慢。</p>
<p>2）多个loss的权重调整比较难，有的Loss之间可能互补，有的Loss之间可能互斥，增加了很多的复杂度。而单个loss训练就比较简单。</p>
<p>因此，本文使用了一个Pretraining Task，就是Mask Then Predict，因为图像经过Vision Transformer Embedding层以后，就变成了一个Sequence of Token，因此可以将图像看成一个Foreign Language叫Imaglish，这样就能将文本和图像用同样的方式去处理，本质上就没有任何区别。多模态的图像文本对就可以把它看成是一个Parallel Sentence，就是句子1后面跟了个句子2，那就没有什么不同了，一切都变成NLP（当然这也从侧面说明Mask Data Modeling目标函数非常的强）。</p>
<p><strong>3. 扩大模型和数据集的规模</strong></p>
<p>作者将模型大小扩展到Billions of Parameters，数据集也扩展的非常大，不过用的都是开源数据集。</p>
<h2 id="BEIT-3预训练框架"><a href="#BEIT-3预训练框架" class="headerlink" title="BEIT-3预训练框架"></a>BEIT-3预训练框架</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161621466.png" alt="image.png"></p>
<p>模型用了Multi-Way Transformer，前面的自注意力全都是Share weights（Shared Multi-Head Self-Attention），只有后面Feed Forward Network不一样，根据不同的Modality训练不同的Vision、Language、Vision Language三个不同的Expert，然后通过调整不同的Input Modality去选择模型分支。Mask Data Modeling目标函数可能遮住了图像，可能是遮住了文本，模型训练学习去恢复它就可以。</p>
<h2 id="下游任务实现框架"><a href="#下游任务实现框架" class="headerlink" title="下游任务实现框架"></a>下游任务实现框架</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161635788.png" alt="image.png"></p>
<p>做下游任务时候：</p>
<ol>
<li>如图3（a），如果用Vision Encoder，就可以做所有图像任务，包括Classification, Detection和Segmentation。 2. 如图3（b），如果用Language Encoder，就可以去做Language的各种任务。 3. 如图3（c），如果用Vision Language Fusion Encoder，就可以做多模态的各种任务Vision Language Understanding Task。 4. 如图3（d），如果用 Dual Encoder，把这两个Vision和Language分开，变成双塔结构，就可以像CLIP一样做比较高效的Image Text Retrieval，如果Funtune，可以再用ITC去Funtune。 5. 如图3（e），如果用Image-to-Text Generation，就可以做生成Captioning任务，给定一句话，将下一个文本Mask掉，然后你Image Grounded的Text Encoder去预测被Mask掉的单词，就可以去做Image Captioning。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BEITv3其实从方法上来说就是之前BEIT、BEITv2、VLBEIT、VLMO等一系列的工作的一个集合体，本身没有提出新的内容，主要就是把它做大做强，展示了一个Unified Framework能达到的性能。</p>
<p>BEiTv3的目标非常明确，就是想做一个更大一统的框架，不论是从模型上统一，而且从训练的目标函数上要统一，还有模型大小，数据集大小，如何scale也要统一，作者称之为Big Convergence。BEiTv3就是把图像也看成了是一种语言（这就是他们题目的意思叫做Image as a Foreign Language），文章把Image叫做Imagelish，文本叫做English，然后把图像文本对叫做Parallel Sentence。因为不论是图像还是文本都可以用Mask Modeling去做,所以就不需要ITC，ITM ，Language Modeling或者Word Patch Alignment等各种Loss，只用一个Loss—– Mask Modeling。模型层面用的是他们之前VLMO提出的MOME，也就是文中的Multi-Way Transformers</p>
<p>总之，BEiTv3用一个非常简单而且非常容易扩展的框架，一个目标函数，Mask Modeling，但是效果非常好。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>ALBEF</title>
    <url>/2025/01/16/ALBEF/</url>
    <content><![CDATA[<p>ALBEF来自于Align before Fuse，作者团队全自来自于Salesforce Research。</p>
<p>论文地址：<a href="https://arxiv.org/pdf/2107.07651.pdf">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>﻿</p>
<p>论文代码：<a href="https://github.com/salesforce/ALBEF">ALBEF</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>最近图像文本的大规模的特征学习非常火爆，大部分已有的方法都是用一个Transformer模型作为多模态的一个编码器，同时编码视觉的Token和文本的Token，视觉Token就是视觉特征，一般是region-based的图像特征。</p>
<p><strong>本文贡献1–ALign BEfore Fuse：</strong></p>
<p>ViLT和ALBEF都认为不需要目标检测的模型，但ViLT只是说用了目标检测模型以后速度太慢，希望推理时间变得更快，但是ALBEF分析认为<strong>使用预训练的目标检测器之后，视觉特征和文本特征无法align对齐</strong>，因为目标检测器是提前训练好的，只抽取特征，没有再进行end-to-end的训练，所以导致视觉特征和文本特征可能相隔很远，此时将两个特征同时送入多模态的编码器之后，编码器学习图像文本之间的交互信息就会变得很有挑战。<strong>作者提出对比学习的ITC Loss，将图像和文本在Fusing之前实现Align，也就是他们论文的题目ALign BEfore Fuse。</strong></p>
<p><strong>本文贡献2–自训练方式学习：</strong></p>
<p>网上爬取的数据特别noisy，因为图片文本对里面的文本通常具备搜索属性，但不是好的描述性句子，甚至没有描述，这种noisy web data的问题影响模型有效地学习文本图像特征，所以作者<strong>使用Momentum Distillation也就是自训练的方式去学习</strong>，自训练就是用Pseudo Label伪标签去学习。<strong>作者采用Moco提出的Momentum Encoder的形式生成Pseudo Target，从而达到一个自训练的效果。</strong></p>
<p>在图文检索的任务上，ALBEF的效果最好，性能反超之前的方法，尤其是在特别大的数据上训练过的模型CLIP和Align。在VQA和VR任务上，ALBEF也比之前state of art提升2.37%、3.84%的准确度，而且推理时间也更快。模型在4 million的训练数据集下，能做到一个8卡A100训练三四天的时间，训练门槛大大降低。</p>
<span id="more"></span>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>参考 ViLT 综述部分</p>
<h1 id="ALBEF方法"><a href="#ALBEF方法" class="headerlink" title="ALBEF方法"></a>ALBEF方法</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160941204.png" alt="image.png"></p>
<p><strong>图像端：</strong>给定任何一张图片，按照Vision Transformer的做法，把它打成patch，然后通过patch embedding layer送给一个Vision Transformer。这里是一个非常标准的12层的Vision Transformer的base模型，如果图片是224x224，那这里的sequence length就是196，然后加上额外的一个CLS token就是197，它的特征维度是768，所以这里绿黄色的特征就是197乘以768。但论文在预训练阶段用的图片是256x256，所以这里绿色的sequence length就会相应的再长一些。它的预训练参数用的DEiT，也就是Data Efficient Vision Transformer在ImageNet 1K数据集上训练出来的初始化参数。</p>
<p><strong>文本端：</strong>文本端为保持计算量与clip类似，并且增强模态融合的部分，用前六层做文本编码，剩下的六层transformer encoder作为multi-model fusion的过程。文本模型用BERT模型做初始化，它中间的特征维度也是768，它也有一个CLS token代表了整个句子的文本信息。</p>
<p><strong>momentum model：</strong>ALBEF为了做momentum distillation，而且为了给ITC loss提供更多negative，增加了momentum model，这个模型参数由左边训练的模型参数通过moving average得到的（和MoCo一模一样），通过把moving average的参数设的非常高（论文里是0.995）来保证momentum model不会那么快更新，产生的特征更加稳定，不仅可以做更稳定的negative sample，而且还可以去做momentum distillation。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li><p><strong>ITC loss</strong>: 对比学习就是首先定义一个正样本对，然后定义很多负样本对，对比使正样本对之间的距离越来越近，正负样本对之间的距离越来越远。在ALBEF里首先将图像I通过vision transformer之后得到图像的全局特征，图中黄色CLS token作为全局特征，也就是一个768×1的向量，文本这边先做tokenize，将文本text变成tokenization的序列，再输入BERT的前六层，得到了一系列的特征，文本端的CLS token作为文本的全局特征，也是一个768×1的向量。接下来与MoCo相同，图像的特征向量先做一下downsample和normalization，将768×1变成了256×1的向量，文本特征向量也是768变成256×1。正样本这两个特征距离尽可能的近，它的负样本全都存在一个q里，含有65536个负样本（因为它由momentum model产生的，没有gradient，所以它并不占很多内存），正负样本之间的对比学习，使得这两个特征距离尽可能的远。这个过程就是align before fuse的align，也就是说在图像特征和这个文本特征输入Multi-model Fusion的encoder之前，就已经通过对比学习的ITC loss让这个图像特征和文本特征尽可能的拉近，在同一个embedding space里，具体使用了cross entropy loss。</p>
</li>
<li><p><strong>ITM loss：Image Text Matching</strong>，属于一个二分类任务，就是给定一个图片，给定一个文本，图像文本通过ALBEF的模型之后输出一个特征，在这个特征之后加一个分类头，也就是一个FC层，然后去判断I和T是不是一个对，</p>
</li>
</ul>
<p>这个loss虽然很合理，但是实际操作的时候发现这个loss太简单，所以这个分类任务，很快它的准确度就提升得很高无法进一步优化。因此ALBEF通过某种方式选择最难的负样本（最接近于正样本的那个负样本），具体来说ALBEF的batch size是512，所以ITM loss正样本对就是512个，对于mini batch里的每一张图像，把这张图片和同一个batch里所有的文本都算一遍cos similarity，然后它在这里选择除了它自己之外相似度最高的文本当做negative，这样ITM loss就非常有难度</p>
<ul>
<li><strong>MLM：Mask Language Modeling</strong>，它把原来完整的句子text T变成一个T’，也就是有些单词被mask掉了, 然后它把缺失的句子和图片一起通过ALBEF的模型，最后把之前的完整的句子给预测出来，它这里也借助了图像的信息去更好的恢复被mask掉的单词。</li>
</ul>
<p>计算ITC loss和ITM loss的时候，输入都是原始的i和t，但是当计算MLM loss的时候，它的输入是原始的i和mask后的T’，所以模型每一个训练的iteration都做了两次模型的forward：一次模型的forward用了原始的i和t，另一次模型的forward用了原始的i和mask后的T’。为了计算不同的loss，这种多次前项在多模态学习中使用普遍。</p>
<p><strong>ALBEF的目标函数:</strong>$\mathcal{L} &#x3D; \mathcal{L}<em>{itc} + \mathcal{L}</em>{mlm} + \mathcal{L}_{itm} $，也就是itc、mlm和itm的合体</p>
<h2 id="momentum-distillation-动量蒸馏"><a href="#momentum-distillation-动量蒸馏" class="headerlink" title="momentum distillation 动量蒸馏"></a>momentum distillation 动量蒸馏</h2><p><strong>动机：</strong></p>
<p>由于从网上爬下来的数据噪声严重，图像文本对经常是弱相关，甚至不匹配，这种noisy的data会导致计算目标函数时有偏差。比如在算ITC的时候，某个负样本文本很有可能也描述了图像里的很多内容，它可能不是爬下来的image text pair，但文本甚至可能比Ground Truth描述的还好。若此时将其作为一个负样本，就会对ITC的学习造成很大的影响。</p>
<p><strong>改进方式：</strong></p>
<p>作者认为one hot label（就是图片和文本就是一对，其他跟它都不是一对）对于ITC和MLM这两个loss来说不好，因为有的负样本也包含了很多的信息。所以作者采取了自训练方式，先构建一个momentum model然后用这个动量模型去生成pseudo targets伪目标（其实就是一个softmax score），这样它就不再是一个one hot label。</p>
<p>动量模型在已有模型之上做exponential moving average EMA，目的是在原始模型训练的时候，不仅希望模型预测与ground truth的one hot label去尽可能的接近，还希望模型预测与动量模型出来的pseudo targets尽可能的匹配，这样就能达到一个比较好的折中点。因为当one hot label正确时，可以学习到很多信息，但当one hot label是错误的，或者是noisy的时候，作者希望稳定的momentum model能够提供一些改进。</p>
<p>以ITC loss为例，它是基于这个one hot label的，所以这里再算一个pseudo target loss去弥补它的一些缺陷和不足。这个loss跟前面equation1里的ITC loss的不同，这里将ground truth换成这个q，就是pseudo targets，q不再是one hot label，而是softmax score，所以这里计算KL divergence而不是cross entropy。</p>
<p><strong>最终结果：</strong></p>
<p>最终ALBEF的训练loss有五个：两个ITC、两个MLM、一个ITM，其中：</p>
<ul>
<li><p>ITC有两个loss：一个是原始的ITC，一个是基于pseudo target的ITC，所以分别加权（1-α）和α的loss weight，最终得到momentum版本的ITC loss。</p>
</li>
<li><p>MLM loss有两个loss：一个是原始的MLM，一个是基于pseudo target的MLM，用新生成的pseudo target去代替了原来的ground truth，分别加权（1-α）和α的loss weight，最终得到momentum版本的MLM loss。</p>
</li>
<li><p>ITM有一个loss：ITM没有动量版本，因为本身它就是基于ground truth，它就是一个二分类任务，而且在ITM里又做了hard negative，这跟momentum model有冲突。</p>
</li>
</ul>
<h1 id="下游Vision-Language任务"><a href="#下游Vision-Language任务" class="headerlink" title="下游Vision Language任务"></a>下游Vision Language任务</h1><p>ALBEF这篇论文做了五个任务：</p>
<ol>
<li><strong>图文检索（图像到文本、文本到图像、图像到图像、文本到文本）：</strong></li>
</ol>
<p>定义：给定一个数据库，去搜索Ground Truth的图像文本对。</p>
<p>衡量指标：因为是检索，衡量的指标就是这个Recall 召回率，一般用的是R1,R5,R10，就是说在你检索回来的一个、五个或者十个Sample里是否有Ground Truth Sample，有就算正确。</p>
<ol>
<li><strong>视觉蕴含(Visual Entailment)：</strong></li>
</ol>
<p>定义：给定一个假设或者一个前提，能否推理出这个前提，如果能推理出来，就是一个蕴含的关系Entailment。如果前后矛盾推不出来，就是矛盾Contradictory。如果没关系，不确定能否推出来，就是中立Neutral。</p>
<p>衡量指标：一般将Visual Entailment变成一个三分类的问题，衡量的指标就是分类准确度。</p>
<ol>
<li><strong>视觉问答（VQA）：</strong></li>
</ol>
<p>定义：给定一个问题一个图片，能否提供一个Answer回答这个问题，一般有两个Setting：一个是看作分类问题，它的这些答案都是固定的，从一个集合中选择一个正确答案，一般称作闭集VQA，例如VQA2.0数据集有一个提前设好的3192个Answer。一个是看作生成问题，需要一个Transformer Decoder做文本生成的任务，一般称作开集VQA，开集VQA它的任务难度大很多，因为有可能生成了正确的或者很相似的答案，但是它跟Ground Truth不一致，还是会被判错。</p>
<p>衡量指标：分类准确度。</p>
<ol>
<li><strong>视觉推理（Visual Reasoning）：</strong></li>
</ol>
<p>定义：预测一个文本能不能描述一张图片，所以它是一个二分类任务问题</p>
<p>衡量指标：分类准确度。</p>
<ol>
<li><strong>Visual Grounding</strong></li>
</ol>
<p>其实Visual Grounding属于它自己的一个领域，很多多模态表征学习的论文里， 都不会去涉及这个的任务. ALBEF虽然做了Answer Generation的问题，但在做推理的时候，还是把生成的答案限制到3192个答案里，所以还是一个分类问题。</p>
<p><strong>消融实验</strong></p>
<p>作者将用了MLM和ITM training loss的模型当做了baseline，因为基本上所有的之前的工作和现在的工作里面都会有这两个loss。</p>
<ul>
<li><p>增加ITC也就是Align Before Fuse里的Align，这个提升是非常巨大的，基本是两个多点，三个点，而且是在检索、VE、VR、VQA这么多任务上都有明显的提升，所以CLIP、MoCo这种对比学习的方式非常厉害</p>
</li>
<li><p>增加ITM里提出的Hard Negative，大概都有0.5左右的提升</p>
</li>
<li><p>增加Momentum Distillation（MoD），在预训练阶段的MoD提升可能只有0.3 0.4个点，但是这个研究方向还是很好的，就是怎么从Noise Data里去学习有效的表征。</p>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>﻿<a href="https://blog.csdn.net/lansebingxuan/article/details/131721728">多模态系列论文–ALBEF 详细解析-CSDN博客</a></li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>CoCa</title>
    <url>/2025/01/16/CoCa/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131611916">https://blog.csdn.net/lansebingxuan/article/details/13161...</a>﻿</p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/2205.01917v1">CoCa: Contrastive Captioners are Image-Text Foundation Models</a>﻿</p>
<p>代码地址：<a href="https://github.com/lucidrains/CoCa-pytorch">CoCa</a>﻿</p>
<p>CoCa代表Contrastive Captioners的缩写，代表模型用两个目标函数训练出来的，一个是Contrastive Loss，一个是Captioning Loss。本文因为数据集更大，模型也更大，所以它的效果很好，在多模态所有的任务均SOTA，而且在单模态里，在ImageNet上也得到了90以上的Top1准确度，在视频动作识别领域，在Paper with Code上CoCa在K400、K600、K700这些数据集上排名前三。</p>
<span id="more"></span>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161514502.png" alt="image.png"></p>
<p>CoCa是ALBEF的一个后续工作，它与ALBEF的模型类似，左边是一个Image Encoder，右边是一个Text <strong>Decoder</strong>，注意，这里是Decoder</p>
<p>不是Encoder。从左右来看还是左边图像分支，右边文本分支，文本分支分两部分，前面用来抽取Unimodel的文本特征，后面做<a href="https://so.csdn.net/so/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81&spm=1001.2101.3001.7020">多模态</a>的特征。整个模型就是用两个目标函数训出来的，一个是ITC，一个是Language Modeling Loss，也就是Contrastive和Captioning，具体步骤如下：</p>
<ol>
<li><p>图像通过Image Encoder，得到一系列的Token，文本通过文本的解码器，得到一系列的文本特征。</p>
</li>
<li><p>图像的CLS Token和文本的CLS Token计算ITC loss</p>
</li>
<li><p>图像其他的Token做Attention Pooling，然后再传到多模态的Text Decoder里做Cross Attention，这样把视觉和文本的特征融合在一起了。多模态的特征用Captioning Loss训练，也就是BLIP、GPT用的Language Modeling Loss。</p>
</li>
</ol>
<p>所以CoCa的布局跟ALBEF是一模一样的，区别是：</p>
<ol>
<li><p>在图像分支做Attentional Pooling，这一部分是可学的，这种可学的Pooling方式能够针对不同的任务学到更好的特征。</p>
</li>
<li><p>不论是单模态的文本特征的学习还是多模态的特征学习，整个文本端统一都用Decoder训练目标函数，使用Captioning的Loss，文本的输入从一开始前面的Self-Attention Layer就是Causal的（也就是mask住一个句子后半部分，然后用前半部分去预测句子后面内容）。因为作者在超大的几十亿的数据集上去做预训练，所以文本如何mask关系不大，模型应该是什么都见过。</p>
</li>
</ol>
<p>Coca的模型实现并不难，但是想复现它难度非常大。原因是：</p>
<ol>
<li><p>模型大：虽然很简单，但它训练出来最大的模型参数量已经达到了2.1 billion，算是视觉或者多模态里面非常大的一个模型（当然在NLP那边已经有几十亿上百亿的模型）</p>
</li>
<li><p>训练的数据集∶作者不只用了之前训练Align用的多模态的数据集，同时还把GFT 3 billion（google私有数据）图像分类的数据集转化成了多模态数据集，加在一起有几十亿的训练数据，所以不论是模型还是这个数据都远超之前所有工作的这个scale，效果也是非常明显的。</p>
</li>
</ol>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ol>
<li><p>ITC loss：Contrastive Loss，图像的CLS Token和文本的CLS Token计算ITC loss。</p>
</li>
<li><p>LM(Captioning) Loss：单模态、多模态的文本特征学习，计算LM Loss。</p>
</li>
</ol>
<p>文本端统一都用Decoder训练目标函数，并且只用一个Captioning Loss而不用ITM Loss，原因是作者这里想解决训练的效率问题，之前不论是ALBEF还是VLMO，因为算各种的目标函数，往往一个Training Iteration要forward这个模型好几次，无形中增加了模型训练的时间长度，比如训练100个Epoch，其实forward三次之后相当于训练了300个Epoch。作者这里<strong>为了让ITC Loss和Captioning Loss能同时计算，所以文本的输入从刚开始就必须是Causal的，这样通过Unimodal Text Decoder出来的特征能直接做ITC Loss，同样的输入得到的多模态特征也直接能做Captioning Loss</strong>。这样一个Iteration就是只forward一次，训练时间就会降低一些。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实不论用ALBEF或者CoCa模型结构，还是VLMO、 BLIP，共享参数都是可以的。当把这个模型做大，数据集做大后，模型性能都差不多，其实往往最后拼的都是数据</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>MOCO</title>
    <url>/2025/01/15/MOCO/</url>
    <content><![CDATA[<p>论文地址 : <a href="https://arxiv.org/abs/1911.05722">https://arxiv.org/abs/1911.05722</a>﻿</p>
<p>论文代码(官方) : <a href="https://github.com/facebookresearch/moco">https://github.com/facebookresearch/moco</a>﻿</p>
<p>视频解读 : <a href="https://www.bilibili.com/video/av422340209">MoCo 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a>﻿</p>
<span id="more"></span>

<h1 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h1><p>参考 CLIP 对比学习章节</p>
<h1 id="MoCo实现思路"><a href="#MoCo实现思路" class="headerlink" title="MoCo实现思路"></a>MoCo实现思路</h1><p>传统的对比学习可以归纳成一种任务：给定一个query和若干keys，找出这个query和以下那个key相似。</p>
<p>如图所示：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250115201923281.png" alt="image.png"></p>
<p>其中keys集合的大小就是Batch Size的大小。（Keys集合也称为“字典”）</p>
<p>如果想要训练出好的模型，那我们就需要如下两点：</p>
<ul>
<li><p>字典越大越好</p>
</li>
<li><p>字典中的key的特征要保持一致性。换句话说：这些正负样本要保证是从一个Encoder中提取的特征。即使不是从一个Encoder提取的，也要是从相似的Encoder中提取的。</p>
</li>
</ul>
<p>而之前的对比学习方法对于这两点没有办法做到兼容。例如对于上述提到的SimCLR的字典大小就是Batch Size大小，所以受限于显卡内存无法做到太大。 而一些其他论文虽然构建了一个大词典，但是词典中的特征向量都是来自不同阶段的Encoder，所以一致性不高。（不同阶段是指在训练过程中，Encoder参数是不断变化的）</p>
<blockquote>
<p>与之前对比学习论文不同，在MoCo中，锚点称为query，而正负样本称为Key。</p>
</blockquote>
<h1 id="MoCo架构"><a href="#MoCo架构" class="headerlink" title="MoCo架构"></a>MoCo架构</h1><p>MoCo主要就是解决了两个问题：</p>
<ol>
<li><p>字典可以搞的很大</p>
</li>
<li><p>字典中的key的一致性很高。</p>
</li>
</ol>
<p>MoCo的架构如下:</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250115201931470.png" alt="image.png"></p>
<p>MoCo架构与之前对比学习类似，query和keys（就是锚点和正负样本）分别经过两个不同的Encoder，然后再计算相似度，最终预测字典中哪个key和query是一对儿。</p>
<p>MoCo与之前对比学习方法的区别主要在keys这边，主要有两点：</p>
<ul>
<li><p><strong>Queue</strong>: MoCo维护了一个先进先出队列(FIFO queue)，这个队列就是字典，里面存储的是图片的特征向量，所以可以很大。而队列的维护方式就是每次入队最新一批图片的特征向量，出队最早一批图片的特征向量。</p>
</li>
<li><p><strong>Momentum Encoder</strong>：为了保证queue中key的一致性，字典这边的encoder必须要缓慢的更新。所以作者采用了动量的方式。简单来说，就是Momentum Encoder &#x3D; 0.99 * Momentum Encoder + 0.01 * Encoder。每次Encoder更新后，只拿它参数的0.01来更新Momentum Encoder，这样就可以保证Momentum Encoder更新缓慢了。</p>
</li>
</ul>
<p>举个例子来描述整个过程：假设字典(队列)大小为65536, batch size为128, Encoder编码的特征向量大小为256.</p>
<ol>
<li><p>初始化Encoder：首先初始化两个Encoder，一个作为Encoder，一个作为Momentum Encoder。然后让两个Encoder的参数保持一致。</p>
</li>
<li><p>初始化队列：拿65536张图片通过Momentum Encoder进行特征提取，然后放入队列。其中每个图片的特征为256.</p>
</li>
<li><p>接下来开始训练：</p>
<ol>
<li><p>从数据集中随机采样128张图片</p>
</li>
<li><p>将这128张图片进行随机数据增强后送给Encoder，计算出其特征向量。</p>
</li>
<li><p>将这128张图片再次随机数据增强后送给Momentum Encoder，计算其特征向量。此时我们就得到了两组特征向量，每组都有128个256维的特征向量。我们这里将其命名为$x_q$和$x_k$﻿</p>
</li>
<li><p>将$x_q$和$x_k$中元素“对应相乘”，得到128个数值。也就是得到了128个锚点与其对应的正样本的内积（相当于将向量内积作为相似度）</p>
</li>
<li><p>将$x_q$和队列中的向量“两两相乘”，得到128x65536个数值。也就是128个锚点和队列中所有负样本的内积。</p>
</li>
<li><p>将锚点和与正样本的内积和与负样本的内积concat到一起，得到128x(1+65536)个内积。</p>
</li>
<li><p>使用CrossEntropyLoss求出锚点与正样本对应的loss。即锚点与正样本越接近loss越小，锚点与负样本远离loss越大。</p>
</li>
<li><p>反向传播更新encoder参数</p>
</li>
<li><p>使用momentum机制更新Momentum Encoder，即Momentum Encoder &#x3D; 0.99 * Momentum Encoder + 0.01 * Encoder</p>
</li>
<li><p>将$x_k$入队，响应的最早的一批128个特征向量出队。</p>
</li>
</ol>
</li>
<li><p>重复3的训练过程</p>
</li>
</ol>
<h1 id="MOCO代码实现"><a href="#MOCO代码实现" class="headerlink" title="MOCO代码实现"></a>MOCO代码实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先假设存在这么几个对象</span></span><br><span class="line"><span class="comment"># f_q, f_k: query和key的Encoder，也就是Encoder和Momentum Encoder</span></span><br><span class="line"><span class="comment"># queue: 字典队列。为一个Shape为CxK的Tensor。其中C为特征向量的维度，K为词典的大小。例如：256x65536</span></span><br><span class="line"><span class="comment"># m: momentum超参。例如 0.99</span></span><br><span class="line"><span class="comment"># t: temperature。温度超参。例如：0.05</span></span><br><span class="line"></span><br><span class="line">f_k.params = f_q.params <span class="comment"># 初始f_k的参数和f_q保持一致</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># 从dataloader中拿出 N 个数据。x为一个batch的图片，例如x的shape为(128, 3, 224, 224)</span></span><br><span class="line">	x_q = aug(x) <span class="comment"># 对 x 进行随机的数据增强</span></span><br><span class="line">	x_k = aug(x) <span class="comment"># 对 x 进行另一种随机的数据增强</span></span><br><span class="line">	</span><br><span class="line">	q = f_q.forward(x_q) <span class="comment"># 使用Encoder提取x_q的特征。q的shape为 NxC。 例如128x256, 即128张图片，每张图片的特征向量为256维</span></span><br><span class="line">	k = f_k.forward(x_k) <span class="comment"># 使用Momentum Encoder提取x_k的特征，q的shape也为 NxC</span></span><br><span class="line">	k = k.detach() <span class="comment"># Momentum Encoder不计算梯度</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 求出锚点与正样本的内积。l_pos的shape为 Nx1。即每个锚点和对应正样本的向量内积（两两相乘）。例如 shape为128x1</span></span><br><span class="line">	l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 让锚点和队列中所有的负样本计算内积。l_neg的shape为 NxK。例如 shape为128x65536</span></span><br><span class="line">	l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将正样本和负样本concat到一起，正样本放在最前面。logits的shape为 Nx(K+1)。例如 128x(65536+1)</span></span><br><span class="line">	logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	labels = zeros(N) <span class="comment"># 构造Label，因为正样本都是在0这个位置，所以构造N个0就行了。</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 使用CrossEntropyLoss计算损失，这里需要除以温度</span></span><br><span class="line">	loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 反向传播并更新f_q的参数。</span></span><br><span class="line">	loss.backward()</span><br><span class="line">	update(f_q.params)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用momentum的方式更新f_k</span></span><br><span class="line">	f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 更新字典</span></span><br><span class="line">	enqueue(queue, k) <span class="comment"># 将当前的 N 个样本的特征向量入队。</span></span><br><span class="line">	dequeue(queue) <span class="comment"># 同时再将最早的 N 个样本出队</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>对比学习</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title>VLMO</title>
    <url>/2025/01/16/VLMO/</url>
    <content><![CDATA[<blockquote>
<p>🔗 原文链接：<a href="https://blog.csdn.net/lansebingxuan/article/details/131749829">https://blog.csdn.net/lansebingxuan/article/details/13174...</a>﻿</p>
</blockquote>
<p>论文地址：<a href="https://arxiv.org/pdf/2111.02358.pdf">VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a>﻿</p>
<p>论文代码：<a href="https://github.com/microsoft/unilm/tree/master/vlmo">VLMO</a>﻿</p>
<p>VLMo提出了一种统一的视觉语言预训练模型（VLMO），该模型既可以用作双编码器，对检索任务的图像和文本进行单独编码，也可以用作融合编码器，对分类任务的图像-文本对的深度交互进行建模。</p>
<span id="more"></span>
<h2 id="研究动机"><a href="#研究动机" class="headerlink" title="研究动机"></a>研究动机</h2><p>现在多模态学习领域有两个主流的模型结构：</p>
<ol>
<li><p>双塔结构：图像有一个模型，文本有一个模型，双塔完全分开，互不干扰，模态之间的交互用非常简单的Cosine Similarity来完成，比如CLIP Align这种dual-encoder。<br> 结构优点：对检索任务极其有效，因为它可以提前把特征都抽好，接下来直接算Similarity矩阵乘法就可以，极其适合大规模的图像文本的检索，非常具有商业价值。<br> 结构的缺点：只计算Cosine Similarity无法做多模态之间深度融合，难一些的任务性能差。</p>
</li>
<li><p>单塔结构，Fusion Encoder的方式，先把图像和文本分开处理一下，但是当做模态交互的时候，用一个Transformer Encoder做模态之间的交互。<br> 结构优点：这个结构弥补双塔模式的缺陷，在VR、VE、VQA任务上效果特别好。<br> 结构缺点：当做检索任务的时候，因为只有一个模型，所以必须同时做推理，当图像文本对特别多，数据集特别大的时候，需要将所有all possible图像文本对全都要同时编码，然后计算Similarity Score，才能做检索，所以推理时间就会非常慢。</p>
</li>
</ol>
<p>本文的研究动机是：</p>
<ol start="3">
<li><p>结合上面两种结构的优点，设计模型网络，在Feed Forward FC层，每个模态就会对应自己不同的Expert，就是视觉有视觉的Vision Expert，Language有Language的Expert，Multi-model有Multi-model对应的Expert，这样在训练的时候输入哪个模态的数据，就训练哪个模态的Expert。推理的时候能根据现在输入的数据决定使用什么模型结构。</p>
</li>
<li><p>多模态的训练数据集不够多，但是在单模态里，就是视觉或者NLP里，可用的数据很多，基于这个研究动机，VLMo的作者提出了stagewise pre-training strategy，就是分阶段去训练，先把vision expert在视觉数据集这边训好，再把language expert在language的数据集上训好，这个时候模型本身的参数非常好的被初始化了，再到多模态的数据上做pre-training，效果就会好很多。</p>
</li>
</ol>
<h2 id="本文贡献1：MOME模型"><a href="#本文贡献1：MOME模型" class="headerlink" title="本文贡献1：MOME模型"></a>本文贡献1：MOME模型</h2><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161108321.png" alt="image.png"></p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>VLMo论文的模型核心是一个transformer encoder的结构，但是它在每个transformer block里面做了一些改动，也就是他们提出的MoME transformer(mixture-of-modality-expert)。</p>
<p>标准的transformer block结构：一个标准的transformer block里先有一个Layer Norm，接下来是一个MSA multi-head self-attention，然后是Layer Norm，然后是一个FFN feed-forward network，最后有一个residual。</p>
<p>本文的transformer block结构：本文结构中layer norm、MSA、layer norm、residual connection这些都是一样的，唯一一个不一样的就是<strong>feed-forward network</strong>，它不是一个feed-forward network，而是针对不同的输入、不同的modality，有vision FFN、language FFN和vision language FFN，也就是switching modality expert，从而构建出MoME transformer block，也就是VLMo整个的模型结构。</p>
<p>注意：虽然的FFN层没有share weights，各自modality有各自的FFN层，但之前的self-attention层是share weights，也就是不论图像、文本还是图像文本信号，输入任何的token sequence，self-attention的model weights都是一样的，这个也是transformer架构的优势，或者说多模态学习接下来的趋势，目前有很多工作证明，同样的self-attention weights可以用来做不同的图像文本音频视频任务，不需要重新去训练自注意力参数。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ol>
<li><p>Image Text Contrastive <strong>ITC</strong></p>
</li>
<li><p>Image Text Matching <strong>ITM</strong>：一个二分类任务</p>
</li>
<li><p>Mask Language Modeling <strong>MLM</strong>：预测这些被mask掉的单词</p>
</li>
</ol>
<p>本文从ALBEF里借鉴用这三个loss训练模型以及hard negative mining的思想。例如当计算ITC contrastive loss的时候，VLMo类似CLIP模型，图像端只有图像，输入ViT，FFN都用的Vision FFN，如果用Vision Transformer Base，就是12层的一个transformer，文本端就是文本的token单独输入language model，后面用的是language expert是一个12层的BERT base，当计算ITM或者mask language modeling 的时候，模型变成fusion encoder的形式，图像和文本的一起输入multi-head self-attention（<strong>这里的self-attention与之前的self-attention和后面的self-attention都是share weights，都是一样的，不论哪个modality，自注意力的参数都不变，都是共享的</strong>）。</p>
<p>在前面的L-F的transformer block里，模型对视觉和language信号分别做训练，也就是分别用Vision Expert（VE）和Language Expert(LE)，只有在最后的F层，才用Vision Language Expert训练。例如，如果用的是transformer base模型，前十层做VE和LE，后面F就是2，也就是后面只有两层transformer block做模态之间的融合。</p>
<h3 id="结构优点"><a href="#结构优点" class="headerlink" title="结构优点"></a>结构优点</h3><p>VLMo的优点是灵活，训练的时候可以通过各种modality选择训练哪个modality expert；推理的时候可以选择使用哪些模型参数。<strong>如果做检索任务，就像CLIP一样就用视觉和文本两个模型就可以，如果做Vision Language分类任务VR、VE、VQA，就用下面的Fusion Encoder模式。</strong></p>
<h3 id="结构缺点"><a href="#结构缺点" class="headerlink" title="结构缺点"></a>结构缺点</h3><p>代价是VLMo里至少做了两次甚至三次的前向过程，VLMo的base模型在4 million的setting下训练，用64张V100的卡需要训练两天，训练量与ViLT同级别，比ALBEF慢。</p>
<h2 id="本文贡献2：分阶段的训练策略"><a href="#本文贡献2：分阶段的训练策略" class="headerlink" title="本文贡献2：分阶段的训练策略"></a>本文贡献2：分阶段的训练策略</h2><p>因为作者希望利用Unimodality里大量的图片文本去做预训练进而得到更好的模型初始化，所以先做Vision Pre-training，然后做Language Pre-training，最后做Vision Language Pre-training。</p>
<p>具体地，做Vision Pre-training的时候，是Unsupervised，用了他们自己的BEIT中的Mask Image Modeling，做Language Modeling的时候，使用Mask Language Modeling，Vision Language Pre-training用的上面三个目标函数。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161118723.png" alt="image.png"></p>
<p>图中蓝色的虚线代表是Frozen FFN，橘黄色的虚线代表Frozen Self-attention。</p>
<ol>
<li><p>第一阶段做Vision Pre-training的时候，因为刚训练完，不需要freeze，所有参数随机初始化，所以12层的Transformer Block，包括前面自注意力和后面的Vision Expert都打开训练。</p>
</li>
<li><p>第二阶段做文本预训练的时候，Vision Expert被冻住，因为训练文本数据不需要训练Vision Expert，所以Vision Expert的FFN层参数就固定下来了，该阶段去训练Language Expert，并且Self-Attention也冻住了，因此<strong>在视觉数据上训练好一个模型，在视觉Token Sequence上训练好一个自注意力模型，可以直接拿来对文本数据进行建模，不需要fine-tune</strong>。但这个过程反过来不行，就是先在Language上去训练然后再在Vision上冻住去做结果不太好，但是先用Vision训练然后再在Text上直接去用Self-Attention，很多工作都证明是有效的。</p>
</li>
<li><p>第三阶段做多模态，所有训练参数都打开训练，包括Self-Attention，后面三个Expert都打开做fine-tune。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>ViLT</title>
    <url>/2025/01/16/ViLT/</url>
    <content><![CDATA[<p><strong>ViLT：最简单的多模态Transformer</strong></p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>作者根据下面两点将当前 VLP(Vision-and-Language Pre-training) 模型分成了四大类，并且总结了不同类别方法的特点：</p>
<ul>
<li><p>图像和文本的表达力度是不是平衡，如参数量和计算</p>
</li>
<li><p>图像和文本两个模态的信息是怎么融合的</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160713958.png" alt="image.png"></p>
<span id="more"></span>

<p><strong>VSE、VSE++和SCAN属于(a)类型。</strong>对图像和文本独立使用encoder，图像的更重，文本的更轻，使用简单的<a href="https://zhida.zhihu.com/search?content_id=170292842&content_type=Article&match_order=1&q=%E7%82%B9%E7%A7%AF&zhida_source=entity">点积</a>或者浅层attention层来表示两种模态特征的相似性。</p>
<p><strong>CLIP属于(b)类型。</strong>每个模态单独使用重的transformer encoder，使用池化后的<a href="https://zhida.zhihu.com/search?content_id=170292842&content_type=Article&match_order=1&q=%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81&zhida_source=entity">图像特征</a>点积计算特征相似性。</p>
<p><strong>ViLBERT、UNTER和</strong><a href="https://zhida.zhihu.com/search?content_id=170292842&content_type=Article&match_order=1&q=Pixel-BERT&zhida_source=entity"><strong>Pixel-BERT</strong></a><strong>属于(c)类型。</strong>这些方法使用深层transformer进行交互作用，但是由于VE仍然使用重的<a href="https://zhida.zhihu.com/search?content_id=170292842&content_type=Article&match_order=1&q=%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&zhida_source=entity">卷积网络</a>进行特征抽取，导致计算量依然很大。</p>
<p><strong>作者提出的ViLT属于(d)类型。</strong>ViLT是首个将VE设计的如TE一样轻量的方法，该方法的主要计算量都集中在<a href="https://zhida.zhihu.com/search?content_id=170292842&content_type=Article&match_order=1&q=%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92&zhida_source=entity">模态交互</a>上。</p>
<p>模态融合很重要，所以要看看当前的模态融合是怎么做的：</p>
<ul>
<li><p><strong>single-stream：</strong>把两个序列 concat 成一个序列，然后让 transformer 自己去学习怎么融合</p>
</li>
<li><p><strong>dual-stream：</strong>先各自对各自的输入做处理，挖掘单独模态中包含的信息，然后再融合</p>
</li>
</ul>
<p>两种融合方法基本效果差不多，dual-stream 有时候会更好一点，但 dual-stream 引入了更多的参数，ViLT 作者还是使用了 single-stream 更轻量的方法</p>
<p>多模态融合之前，特征怎么提取：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160726329.png" alt="image.png"></p>
<p><strong>文本端：</strong>使用预训练好的 BERT</p>
<p><strong>视觉端：</strong></p>
<ul>
<li><p>之前的方法是先将输入图像通过 backbone 来提取特征，然后使用 RPN 网络来抽取一些 ROI，做一次 NMS 降低区域的数量，然后经过 head 得到 bbox，整个过程非常贵</p>
</li>
<li><p>还有一些方法使用 Grid Feature，就是直接用抽出的特征，拉平后直接用，但效果不好</p>
</li>
<li><p>所以 ViLT 借鉴 ViT 的 patch embedding layer 来抽取特征，又快又好</p>
</li>
</ul>
<h1 id="ViLT-方法"><a href="#ViLT-方法" class="headerlink" title="ViLT 方法"></a>ViLT 方法</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116160811143.png" alt="image.png"></p>
<p>ViLT 是一个 single-stream 的结构，所以只有一个模型：</p>
<ul>
<li><p>文本输入：文本先输入 BERT tokenizer，得到 word embedding，假设文本的长度是 L，H 就是 embedding 维度，一般 base 模型就是 768，就是 Lx H 大小</p>
</li>
<li><p>图像输入：把图像打成 patch，每个 patch 进行 patch embedding，得到一系列的 token 编码（紫色），假设图像 token 长度为 N 维度也是 H 那么就 NxH 的特征</p>
</li>
<li><p>*：cls token，绿色的是文本的，紫色的是图像的</p>
</li>
<li><p>灰色：表示模态，0 表示文本模态，1 表示图像模态，需要 modal-type embedding 的原因在于，对于 single stream 的方法来说，是会把图像和文本的输入拼接在一起的，如果不告诉模型哪些是文本或哪些是图像的话，是不利于学习的，如果告诉了后，模型就可能互相学习文本和图像之间的关系，更有利于找到潜在的关系</p>
</li>
<li><p>输入输出序列长度：每个 token embedding 输入 transformer encoder 之前呢，都是由三部分组成的：灰色+绿色+淡绿色的，是加到一起的，而不是 concat，然后所有相加后的特征会被 concat 起来，送入 transformer encoder，所以输入长度就是 1+L+1+N，整个输入的特征就是（N+L+2)xH，输出的序列也是（N+L+2)xH</p>
</li>
</ul>
<p>预训练的两个 Loss：</p>
<ul>
<li><p>Image-text matching loss：</p>
<ul>
<li><p>该 loss 是用于衡量文本和图像之间的距离，是人为设计的任务，也就是二分类的任务，判断匹配与否，真正有用的位置是 Pooler 位置，在 Pooler 之前呢，这个位置输出的特征是 1xH，经过 Pooler 相当于权重矩阵，得到 HxH，经过 FC 得到 1xH 的输出，做二分类。</p>
</li>
<li><p>ViLT 中还使用了 word patch alignment loss，使用最优传输理论来计算分布之间的距离</p>
</li>
</ul>
</li>
<li><p>Masked language modeling loss：</p>
<ul>
<li>针对文本的目标函数，也就是 NLP 那边常见的完形填空，也就是在输入文本的时候，随机 MASK 掉一个单词，然后在输出的时候重建出来这个 mask</li>
</ul>
</li>
</ul>
<p>文本模型中使用的小技巧：whole word masking：</p>
<ul>
<li><p>作者使用将整个单词都 mask 掉的方法，能进一步的加强文本和图像之间的关系的学习</p>
</li>
<li><p>当把单词抹掉之后，句子重建就很困难，那就只能从图像中来拿取信息，所以就能迫使模型来建立图像和文本的关系</p>
</li>
</ul>
<p>图像模型中使用的小技巧：</p>
<ul>
<li><p>前面说过数据增强在图像中很有用，但多模态中不太能用好数据增强</p>
</li>
<li><p>本文作者做了改动，也就是不用 color 和 cutout，就能尽可能的保证图像和文本保持原有的匹配关系</p>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><p>﻿<a href="https://zhuanlan.zhihu.com/p/369733979">https://zhuanlan.zhihu.com/p/369733979</a>﻿</p>
</li>
<li><p>﻿<a href="https://blog.csdn.net/jiaoyangwm/article/details/132247400">【多模态】25、ViLT | 轻量级多模态预训练模型（ICML2021）_vilt预训练模型-CSDN博客</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态总结</title>
    <url>/2025/01/16/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116161802558.png" alt="image.png"></p>
<p><strong>VIT</strong>: 多模态学习之前, 都是Oscar或者Uniter这些工作, 他们的缺陷都是因为里面用了一个Object Detection的模型去做视觉特征的抽取, 这种方法就太慢而且太贵了</p>
<span id="more"></span>
<p><strong>Vilt</strong>: 在Vision Transformer出来之后, Vilt的作者就想到在多模态任务中可以把Vision这边用一个Vision Transformer去代替(一个Embedding层就足够了), 这样大大的简化了模型结构</p>
<p><strong>ALBEF</strong>: ViLT和Clip都是ICML21的工作, ALBEF作者发现, Clip比较高效, 适合做Image Text Retrieval, 原始的这些方法(Oscar或者Uniter这些工作)因为Modality Fusion做得好, 所以多模态任务非常强, 而ViLT的结构比较简单, 最后综合三家的长处推出了ALBEF这么一个Fusion Encoder的模式, 取得了不错的结果. 因为ALBEF也release了代码, 而且结果不错, 模型也比较简单, 所以在它之上又延伸出来了很多工作,</p>
<p><strong>Coca</strong>: SimVLM之前是用Encoder Decoder去做多模态的, SimVLM的作者在ALBEF的基础上就用Contrast和Captioning的两个Loss去训练出来了非常强大的模型</p>
<p><strong>VLMO</strong>: 有了ALBEF之后, 微软的研究者推出了VLMO, 用共享参数的方式推出一个统一的做多模态的框架,</p>
<p><strong>Blip</strong>: 基于这种参数共享的思想, ALBEF的作者又推出了Blip模型, 能做非常好的Captioning的功能, 而且它的Caption Filter模型, 也非常的好用, 能够像一个普适的工具一样用到各种各样的情形中去</p>
<p><strong>VIT</strong>: Vision Transformer在文章中也做了用Mask Data Modeling的方式去做Self-Supplied Learning, 但是当时的效果不是很好</p>
<p><strong>BEIT</strong>: 顺着Bert的思想, 微软的研究者就提出BEIT</p>
<p><strong>BEITv2</strong>: 在BEIT的基础上, 很快又推出了BEITv2, 但是主要是做视觉Task的, 并不是做多模态的</p>
<p><strong>Vision Language BEIT:</strong> 因为BEIT可以在视觉上做Mask Modeling, Bert可以在文本上做Mask Modeling, 作者就想视觉和文本是不是可以合在一起, 所以又推出了Vision Language BEIT</p>
<p><strong>BEITv3</strong>: 在一系列的实验经验的积累之下, 作者最后把VLMO, VL-BEIT和BEITv2, 三个工作合起来, 推出了多模态的BEITv3, 大幅超过了之前CoCa, Blip在单模态和多模态上的各种表现</p>
<p><strong>VIT</strong>: 对于Mask的Data Modeling, 可以Mask and Predict不同的东西. 比如<strong>BEIT</strong>就是Predict <strong>Patch, MAE</strong>是Mask Predict <strong>Pixel,</strong> 当然不论是恢复Patch还是恢复Pixel, 其实Vision Transformer这篇paper, 原来都已经做了, 但是效果都不是很好, BEIT和MAE都把效果推到一个非常高的高度</p>
<p><strong>MAE</strong>: Mask Auto Encoder是Mask Predict <strong>Pixel,</strong> MAE有一个非常好的特性就是在视觉那端把大量的Patch全都给Mask掉之后, 只把那些没有Mask掉过的Patch扔给了Vision Transformer去学习, 这样就大大减少了计算量</p>
<p><strong>Flip</strong>: Fast Language Image Prediction把MAE的有用的特性用到Clip的结构里, 模型就是Clip没有任何的改变, 只不过是在视觉这端跟MAE一样都是只用那些没有Mask的Token, 把那些Mask的Token就扔掉了, 这样无形之中就把Sequence Length降低了很多, 所以训练就快了, 也就是他说的Fast Language Image Prediction</p>
<p><strong>一些建议</strong>: 有的代码库是封装的比较好，所以对于好的论文和代码库，看是不够的，必须得上手跑。从main.py里的第一行开始，一行一行往下跑，遇到function就跳过去，再一行一行跑，对照着paper和implementation details去理解。一般一个细分领域，好的代码库跑3<del>5个就足够了解了，然后好的论文可能5</del>20篇。再多的论文基本就是看看有什么区别和联系，找点insight就可以了。</p>
]]></content>
      <categories>
        <category>大模型</category>
        <category>论文精读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>论文精读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
</search>
