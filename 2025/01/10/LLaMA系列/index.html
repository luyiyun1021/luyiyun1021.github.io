<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"luyiyun1021.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="article">
<meta property="og:title" content="LLaMA系列">
<meta property="og:url" content="https://luyiyun1021.github.io/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/index.html">
<meta property="og:site_name" content="Ronny Lu&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165618253.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165628374.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165639343.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165646305.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165655594.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165721292.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165751368.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164739603.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165800937.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165809840.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165820235.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165828163.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165836993.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165851458.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165901170.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165910905.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165918431.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165928491.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165937525.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165943584.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165951462.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170001367.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170012232.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170018823.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170024662.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170032874.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170040017.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170046580.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170053043.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170058416.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170105884.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170116630.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164815049.png">
<meta property="og:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170126831.png">
<meta property="article:published_time" content="2025-01-10T09:04:26.000Z">
<meta property="article:modified_time" content="2025-01-16T08:48:23.782Z">
<meta property="article:author" content="Ronny Lu">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="论文精读">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165618253.png">


<link rel="canonical" href="https://luyiyun1021.github.io/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://luyiyun1021.github.io/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/","path":"2025/01/10/LLaMA系列/","title":"LLaMA系列"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LLaMA系列 | Ronny Lu's blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Ronny Lu's blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">1.</span> <span class="nav-text">前置知识</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98"><span class="nav-number">2.</span> <span class="nav-text">主题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer-vs-LLaMA"><span class="nav-number">3.</span> <span class="nav-text">Transformer vs LLaMA</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LLaMA1-LLaMA2%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F"><span class="nav-number">4.</span> <span class="nav-text">LLaMA1, LLaMA2模型参数量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">5.</span> <span class="nav-text">模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-number">5.0.1.</span> <span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalization"><span class="nav-number">5.0.2.</span> <span class="nav-text">Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5"><span class="nav-number">5.0.2.1.</span> <span class="nav-text">回顾神经网络的数学概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BETransformer%E4%B8%AD%E7%9A%84Layer-Normalization"><span class="nav-number">5.0.2.2.</span> <span class="nav-text">回顾Transformer中的Layer Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaMA%E4%BD%BF%E7%94%A8%E7%9A%84normalization-Root-Mean-Square-Normalization"><span class="nav-number">5.0.2.3.</span> <span class="nav-text">LLaMA使用的normalization: Root Mean Square Normalization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-number">5.0.3.</span> <span class="nav-text">Positional Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E9%A1%BETransformer%E4%B8%AD%E7%9A%84Positional-Encoding"><span class="nav-number">5.0.3.1.</span> <span class="nav-text">回顾Transformer中的Positional Encoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Absolute-Positional-Encodings-v-s-Relative-Positional-Encodings"><span class="nav-number">5.0.3.2.</span> <span class="nav-text">Absolute Positional Encodings v.s. Relative Positional Encodings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rotary-Position-Embeddings"><span class="nav-number">5.0.3.3.</span> <span class="nav-text">Rotary Position Embeddings</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Attention"><span class="nav-number">5.0.4.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KV-cache"><span class="nav-number">5.0.5.</span> <span class="nav-text">KV cache</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Next-Token-Prediction-Task"><span class="nav-number">5.0.5.1.</span> <span class="nav-text">Next Token Prediction Task</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation-behind-KV-cache"><span class="nav-number">5.0.5.2.</span> <span class="nav-text">Motivation behind KV cache</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KV-cache-1"><span class="nav-number">5.0.5.3.</span> <span class="nav-text">KV cache</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Grouped-Multi-Query-Attention"><span class="nav-number">5.0.6.</span> <span class="nav-text">Grouped Multi-Query Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Multi-Query-Attention%EF%BC%88MQA%EF%BC%89"><span class="nav-number">5.0.6.0.1.</span> <span class="nav-text">Multi Query Attention（MQA）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Grouped-Multi-Query-Attention-GMQA"><span class="nav-number">5.0.6.0.2.</span> <span class="nav-text">Grouped Multi-Query Attention(GMQA)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SwiGLU-Function"><span class="nav-number">5.0.7.</span> <span class="nav-text">SwiGLU Function</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ronny Lu"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Ronny Lu</p>
  <div class="site-description" itemprop="description">Tech notes on LLM, LLM Infra, C++</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://luyiyun1021.github.io/2025/01/10/LLaMA%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Ronny Lu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ronny Lu's blog">
      <meta itemprop="description" content="Tech notes on LLM, LLM Infra, C++">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LLaMA系列 | Ronny Lu's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLaMA系列
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-10 17:04:26" itemprop="dateCreated datePublished" datetime="2025-01-10T17:04:26+08:00">2025-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-16 16:48:23" itemprop="dateModified" datetime="2025-01-16T16:48:23+08:00">2025-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">大模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文精读</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165618253.png" alt="image.png"></p>
<span id="more"></span>

<h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><ul>
<li><p><strong>Transformer模型的结构以及注意力机制的工作原理</strong>。</p>
</li>
<li><p><strong>Transformer模型是如何训练和推理的</strong>。</p>
</li>
<li><p><strong>线性代数</strong>：矩阵乘法，点积。</p>
</li>
<li><p><strong>复数</strong>：欧拉公式（非必需，了解更好）。</p>
</li>
</ul>
<h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><ul>
<li><p><strong>原始Transformer与LLaMA的架构差异</strong>。</p>
</li>
<li><p><strong>RMS Normalization</strong></p>
</li>
<li><p><strong>Rotary Positional Embeddings</strong></p>
</li>
<li><p><strong>KV-Cache</strong></p>
</li>
<li><p><strong>Multi-Query Attention</strong></p>
</li>
<li><p><strong>Grouped Multi-Query Attention</strong></p>
</li>
<li><p><strong>SwiGLU Activation Function</strong></p>
</li>
</ul>
<h1 id="Transformer-vs-LLaMA"><a href="#Transformer-vs-LLaMA" class="headerlink" title="Transformer vs LLaMA"></a>Transformer vs LLaMA</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165628374.png" alt="image.png"></p>
<ul>
<li><p><strong>llama只有encoder</strong>, 因为它基于next prediction token task训练, 只需要自注意力机制来预测下一个 token</p>
</li>
<li><p>**所有的归一化被移到了block之前(**Attention, Feed Forward)</p>
</li>
<li><p>llama不再使用Transformer的Postional Encoding, 而是<strong>使用Rotary Positional Embeddings, 并且只应用在Query, Key上</strong></p>
</li>
<li><p><strong>Attention层增加了KV cache, Grouped Multi-Query Attention</strong></p>
</li>
<li><p><strong>Feed Forward block中的激活函数从ReLU变为SwiGLU</strong></p>
</li>
</ul>
<h1 id="LLaMA1-LLaMA2模型参数量"><a href="#LLaMA1-LLaMA2模型参数量" class="headerlink" title="LLaMA1, LLaMA2模型参数量"></a>LLaMA1, LLaMA2模型参数量</h1><p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165639343.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165646305.png" alt="image.png"></p>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>句子通过分词器(Tokenizer)变成token(字典中的位置), token通过embedding层被映射到一个向量中(Transformer为512维, 而llama为4096维), 用来表示这个token的特征, 这些embedding是可学习的,是模型参数的一部分</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165655594.png" alt="image.png"></p>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><h4 id="回顾神经网络的数学概念"><a href="#回顾神经网络的数学概念" class="headerlink" title="回顾神经网络的数学概念"></a><strong>回顾神经网络的数学概念</strong></h4><p>假设我们有一个线性层 nn.Linear(in_features&#x3D;3, out_features&#x3D;5, bias&#x3D;True), 输入为(10, 3). 这意味着我们有10个item, 每个item有3个feature.</p>
<p>对于这个带有bias的线性层, 我们有两个权重矩阵, 一个是大小为(5, 3)的W, 一个是大小为(1, 5)的b</p>
<p>如下图所示, item1的第一个输出feature的计算方法为$a_1 * w_1 + a_2 * w_2 + a_3 * w_3 + b_1$﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165721292.png"></p>
<p>我们有以下结论:</p>
<ul>
<li><p>神经元对数据项的输出取决于输入数据项的特征（以及神经元的参数）。</p>
</li>
<li><p>我们可以将一个神经元的输入视为前一个线性层的输出。</p>
</li>
<li><p><strong>如果前一层在经过梯度下降更新权重后，其输出发生了剧烈变化，那么下一层的输入也会发生剧烈变化，因此在下一步梯度下降时，下一层的权重也会被迫进行大幅调整。</strong></p>
</li>
<li><p><strong>这种神经网络内部节点（神经元）的分布发生变化的现象称为Internal Covariate Shift(内部协变量偏移)。我们希望避免这种情况，因为它会使网络训练变得更慢，迫使神经元因为前一层输出的剧烈变化而大幅调整其权重。</strong></p>
</li>
</ul>
<h4 id="回顾Transformer中的Layer-Normalization"><a href="#回顾Transformer中的Layer-Normalization" class="headerlink" title="回顾Transformer中的Layer Normalization"></a><strong>回顾Transformer中的Layer Normalization</strong></h4><ul>
<li><p>对于每个item,我们独立统计两个统计量,均值和方差</p>
</li>
<li><p>每个item都会用其归一化后的值进行更新，使其变成均值为0、方差为1的正态分布。</p>
</li>
<li><p>参数 gamma 和 beta 是可学习的参数，它们允许模型根据损失函数的需求“放大”每个特征的尺度或对特征进行平移。</p>
</li>
<li><p>和传统的batch norm相比, layer norm按行(即item)进行计算统计, 而batch norm按列(即feature)计算统计, 好处是可以更好的处理语言模型中,每次input的sequence length不同的情况</p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165751368.png" alt="image.png"></p>
<h4 id="LLaMA使用的normalization-Root-Mean-Square-Normalization"><a href="#LLaMA使用的normalization-Root-Mean-Square-Normalization" class="headerlink" title="LLaMA使用的normalization: Root Mean Square Normalization"></a><strong>LLaMA使用的normalization: Root Mean Square Normalization</strong></h4><ul>
<li><p>RMS的作者认为, <strong>导致Layer norm起作用的更多的是由于re-scaling(除以方差), 而不是re-centering(减去均值)</strong>. 因此他们想要找到一个不依赖均值的统计量.</p>
</li>
<li><p><strong>RMS提出使用均方根归一化, 即每个item除以均方根,在乘以一个可学习的参数gamma, 如下图所示</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164739603.png" alt="image.png"></p>
<ul>
<li><strong>RMS只需要计算一个统计量, 因此计算量更少, 同时在实践中表现很好</strong></li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>﻿<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1xR1RY9ECm">位置编码有什么用？简单讲解位置编码原理 + 源码解读（绝对 &#x2F; 相对 &#x2F; RoPE）_哔哩哔哩_bilibili</a>﻿</p>
<h4 id="回顾Transformer中的Positional-Encoding"><a href="#回顾Transformer中的Positional-Encoding" class="headerlink" title="回顾Transformer中的Positional Encoding"></a><strong>回顾Transformer中的Positional Encoding</strong></h4><ul>
<li><p><strong>我们希望每个词都能携带一些关于其在句子中位置的信息。</strong></p>
</li>
<li><p>我们希望模型将出现在彼此相邻的词视为“接近”，而将距离较远的词视为“远离”。</p>
</li>
<li><p>通过下图的公式, 计算出句子中每个词, 每一维embedding所对应的Positional Encoding,</p>
</li>
<li><p><strong>我们只需要计算一次位置编码，然后在训练或推理过程中都可以重复使用它</strong></p>
</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165800937.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165809840.png" alt="image.png"></p>
<h4 id="Absolute-Positional-Encodings-v-s-Relative-Positional-Encodings"><a href="#Absolute-Positional-Encodings-v-s-Relative-Positional-Encodings" class="headerlink" title="Absolute Positional Encodings v.s. Relative Positional Encodings"></a>Absolute Positional Encodings v.s. Relative Positional Encodings</h4><ul>
<li><p><strong>绝对位置编码</strong>是固定的向量，它们被添加到一个token的embedding中，用来表示其在句子中的绝对位置。因此，它<strong>一次只处理一个token</strong>。你可以将其类比为地图上的（纬度，经度）对：地球上的每个点都有一个唯一的坐标对。绝对位置编码的优势在于它不依赖于序列中的其他元素，可以独立地表示每个位置的信息。</p>
</li>
<li><p><strong>相对位置编码</strong>则一次处理两个token，并在我们计算注意力时起作用。因为注意力机制捕捉的是两个词之间相关性的“强度”，相对位置编码则告诉注意力机制这两个词之间的距离。因此，<strong>给定两个token，我们创建一个向量来表示它们之间的距离。</strong>相对于绝对位置编码，相对位置编码更关注序列中位置之间的相对顺序和距离，它可以更好地处理长序列中的位置信息, 也具有更好的外推性。</p>
</li>
</ul>
<blockquote>
<p><strong>备注：什么是大模型外推性？（Length Extrapolation）</strong></p>
<p><strong>外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。</strong>例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。</p>
</blockquote>
<ul>
<li>如下图所示, 相对位置编码没有完整建模整个序列的位置信息，而是在算当前位置的Attention的时候，考虑了当前位置和被Attention位置之间的相对距离（这一操作可以体现<strong>在计算Attention过程中，在计算中引入一个相对位置向量进行学习</strong>)</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165820235.png" alt="image.png"></p>
<h4 id="Rotary-Position-Embeddings"><a href="#Rotary-Position-Embeddings" class="headerlink" title="Rotary Position Embeddings"></a>Rotary Position Embeddings</h4><p><strong>旋转位置编码的出发点: 把绝对位置编码和相对位置编码相结合. 具体来说, 我们希望给每个位置分配一个绝对位置编码, 然后不同位置的qk内积又直接包含位置差的信息(注意是“直接”，显式地包含).</strong></p>
<p><strong>也就是我们需要找到满足这个公式</strong>$\langle f_q(x_m, m), f_k(x_n, n) \rangle &#x3D; g(x_m, x_n, m - n)$<strong>的函数f, 同时希望编码本身有一些其他的性质(如如远程衰减、易于外推等)</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165828163.png" alt="image.png"></p>
<p><strong>回顾transformer用到的三角式位置编码, 虽然是绝对位置编码, 但是旋转矩阵的引入也让它包含了一定的相对位置信息</strong></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165836993.png" alt="image.png"></p>
<p><strong>先研究2D情况</strong></p>
<p>利用三角函数运算, 可以将m-n的旋转矩阵分解为m和n的矩阵乘积, 也就实现了相对位置到绝对位置的转变</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165851458.png" alt="image.png"></p>
<p>如果左右乘上相应位置的q, k, 就得到了最终的内积形式, 这正是我们追求的目标形式$\langle f_q(x_m, m), f_k(x_n, n) \rangle &#x3D; g(x_m, x_n, m - n)$﻿</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165901170.png" alt="image.png"></p>
<p><strong>扩展到全维度</strong></p>
<p>总结起来就是给词向量左乘一个旋转矩阵</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165910905.png" alt="image.png"></p>
<p><strong>优化</strong></p>
<p>由于$R_m$比较稀疏, 直接用矩阵乘法实现效率较低, 论文中提供了一种高效的实现方法, 用矩阵元素相乘替代矩阵乘法, 最后奇偶维度错位相加.</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165918431.png" alt="image.png"></p>
<p><strong>总结</strong></p>
<ul>
<li><p>RoPE就像绝对位置编码一样，我们计算一次，然后可以对我们将要训练模型的所有句子重复使用。</p>
</li>
<li><p>使用RoPE编码的两个token之间的关系“强度”会随着它们之间距离的增加而在数值上变小, 称为long-term decay</p>
</li>
<li><p>RoPE只应用在Q, K上</p>
</li>
</ul>
<blockquote>
<p>在自注意力机制中，<strong>Query</strong> 和 <strong>Key</strong> 的内积决定了不同位置的词元之间的相关性（或称为“注意力权重”）。注意力机制的目标是根据这些权重，确定每个词元在多大程度上应该关注序列中的其他词元。因此，位置编码需要引入到 Query 和 Key 中，以影响注意力计算时如何考虑不同词元之间的相对位置。</p>
<p><strong>Value</strong> 并不直接参与注意力权重的计算，它只是在计算出注意力权重之后，根据这些权重聚合输入内容。因此，Value 中不需要加入位置编码。在自注意力机制中，Attention 输出是通过加权求和值得出的，而 Value 是被这些权重加权的部分。因为位置编码的目的是为了告诉模型不同位置之间的关系，而不是影响如何组合内容，所以没有必要将位置编码应用于 Value 上。</p>
</blockquote>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p><strong>自注意力机制允许模型将词与其他词关联起来。</strong></p>
<p>假设我们考虑一个序列长度为6, embedding维度为512的输入, 由于是自注意力, QKV都是相同的</p>
<p>﻿$softmax(\frac{QK^T}{\sqrt{d_k}})$捕捉了两个token之间的相关性</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165928491.png" alt="image.png"></p>
<p>上一步的结果再和V做矩阵乘, 得到与输入维度相同的输出矩阵, <strong>每行(即经过计算后的的新的embedding)不仅捕捉了词语的含义（由嵌入表示）或在句子中的位置（由位置编码表示），还捕捉了每个词与其他词的相互作用</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165937525.png" alt="image.png"></p>
<p>给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165943584.png" alt="image.png"></p>
<p><strong>Multi-Head Attention</strong></p>
<p>沿着d模型维度将QKV分割成较小的矩阵，并在这些较小的矩阵之间计算注意力. 所以每个头都在观察完整的句子，但是是每个embedding的不同方面。这样做的原因是<strong>我们希望每个头观察同一个词的不同方面</strong>。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110165951462.png" alt="image.png"></p>
<p>再给出一张非常直观的图</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170001367.png" alt="image.png"></p>
<h3 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV cache"></a>KV cache</h3><h4 id="Next-Token-Prediction-Task"><a href="#Next-Token-Prediction-Task" class="headerlink" title="Next Token Prediction Task"></a>Next Token Prediction Task</h4><p>首先来了解一下LLama的训练（下词预测任务）：seq2seq的生成，但迭代T次，seq_len逐渐增加。</p>
<p>即输入序列的第一个token将映射到输出序列的第一个token, [SOS] -&gt; Love, 第二次迭代中将用[SOS] Love预测that, 以此类推</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170012232.png" alt="image.png"></p>
<h4 id="Motivation-behind-KV-cache"><a href="#Motivation-behind-KV-cache" class="headerlink" title="Motivation behind KV cache"></a>Motivation behind KV cache</h4><p>在推理的每一步，我们只关心模型输出的最后一个词元，因为我们已经有了之前的词元。然而，模型需要访问所有之前的词元来决定输出哪个词元，因为它们构成了模型的上下文（或称为“提示”）。<br>有没有办法让模型在推理时对已经看到的词元进行更少的计算呢？</p>
<h4 id="KV-cache-1"><a href="#KV-cache-1" class="headerlink" title="KV cache"></a>KV cache</h4><p>下句预测时的Self-Attention：</p>
<ul>
<li>timpstep&#x3D;1时seq_len&#x3D;1，给[SOS]时，预测Love；</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170018823.png" alt="image.png"></p>
<ul>
<li>timpstep&#x3D;2时<code>seq_len=2</code>，给[SOS] 和 Love时，预测that</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170024662.png" alt="image.png"></p>
<ul>
<li>timpstep&#x3D;4时<code>seq_len=4</code>，给[SOS] 和 Love 和 can 和 quickly时，预测seize…</li>
</ul>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170032874.png" alt="image.png"></p>
<p>再来分析一下，<strong>每次个timestep的self-attention中我们到底需要哪些</strong>：因为我们只关注<strong>最后一个token的</strong><code>**attention_output**</code>，如下图timestep&#x3D;4，我们只需要attention_output的第4个token。</p>
<p>因此我们只需要<strong>Q的最后一个token</strong>和<strong>K的所有token</strong>相乘，得到最后一个token的<code>attention_score</code>，然后用<strong>V的所有token</strong>再与<code>attention_score</code>点积(相乘求和)，得到最后一个token的<code>attention_output</code>：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170040017.png" alt="image.png"></p>
<p>由上分析可知，<strong>每个timestep，我们的Q只需要新增的那个token即可，而K和V要缓存之前timestep的token，保证token是全的</strong>。<strong>每次计算出来的attention_output就是那个新增的token的attention。</strong> 这样就可以节省大量计算开销。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170046580.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170053043.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170058416.png" alt="image.png"></p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170105884.png" alt="image.png"></p>
<p><strong>参数量的计算方法:</strong> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a>﻿</p>
<h3 id="Grouped-Multi-Query-Attention"><a href="#Grouped-Multi-Query-Attention" class="headerlink" title="Grouped Multi-Query Attention"></a>Grouped Multi-Query Attention</h3><p>回顾原始的<strong>多头注意力Multi-Head Attention</strong>：时间开销的瓶颈在于<strong>矩阵的运算</strong><code>**matrix computation**</code>。</p>
<ul>
<li><p>多头注意力机制如同在原始论文《Attention is all you need》中所展示的。</p>
</li>
<li><p>通过设定 $m &#x3D; n$（查询的序列长度 &#x3D; 键和值的序列长度）。</p>
</li>
<li><p>执行的算术运算数量为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量（包括派生张量）之和给出，为 $O(bnd + bhn^2 + d^2)$。</p>
</li>
<li><p>总内存与算术运算数量之间的比率为 $O\left( \frac{1}{k} + \frac{1}{bn} \right)$。</p>
</li>
<li><p>在这种情况下，该比率远小于 1，这意味着我们进行的内存访问量远少于算术运算的数量，因此内存访问<strong>不是</strong>这里的瓶颈。</p>
</li>
</ul>
<p>当我们<strong>使用KV-Cache</strong>后：时间开销的瓶颈在于<strong>内存的访问</strong><code>**memory access**</code>。</p>
<ul>
<li><p>使用 KV 缓存来减少执行的操作次数。</p>
</li>
<li><p>通过设定 $m &#x3D; n$（查询的序列长度 &#x3D; 键和值的序列长度）。</p>
</li>
<li><p>执行的算术运算次数为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 $O(bn^2d + nd^2)$。</p>
</li>
<li><p>总内存量与算术运算次数之间的比率为 $O(n&#x2F;d + 1&#x2F;b)$。</p>
</li>
<li><p>当 n ≈ d（序列长度接近嵌入向量的维度大小）或 b ≈ 1（批次大小为 1）时，比率变为 1，此时内存访问成为算法的瓶颈。对于批次大小通常没有问题，因为它通常远大于 1，而对于 n&#x2F;d 项，我们需要减少序列长度。但还有更好的方法……</p>
</li>
</ul>
<h5 id="Multi-Query-Attention（MQA）"><a href="#Multi-Query-Attention（MQA）" class="headerlink" title="Multi Query Attention（MQA）"></a>Multi Query Attention（MQA）</h5><p>为了提升attention计算效率，<strong>多查询注意力（</strong><code>**Multi Query Attention，MQA**</code><strong>）</strong> 是多头注意力的一种变体。其主要区别在于，在<strong>多查询注意力中</strong><code>**多个不同的注意力head**</code><strong>共享</strong><code>**一个**</code><strong>k和v的集合，每个head只单独保留了一份q参数。</strong> 具体操作上，<code>**去除 K和V 的head维度，只为Q保留head维度**</code>。因此这就是被叫做Multi Query Attention的原因。</p>
<ul>
<li><p>我们从 K 和 V 中移除了 h 维度，同时保留了 Q 的 h 维度。这意味着所有不同的查询头将共享相同的键和值。</p>
</li>
<li><p>执行的算术运算次数为 $O(bnd^2)$。</p>
</li>
<li><p>所涉及的总内存量，由计算中涉及的所有张量之和（包括派生张量）给出，为 $O(bnd + bn^2k + nd^2)$。</p>
</li>
<li><p>总内存量与算术运算次数之间的比率为 $O(1&#x2F;d + n&#x2F;dh + 1&#x2F;b)$。</p>
</li>
<li><p>与之前的方法相比，我们通过一个 $h$ 因子减少了昂贵的 $n&#x2F;d$ 项。</p>
</li>
<li><p>性能提升显著，而模型的质量只出现了少许下降。</p>
</li>
</ul>
<p>因此<code>K和V的矩阵的数量仅为1个</code>（不分head），大幅度减少了显存占用，使其更高效。<strong>由于多查询注意力改变了注意力机制的结构，因此模型通常需要从训练开始就支持多查询注意力。</strong></p>
<p>研究结果表明，可以通过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可以达到不错的效果。包括Falcon、SantaCoder、StarCoder等在内很多模型都采用了多查询注意力机制。</p>
<h5 id="Grouped-Multi-Query-Attention-GMQA"><a href="#Grouped-Multi-Query-Attention-GMQA" class="headerlink" title="Grouped Multi-Query Attention(GMQA)"></a>Grouped Multi-Query Attention(GMQA)</h5><p>就是在 Multi-Query Attention的基础上，对input进行分组，<strong>如下图2个head分为1组，每组都有自己的K，V，每个组包含2个Q。</strong> （与MQA的区别在于：<strong>MQA的KV只有1份；GQA的KV有</strong><code>**group**</code><strong>份(LLama-70B中是</strong><code>**kv_heads**</code>**&#x3D;8，即每个kv对应8个q)**）</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170116630.png" alt="image.png"></p>
<p><strong>参数量的计算方法:</strong> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624740065"><strong>分析transformer模型的参数量、计算量、中间激活、KV cache</strong></a>﻿</p>
<h3 id="SwiGLU-Function"><a href="#SwiGLU-Function" class="headerlink" title="SwiGLU Function"></a>SwiGLU Function</h3><p>SwiGLU 激活函数是Shazeer 在文献中提出，并在PaLM等模中进行了广泛应用，并且取得了不错的效果，<strong>相较于ReLU 函数在大部分评测中都有不少提升</strong>。在LLaMA 中全连接层使用带有SwiGLU 激活函数的FFN（Position-wise Feed-Forward Network）的计算公式如下：</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250116164815049.png" alt="image.png"></p>
<p>其中，<strong>σ(x) 是Sigmoid 函数</strong>。下图给出了Swish 激活函数在参数β 不同取值下的形状。可以看到当β 趋近于0 时，Swish 函数趋近于线性函数y &#x3D; x，当β 趋近于无穷大时，Swish 函数趋近于ReLU 函数，β 取值为1 时，Swish 函数是光滑且非单调。</p>
<p><img src="https://ronny-1333301201.cos.ap-shanghai.myqcloud.com/20250110170126831.png" alt="image.png"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="tag"># 大模型</a>
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" rel="tag"># 论文精读</a>
              <a href="/tags/nlp/" rel="tag"># nlp</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/01/09/GPT%E7%B3%BB%E5%88%97/" rel="prev" title="GPT系列">
                  <i class="fa fa-angle-left"></i> GPT系列
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/01/10/ViT-Vision-Transformer/" rel="next" title="ViT (Vision Transformer)">
                  ViT (Vision Transformer) <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Ronny Lu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">75k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:08</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动 -->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
